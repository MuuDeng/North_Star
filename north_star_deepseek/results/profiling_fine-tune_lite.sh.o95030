 Data for JOB [5856,1] offset 0 Total slots allocated 16

 ========================   JOB MAP   ========================

 Data for node: a2ap-dgx003	Num slots: 8	Max slots: 0	Num procs: 8
 	Process OMPI jobid: [5856,1] App: 0 Process rank: 0 Bound: socket 0[core 0[hwt 0-1]], socket 0[core 1[hwt 0-1]], socket 0[core 2[hwt 0-1]], socket 0[core 3[hwt 0-1]], socket 0[core 4[hwt 0-1]], socket 0[core 5[hwt 0-1]], socket 0[core 6[hwt 0-1]], socket 0[core 7[hwt 0-1]], socket 0[core 8[hwt 0-1]], socket 0[core 9[hwt 0-1]], socket 0[core 10[hwt 0-1]], socket 0[core 11[hwt 0-1]], socket 0[core 12[hwt 0-1]], socket 0[core 13[hwt 0-1]]:[BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../..][../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../..]
 	Process OMPI jobid: [5856,1] App: 0 Process rank: 1 Bound: socket 0[core 14[hwt 0-1]], socket 0[core 15[hwt 0-1]], socket 0[core 16[hwt 0-1]], socket 0[core 17[hwt 0-1]], socket 0[core 18[hwt 0-1]], socket 0[core 19[hwt 0-1]], socket 0[core 20[hwt 0-1]], socket 0[core 21[hwt 0-1]], socket 0[core 22[hwt 0-1]], socket 0[core 23[hwt 0-1]], socket 0[core 24[hwt 0-1]], socket 0[core 25[hwt 0-1]], socket 0[core 26[hwt 0-1]], socket 0[core 27[hwt 0-1]]:[../../../../../../../../../../../../../../BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/../../../../../../../../../../../../../../../../../../../../../../../../../../../..][../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../..]
 	Process OMPI jobid: [5856,1] App: 0 Process rank: 2 Bound: socket 0[core 28[hwt 0-1]], socket 0[core 29[hwt 0-1]], socket 0[core 30[hwt 0-1]], socket 0[core 31[hwt 0-1]], socket 0[core 32[hwt 0-1]], socket 0[core 33[hwt 0-1]], socket 0[core 34[hwt 0-1]], socket 0[core 35[hwt 0-1]], socket 0[core 36[hwt 0-1]], socket 0[core 37[hwt 0-1]], socket 0[core 38[hwt 0-1]], socket 0[core 39[hwt 0-1]], socket 0[core 40[hwt 0-1]], socket 0[core 41[hwt 0-1]]:[../../../../../../../../../../../../../../../../../../../../../../../../../../../../BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/../../../../../../../../../../../../../..][../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../..]
 	Process OMPI jobid: [5856,1] App: 0 Process rank: 3 Bound: socket 0[core 42[hwt 0-1]], socket 0[core 43[hwt 0-1]], socket 0[core 44[hwt 0-1]], socket 0[core 45[hwt 0-1]], socket 0[core 46[hwt 0-1]], socket 0[core 47[hwt 0-1]], socket 0[core 48[hwt 0-1]], socket 0[core 49[hwt 0-1]], socket 0[core 50[hwt 0-1]], socket 0[core 51[hwt 0-1]], socket 0[core 52[hwt 0-1]], socket 0[core 53[hwt 0-1]], socket 0[core 54[hwt 0-1]], socket 0[core 55[hwt 0-1]]:[../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB][../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../..]
 	Process OMPI jobid: [5856,1] App: 0 Process rank: 4 Bound: socket 1[core 56[hwt 0-1]], socket 1[core 57[hwt 0-1]], socket 1[core 58[hwt 0-1]], socket 1[core 59[hwt 0-1]], socket 1[core 60[hwt 0-1]], socket 1[core 61[hwt 0-1]], socket 1[core 62[hwt 0-1]], socket 1[core 63[hwt 0-1]], socket 1[core 64[hwt 0-1]], socket 1[core 65[hwt 0-1]], socket 1[core 66[hwt 0-1]], socket 1[core 67[hwt 0-1]], socket 1[core 68[hwt 0-1]], socket 1[core 69[hwt 0-1]]:[../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../..][BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../..]
 	Process OMPI jobid: [5856,1] App: 0 Process rank: 5 Bound: socket 1[core 70[hwt 0-1]], socket 1[core 71[hwt 0-1]], socket 1[core 72[hwt 0-1]], socket 1[core 73[hwt 0-1]], socket 1[core 74[hwt 0-1]], socket 1[core 75[hwt 0-1]], socket 1[core 76[hwt 0-1]], socket 1[core 77[hwt 0-1]], socket 1[core 78[hwt 0-1]], socket 1[core 79[hwt 0-1]], socket 1[core 80[hwt 0-1]], socket 1[core 81[hwt 0-1]], socket 1[core 82[hwt 0-1]], socket 1[core 83[hwt 0-1]]:[../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../..][../../../../../../../../../../../../../../BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/../../../../../../../../../../../../../../../../../../../../../../../../../../../..]
 	Process OMPI jobid: [5856,1] App: 0 Process rank: 6 Bound: socket 1[core 84[hwt 0-1]], socket 1[core 85[hwt 0-1]], socket 1[core 86[hwt 0-1]], socket 1[core 87[hwt 0-1]], socket 1[core 88[hwt 0-1]], socket 1[core 89[hwt 0-1]], socket 1[core 90[hwt 0-1]], socket 1[core 91[hwt 0-1]], socket 1[core 92[hwt 0-1]], socket 1[core 93[hwt 0-1]], socket 1[core 94[hwt 0-1]], socket 1[core 95[hwt 0-1]], socket 1[core 96[hwt 0-1]], socket 1[core 97[hwt 0-1]]:[../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../..][../../../../../../../../../../../../../../../../../../../../../../../../../../../../BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/../../../../../../../../../../../../../..]
 	Process OMPI jobid: [5856,1] App: 0 Process rank: 7 Bound: socket 1[core 98[hwt 0-1]], socket 1[core 99[hwt 0-1]], socket 1[core 100[hwt 0-1]], socket 1[core 101[hwt 0-1]], socket 1[core 102[hwt 0-1]], socket 1[core 103[hwt 0-1]], socket 1[core 104[hwt 0-1]], socket 1[core 105[hwt 0-1]], socket 1[core 106[hwt 0-1]], socket 1[core 107[hwt 0-1]], socket 1[core 108[hwt 0-1]], socket 1[core 109[hwt 0-1]], socket 1[core 110[hwt 0-1]], socket 1[core 111[hwt 0-1]]:[../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../..][../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB]

 Data for node: a2ap-dgx006	Num slots: 8	Max slots: 0	Num procs: 8
 	Process OMPI jobid: [5856,1] App: 0 Process rank: 8 Bound: N/A
 	Process OMPI jobid: [5856,1] App: 0 Process rank: 9 Bound: N/A
 	Process OMPI jobid: [5856,1] App: 0 Process rank: 10 Bound: N/A
 	Process OMPI jobid: [5856,1] App: 0 Process rank: 11 Bound: N/A
 	Process OMPI jobid: [5856,1] App: 0 Process rank: 12 Bound: N/A
 	Process OMPI jobid: [5856,1] App: 0 Process rank: 13 Bound: N/A
 	Process OMPI jobid: [5856,1] App: 0 Process rank: 14 Bound: N/A
 	Process OMPI jobid: [5856,1] App: 0 Process rank: 15 Bound: N/A

 =============================================================
 Data for JOB [5856,1] offset 0 Total slots allocated 16

 ========================   JOB MAP   ========================

 Data for node: a2ap-dgx003	Num slots: 8	Max slots: 0	Num procs: 8
 	Process OMPI jobid: [5856,1] App: 0 Process rank: 0 Bound: N/A
 	Process OMPI jobid: [5856,1] App: 0 Process rank: 1 Bound: N/A
 	Process OMPI jobid: [5856,1] App: 0 Process rank: 2 Bound: N/A
 	Process OMPI jobid: [5856,1] App: 0 Process rank: 3 Bound: N/A
 	Process OMPI jobid: [5856,1] App: 0 Process rank: 4 Bound: N/A
 	Process OMPI jobid: [5856,1] App: 0 Process rank: 5 Bound: N/A
 	Process OMPI jobid: [5856,1] App: 0 Process rank: 6 Bound: N/A
 	Process OMPI jobid: [5856,1] App: 0 Process rank: 7 Bound: N/A

 Data for node: a2ap-dgx006	Num slots: 8	Max slots: 0	Num procs: 8
 	Process OMPI jobid: [5856,1] App: 0 Process rank: 8 Bound: socket 0[core 0[hwt 0-1]], socket 0[core 1[hwt 0-1]], socket 0[core 2[hwt 0-1]], socket 0[core 3[hwt 0-1]], socket 0[core 4[hwt 0-1]], socket 0[core 5[hwt 0-1]], socket 0[core 6[hwt 0-1]], socket 0[core 7[hwt 0-1]], socket 0[core 8[hwt 0-1]], socket 0[core 9[hwt 0-1]], socket 0[core 10[hwt 0-1]], socket 0[core 11[hwt 0-1]], socket 0[core 12[hwt 0-1]], socket 0[core 13[hwt 0-1]]:[BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../..][../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../..]
 	Process OMPI jobid: [5856,1] App: 0 Process rank: 9 Bound: socket 0[core 14[hwt 0-1]], socket 0[core 15[hwt 0-1]], socket 0[core 16[hwt 0-1]], socket 0[core 17[hwt 0-1]], socket 0[core 18[hwt 0-1]], socket 0[core 19[hwt 0-1]], socket 0[core 20[hwt 0-1]], socket 0[core 21[hwt 0-1]], socket 0[core 22[hwt 0-1]], socket 0[core 23[hwt 0-1]], socket 0[core 24[hwt 0-1]], socket 0[core 25[hwt 0-1]], socket 0[core 26[hwt 0-1]], socket 0[core 27[hwt 0-1]]:[../../../../../../../../../../../../../../BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/../../../../../../../../../../../../../../../../../../../../../../../../../../../..][../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../..]
 	Process OMPI jobid: [5856,1] App: 0 Process rank: 10 Bound: socket 0[core 28[hwt 0-1]], socket 0[core 29[hwt 0-1]], socket 0[core 30[hwt 0-1]], socket 0[core 31[hwt 0-1]], socket 0[core 32[hwt 0-1]], socket 0[core 33[hwt 0-1]], socket 0[core 34[hwt 0-1]], socket 0[core 35[hwt 0-1]], socket 0[core 36[hwt 0-1]], socket 0[core 37[hwt 0-1]], socket 0[core 38[hwt 0-1]], socket 0[core 39[hwt 0-1]], socket 0[core 40[hwt 0-1]], socket 0[core 41[hwt 0-1]]:[../../../../../../../../../../../../../../../../../../../../../../../../../../../../BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/../../../../../../../../../../../../../..][../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../..]
 	Process OMPI jobid: [5856,1] App: 0 Process rank: 11 Bound: socket 0[core 42[hwt 0-1]], socket 0[core 43[hwt 0-1]], socket 0[core 44[hwt 0-1]], socket 0[core 45[hwt 0-1]], socket 0[core 46[hwt 0-1]], socket 0[core 47[hwt 0-1]], socket 0[core 48[hwt 0-1]], socket 0[core 49[hwt 0-1]], socket 0[core 50[hwt 0-1]], socket 0[core 51[hwt 0-1]], socket 0[core 52[hwt 0-1]], socket 0[core 53[hwt 0-1]], socket 0[core 54[hwt 0-1]], socket 0[core 55[hwt 0-1]]:[../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB][../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../..]
 	Process OMPI jobid: [5856,1] App: 0 Process rank: 12 Bound: socket 1[core 56[hwt 0-1]], socket 1[core 57[hwt 0-1]], socket 1[core 58[hwt 0-1]], socket 1[core 59[hwt 0-1]], socket 1[core 60[hwt 0-1]], socket 1[core 61[hwt 0-1]], socket 1[core 62[hwt 0-1]], socket 1[core 63[hwt 0-1]], socket 1[core 64[hwt 0-1]], socket 1[core 65[hwt 0-1]], socket 1[core 66[hwt 0-1]], socket 1[core 67[hwt 0-1]], socket 1[core 68[hwt 0-1]], socket 1[core 69[hwt 0-1]]:[../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../..][BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../..]
 	Process OMPI jobid: [5856,1] App: 0 Process rank: 13 Bound: socket 1[core 70[hwt 0-1]], socket 1[core 71[hwt 0-1]], socket 1[core 72[hwt 0-1]], socket 1[core 73[hwt 0-1]], socket 1[core 74[hwt 0-1]], socket 1[core 75[hwt 0-1]], socket 1[core 76[hwt 0-1]], socket 1[core 77[hwt 0-1]], socket 1[core 78[hwt 0-1]], socket 1[core 79[hwt 0-1]], socket 1[core 80[hwt 0-1]], socket 1[core 81[hwt 0-1]], socket 1[core 82[hwt 0-1]], socket 1[core 83[hwt 0-1]]:[../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../..][../../../../../../../../../../../../../../BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/../../../../../../../../../../../../../../../../../../../../../../../../../../../..]
 	Process OMPI jobid: [5856,1] App: 0 Process rank: 14 Bound: socket 1[core 84[hwt 0-1]], socket 1[core 85[hwt 0-1]], socket 1[core 86[hwt 0-1]], socket 1[core 87[hwt 0-1]], socket 1[core 88[hwt 0-1]], socket 1[core 89[hwt 0-1]], socket 1[core 90[hwt 0-1]], socket 1[core 91[hwt 0-1]], socket 1[core 92[hwt 0-1]], socket 1[core 93[hwt 0-1]], socket 1[core 94[hwt 0-1]], socket 1[core 95[hwt 0-1]], socket 1[core 96[hwt 0-1]], socket 1[core 97[hwt 0-1]]:[../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../..][../../../../../../../../../../../../../../../../../../../../../../../../../../../../BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/../../../../../../../../../../../../../..]
 	Process OMPI jobid: [5856,1] App: 0 Process rank: 15 Bound: socket 1[core 98[hwt 0-1]], socket 1[core 99[hwt 0-1]], socket 1[core 100[hwt 0-1]], socket 1[core 101[hwt 0-1]], socket 1[core 102[hwt 0-1]], socket 1[core 103[hwt 0-1]], socket 1[core 104[hwt 0-1]], socket 1[core 105[hwt 0-1]], socket 1[core 106[hwt 0-1]], socket 1[core 107[hwt 0-1]], socket 1[core 108[hwt 0-1]], socket 1[core 109[hwt 0-1]], socket 1[core 110[hwt 0-1]], socket 1[core 111[hwt 0-1]]:[../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../..][../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB]

 =============================================================
[1,0]<stderr>:[a2ap-dgx003:866974] MCW rank 0 bound to socket 0[core 0[hwt 0-1]], socket 0[core 1[hwt 0-1]], socket 0[core 2[hwt 0-1]], socket 0[core 3[hwt 0-1]], socket 0[core 4[hwt 0-1]], socket 0[core 5[hwt 0-1]], socket 0[core 6[hwt 0-1]], socket 0[core 7[hwt 0-1]], socket 0[core 8[hwt 0-1]], socket 0[core 9[hwt 0-1]], socket 0[core 10[hwt 0-1]], socket 0[core 11[hwt 0-1]], socket 0[core 12[hwt 0-1]], socket 0[core 13[hwt 0-1]]: [BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../..][../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../..]
[1,1]<stderr>:[a2ap-dgx003:866974] MCW rank 1 bound to socket 0[core 14[hwt 0-1]], socket 0[core 15[hwt 0-1]], socket 0[core 16[hwt 0-1]], socket 0[core 17[hwt 0-1]], socket 0[core 18[hwt 0-1]], socket 0[core 19[hwt 0-1]], socket 0[core 20[hwt 0-1]], socket 0[core 21[hwt 0-1]], socket 0[core 22[hwt 0-1]], socket 0[core 23[hwt 0-1]], socket 0[core 24[hwt 0-1]], socket 0[core 25[hwt 0-1]], socket 0[core 26[hwt 0-1]], socket 0[core 27[hwt 0-1]]: [../../../../../../../../../../../../../../BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/../../../../../../../../../../../../../../../../../../../../../../../../../../../..][../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../..]
[1,2]<stderr>:[a2ap-dgx003:866974] MCW rank 2 bound to socket 0[core 28[hwt 0-1]], socket 0[core 29[hwt 0-1]], socket 0[core 30[hwt 0-1]], socket 0[core 31[hwt 0-1]], socket 0[core 32[hwt 0-1]], socket 0[core 33[hwt 0-1]], socket 0[core 34[hwt 0-1]], socket 0[core 35[hwt 0-1]], socket 0[core 36[hwt 0-1]], socket 0[core 37[hwt 0-1]], socket 0[core 38[hwt 0-1]], socket 0[core 39[hwt 0-1]], socket 0[core 40[hwt 0-1]], socket 0[core 41[hwt 0-1]]: [../../../../../../../../../../../../../../../../../../../../../../../../../../../../BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/../../../../../../../../../../../../../..][../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../..]
[1,3]<stderr>:[a2ap-dgx003:866974] MCW rank 3 bound to socket 0[core 42[hwt 0-1]], socket 0[core 43[hwt 0-1]], socket 0[core 44[hwt 0-1]], socket 0[core 45[hwt 0-1]], socket 0[core 46[hwt 0-1]], socket 0[core 47[hwt 0-1]], socket 0[core 48[hwt 0-1]], socket 0[core 49[hwt 0-1]], socket 0[core 50[hwt 0-1]], socket 0[core 51[hwt 0-1]], socket 0[core 52[hwt 0-1]], socket 0[core 53[hwt 0-1]], socket 0[core 54[hwt 0-1]], socket 0[core 55[hwt 0-1]]: [../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB][../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../..]
[1,4]<stderr>:[a2ap-dgx003:866974] MCW rank 4 bound to socket 1[core 56[hwt 0-1]], socket 1[core 57[hwt 0-1]], socket 1[core 58[hwt 0-1]], socket 1[core 59[hwt 0-1]], socket 1[core 60[hwt 0-1]], socket 1[core 61[hwt 0-1]], socket 1[core 62[hwt 0-1]], socket 1[core 63[hwt 0-1]], socket 1[core 64[hwt 0-1]], socket 1[core 65[hwt 0-1]], socket 1[core 66[hwt 0-1]], socket 1[core 67[hwt 0-1]], socket 1[core 68[hwt 0-1]], socket 1[core 69[hwt 0-1]]: [../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../..][BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../..]
[1,5]<stderr>:[a2ap-dgx003:866974] MCW rank 5 bound to socket 1[core 70[hwt 0-1]], socket 1[core 71[hwt 0-1]], socket 1[core 72[hwt 0-1]], socket 1[core 73[hwt 0-1]], socket 1[core 74[hwt 0-1]], socket 1[core 75[hwt 0-1]], socket 1[core 76[hwt 0-1]], socket 1[core 77[hwt 0-1]], socket 1[core 78[hwt 0-1]], socket 1[core 79[hwt 0-1]], socket 1[core 80[hwt 0-1]], socket 1[core 81[hwt 0-1]], socket 1[core 82[hwt 0-1]], socket 1[core 83[hwt 0-1]]: [../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../..][../../../../../../../../../../../../../../BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/../../../../../../../../../../../../../../../../../../../../../../../../../../../..]
[1,6]<stderr>:[a2ap-dgx003:866974] MCW rank 6 bound to socket 1[core 84[hwt 0-1]], socket 1[core 85[hwt 0-1]], socket 1[core 86[hwt 0-1]], socket 1[core 87[hwt 0-1]], socket 1[core 88[hwt 0-1]], socket 1[core 89[hwt 0-1]], socket 1[core 90[hwt 0-1]], socket 1[core 91[hwt 0-1]], socket 1[core 92[hwt 0-1]], socket 1[core 93[hwt 0-1]], socket 1[core 94[hwt 0-1]], socket 1[core 95[hwt 0-1]], socket 1[core 96[hwt 0-1]], socket 1[core 97[hwt 0-1]]: [../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../..][../../../../../../../../../../../../../../../../../../../../../../../../../../../../BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/../../../../../../../../../../../../../..]
[1,7]<stderr>:[a2ap-dgx003:866974] MCW rank 7 bound to socket 1[core 98[hwt 0-1]], socket 1[core 99[hwt 0-1]], socket 1[core 100[hwt 0-1]], socket 1[core 101[hwt 0-1]], socket 1[core 102[hwt 0-1]], socket 1[core 103[hwt 0-1]], socket 1[core 104[hwt 0-1]], socket 1[core 105[hwt 0-1]], socket 1[core 106[hwt 0-1]], socket 1[core 107[hwt 0-1]], socket 1[core 108[hwt 0-1]], socket 1[core 109[hwt 0-1]], socket 1[core 110[hwt 0-1]], socket 1[core 111[hwt 0-1]]: [../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../..][../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB]
[1,8]<stderr>:[a2ap-dgx006:1033189] MCW rank 8 bound to socket 0[core 0[hwt 0-1]], socket 0[core 1[hwt 0-1]], socket 0[core 2[hwt 0-1]], socket 0[core 3[hwt 0-1]], socket 0[core 4[hwt 0-1]], socket 0[core 5[hwt 0-1]], socket 0[core 6[hwt 0-1]], socket 0[core 7[hwt 0-1]], socket 0[core 8[hwt 0-1]], socket 0[core 9[hwt 0-1]], socket 0[core 10[hwt 0-1]], socket 0[core 11[hwt 0-1]], socket 0[core 12[hwt 0-1]], socket 0[core 13[hwt 0-1]]: [BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../..][../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../..]
[1,9]<stderr>:[a2ap-dgx006:1033189] MCW rank 9 bound to socket 0[core 14[hwt 0-1]], socket 0[core 15[hwt 0-1]], socket 0[core 16[hwt 0-1]], socket 0[core 17[hwt 0-1]], socket 0[core 18[hwt 0-1]], socket 0[core 19[hwt 0-1]], socket 0[core 20[hwt 0-1]], socket 0[core 21[hwt 0-1]], socket 0[core 22[hwt 0-1]], socket 0[core 23[hwt 0-1]], socket 0[core 24[hwt 0-1]], socket 0[core 25[hwt 0-1]], socket 0[core 26[hwt 0-1]], socket 0[core 27[hwt 0-1]]: [../../../../../../../../../../../../../../BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/../../../../../../../../../../../../../../../../../../../../../../../../../../../..][../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../..]
[1,10]<stderr>:[a2ap-dgx006:1033189] MCW rank 10 bound to socket 0[core 28[hwt 0-1]], socket 0[core 29[hwt 0-1]], socket 0[core 30[hwt 0-1]], socket 0[core 31[hwt 0-1]], socket 0[core 32[hwt 0-1]], socket 0[core 33[hwt 0-1]], socket 0[core 34[hwt 0-1]], socket 0[core 35[hwt 0-1]], socket 0[core 36[hwt 0-1]], socket 0[core 37[hwt 0-1]], socket 0[core 38[hwt 0-1]], socket 0[core 39[hwt 0-1]], socket 0[core 40[hwt 0-1]], socket 0[core 41[hwt 0-1]]: [../../../../../../../../../../../../../../../../../../../../../../../../../../../../BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/../../../../../../../../../../../../../..][../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../..]
[1,11]<stderr>:[a2ap-dgx006:1033189] MCW rank 11 bound to socket 0[core 42[hwt 0-1]], socket 0[core 43[hwt 0-1]], socket 0[core 44[hwt 0-1]], socket 0[core 45[hwt 0-1]], socket 0[core 46[hwt 0-1]], socket 0[core 47[hwt 0-1]], socket 0[core 48[hwt 0-1]], socket 0[core 49[hwt 0-1]], socket 0[core 50[hwt 0-1]], socket 0[core 51[hwt 0-1]], socket 0[core 52[hwt 0-1]], socket 0[core 53[hwt 0-1]], socket 0[core 54[hwt 0-1]], socket 0[core 55[hwt 0-1]]: [../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB][../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../..]
[1,12]<stderr>:[a2ap-dgx006:1033189] MCW rank 12 bound to socket 1[core 56[hwt 0-1]], socket 1[core 57[hwt 0-1]], socket 1[core 58[hwt 0-1]], socket 1[core 59[hwt 0-1]], socket 1[core 60[hwt 0-1]], socket 1[core 61[hwt 0-1]], socket 1[core 62[hwt 0-1]], socket 1[core 63[hwt 0-1]], socket 1[core 64[hwt 0-1]], socket 1[core 65[hwt 0-1]], socket 1[core 66[hwt 0-1]], socket 1[core 67[hwt 0-1]], socket 1[core 68[hwt 0-1]], socket 1[core 69[hwt 0-1]]: [../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../..][BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../..]
[1,13]<stderr>:[a2ap-dgx006:1033189] MCW rank 13 bound to socket 1[core 70[hwt 0-1]], socket 1[core 71[hwt 0-1]], socket 1[core 72[hwt 0-1]], socket 1[core 73[hwt 0-1]], socket 1[core 74[hwt 0-1]], socket 1[core 75[hwt 0-1]], socket 1[core 76[hwt 0-1]], socket 1[core 77[hwt 0-1]], socket 1[core 78[hwt 0-1]], socket 1[core 79[hwt 0-1]], socket 1[core 80[hwt 0-1]], socket 1[core 81[hwt 0-1]], socket 1[core 82[hwt 0-1]], socket 1[core 83[hwt 0-1]]: [../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../..][../../../../../../../../../../../../../../BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/../../../../../../../../../../../../../../../../../../../../../../../../../../../..]
[1,14]<stderr>:[a2ap-dgx006:1033189] MCW rank 14 bound to socket 1[core 84[hwt 0-1]], socket 1[core 85[hwt 0-1]], socket 1[core 86[hwt 0-1]], socket 1[core 87[hwt 0-1]], socket 1[core 88[hwt 0-1]], socket 1[core 89[hwt 0-1]], socket 1[core 90[hwt 0-1]], socket 1[core 91[hwt 0-1]], socket 1[core 92[hwt 0-1]], socket 1[core 93[hwt 0-1]], socket 1[core 94[hwt 0-1]], socket 1[core 95[hwt 0-1]], socket 1[core 96[hwt 0-1]], socket 1[core 97[hwt 0-1]]: [../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../..][../../../../../../../../../../../../../../../../../../../../../../../../../../../../BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/../../../../../../../../../../../../../..]
[1,15]<stderr>:[a2ap-dgx006:1033189] MCW rank 15 bound to socket 1[core 98[hwt 0-1]], socket 1[core 99[hwt 0-1]], socket 1[core 100[hwt 0-1]], socket 1[core 101[hwt 0-1]], socket 1[core 102[hwt 0-1]], socket 1[core 103[hwt 0-1]], socket 1[core 104[hwt 0-1]], socket 1[core 105[hwt 0-1]], socket 1[core 106[hwt 0-1]], socket 1[core 107[hwt 0-1]], socket 1[core 108[hwt 0-1]], socket 1[core 109[hwt 0-1]], socket 1[core 110[hwt 0-1]], socket 1[core 111[hwt 0-1]]: [../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../..][../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB]
[1,10]<stderr>:WARNING: CPU IP/backtrace sampling not supported, disabling.
[1,10]<stderr>:Try the 'nsys status --environment' command to learn more.
[1,10]<stderr>:
[1,14]<stderr>:WARNING: CPU IP/backtrace sampling not supported, disabling.
[1,14]<stderr>:Try the 'nsys status --environment' command to learn more.
[1,14]<stderr>:
[1,12]<stderr>:WARNING: CPU IP/backtrace sampling not supported, disabling.
[1,12]<stderr>:Try the 'nsys status --environment' command to learn more.
[1,12]<stderr>:
[1,15]<stderr>:WARNING: CPU IP/backtrace sampling not supported, disabling.
[1,15]<stderr>:Try the 'nsys status --environment' command to learn more.
[1,15]<stderr>:
[1,9]<stderr>:WARNING: CPU IP/backtrace sampling not supported, disabling.
[1,9]<stderr>:Try the 'nsys status --environment' command to learn more.
[1,9]<stderr>:
[1,11]<stderr>:WARNING: CPU IP/backtrace sampling not supported, disabling.
[1,11]<stderr>:Try the 'nsys status --environment' command to learn more.
[1,11]<stderr>:
[1,13]<stderr>:WARNING: CPU IP/backtrace sampling not supported, disabling.
[1,13]<stderr>:Try the 'nsys status --environment' command to learn more.
[1,13]<stderr>:
[1,8]<stderr>:WARNING: CPU IP/backtrace sampling not supported, disabling.
[1,8]<stderr>:Try the 'nsys status --environment' command to learn more.
[1,8]<stderr>:
[1,2]<stderr>:WARNING: CPU IP/backtrace sampling not supported, disabling.
[1,2]<stderr>:Try the 'nsys status --environment' command to learn more.
[1,2]<stderr>:
[1,1]<stderr>:WARNING: CPU IP/backtrace sampling not supported, disabling.
[1,1]<stderr>:Try the 'nsys status --environment' command to learn more.
[1,1]<stderr>:
[1,0]<stderr>:WARNING: CPU IP/backtrace sampling not supported, disabling.
[1,0]<stderr>:Try the 'nsys status --environment' command to learn more.
[1,0]<stderr>:
[1,5]<stderr>:WARNING: CPU IP/backtrace sampling not supported, disabling.
[1,5]<stderr>:Try the 'nsys status --environment' command to learn more.
[1,5]<stderr>:
[1,4]<stderr>:WARNING: CPU IP/backtrace sampling not supported, disabling.
[1,4]<stderr>:Try the 'nsys status --environment' command to learn more.
[1,4]<stderr>:
[1,7]<stderr>:WARNING: CPU IP/backtrace sampling not supported, disabling.
[1,7]<stderr>:Try the 'nsys status --environment' command to learn more.
[1,7]<stderr>:
[1,6]<stderr>:WARNING: CPU IP/backtrace sampling not supported, disabling.
[1,6]<stderr>:Try the 'nsys status --environment' command to learn more.
[1,6]<stderr>:
[1,3]<stderr>:WARNING: CPU IP/backtrace sampling not supported, disabling.
[1,3]<stderr>:Try the 'nsys status --environment' command to learn more.
[1,3]<stderr>:
[1,10]<stdout>:Collecting data...
[1,14]<stdout>:Collecting data...
[1,11]<stdout>:Collecting data...
[1,12]<stdout>:Collecting data...
[1,0]<stdout>:Collecting data...
[1,2]<stdout>:Collecting data...
[1,1]<stdout>:Collecting data...
[1,8]<stdout>:Collecting data...
[1,9]<stdout>:Collecting data...
[1,15]<stdout>:Collecting data...
[1,13]<stdout>:Collecting data...
[1,5]<stdout>:Collecting data...
[1,3]<stdout>:Collecting data...
[1,4]<stdout>:Collecting data...
[1,6]<stdout>:Collecting data...
[1,7]<stdout>:Collecting data...
[1,10]<stderr>:WARNING:sglang.srt.layers.quantization.deep_gemm_wrapper.compile_utils:NVCC Compiler not found, use NVRTC for DeepGEMM JIT and may have performance loss with some cases.
[1,12]<stderr>:WARNING:sglang.srt.layers.quantization.deep_gemm_wrapper.compile_utils:NVCC Compiler not found, use NVRTC for DeepGEMM JIT and may have performance loss with some cases.
[1,8]<stderr>:WARNING:sglang.srt.layers.quantization.deep_gemm_wrapper.compile_utils:NVCC Compiler not found, use NVRTC for DeepGEMM JIT and may have performance loss with some cases.
[1,15]<stderr>:WARNING:sglang.srt.layers.quantization.deep_gemm_wrapper.compile_utils:NVCC Compiler not found, use NVRTC for DeepGEMM JIT and may have performance loss with some cases.
[1,11]<stderr>:WARNING:sglang.srt.layers.quantization.deep_gemm_wrapper.compile_utils:NVCC Compiler not found, use NVRTC for DeepGEMM JIT and may have performance loss with some cases.
[1,9]<stderr>:WARNING:sglang.srt.layers.quantization.deep_gemm_wrapper.compile_utils:NVCC Compiler not found, use NVRTC for DeepGEMM JIT and may have performance loss with some cases.
[1,14]<stderr>:WARNING:sglang.srt.layers.quantization.deep_gemm_wrapper.compile_utils:NVCC Compiler not found, use NVRTC for DeepGEMM JIT and may have performance loss with some cases.
[1,13]<stderr>:WARNING:sglang.srt.layers.quantization.deep_gemm_wrapper.compile_utils:NVCC Compiler not found, use NVRTC for DeepGEMM JIT and may have performance loss with some cases.
[1,13]<stderr>:W1007 12:06:51.608000 1034724 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[1,13]<stderr>:W1007 12:06:51.608000 1034724 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[1,10]<stderr>:W1007 12:06:51.608000 1034684 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[1,10]<stderr>:W1007 12:06:51.608000 1034684 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[1,12]<stderr>:W1007 12:06:51.608000 1034706 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[1,12]<stderr>:W1007 12:06:51.608000 1034706 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[1,11]<stderr>:W1007 12:06:51.608000 1034708 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[1,11]<stderr>:W1007 12:06:51.608000 1034708 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[1,8]<stderr>:W1007 12:06:51.608000 1034731 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[1,8]<stderr>:W1007 12:06:51.608000 1034731 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[1,14]<stderr>:W1007 12:06:51.608000 1034691 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[1,14]<stderr>:W1007 12:06:51.608000 1034691 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[1,9]<stderr>:W1007 12:06:51.608000 1034754 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[1,9]<stderr>:W1007 12:06:51.608000 1034754 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[1,15]<stderr>:W1007 12:06:51.608000 1034745 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[1,15]<stderr>:W1007 12:06:51.608000 1034745 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[1,7]<stderr>:W1007 12:06:56.709000 868287 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[1,7]<stderr>:W1007 12:06:56.709000 868287 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[1,5]<stderr>:W1007 12:06:56.709000 868265 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[1,5]<stderr>:W1007 12:06:56.709000 868265 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[1,6]<stderr>:W1007 12:06:56.709000 868281 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[1,6]<stderr>:W1007 12:06:56.709000 868281 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[1,3]<stderr>:W1007 12:06:56.709000 868271 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[1,3]<stderr>:W1007 12:06:56.709000 868271 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[1,0]<stderr>:W1007 12:06:56.709000 868123 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[1,0]<stderr>:W1007 12:06:56.709000 868123 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[1,4]<stderr>:W1007 12:06:56.709000 868275 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[1,4]<stderr>:W1007 12:06:56.709000 868275 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[1,2]<stderr>:W1007 12:06:56.709000 868118 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[1,2]<stderr>:W1007 12:06:56.709000 868118 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[1,1]<stderr>:W1007 12:06:56.709000 868184 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[1,1]<stderr>:W1007 12:06:56.709000 868184 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[1,8]<stderr>:[2025-10-07 12:07:02] Using default HuggingFace chat template with detected content format: string
[1,0]<stderr>:[2025-10-07 12:07:05] Using default HuggingFace chat template with detected content format: string
[1,0]<stderr>:W1007 12:07:44.093000 870989 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[1,0]<stderr>:W1007 12:07:44.093000 870989 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[1,0]<stderr>:[2025-10-07 12:07:45 TP0] Attention backend not explicitly specified. Use fa3 backend by default.
[1,0]<stderr>:[2025-10-07 12:07:45 TP0] Chunked prefix cache is turned on.
[1,0]<stderr>:[2025-10-07 12:07:45 TP0] Init torch distributed begin.
[1,13]<stderr>:WARNING:sglang.srt.layers.quantization.deep_gemm_wrapper.compile_utils:NVCC Compiler not found, use NVRTC for DeepGEMM JIT and may have performance loss with some cases.
[1,13]<stderr>:W1007 12:07:46.712000 1037169 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[1,13]<stderr>:W1007 12:07:46.712000 1037169 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[1,13]<stderr>:WARNING:sglang.srt.layers.quantization.deep_gemm_wrapper.compile_utils:NVCC Compiler not found, use NVRTC for DeepGEMM JIT and may have performance loss with some cases.
[1,13]<stderr>:W1007 12:07:47.649000 1037171 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[1,13]<stderr>:W1007 12:07:47.649000 1037171 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[1,13]<stderr>:WARNING:sglang.srt.layers.quantization.deep_gemm_wrapper.compile_utils:NVCC Compiler not found, use NVRTC for DeepGEMM JIT and may have performance loss with some cases.
[1,13]<stderr>:W1007 12:07:48.290000 1037170 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[1,13]<stderr>:W1007 12:07:48.290000 1037170 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[1,14]<stderr>:WARNING:sglang.srt.layers.quantization.deep_gemm_wrapper.compile_utils:NVCC Compiler not found, use NVRTC for DeepGEMM JIT and may have performance loss with some cases.
[1,14]<stderr>:W1007 12:07:48.960000 1037455 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[1,14]<stderr>:W1007 12:07:48.960000 1037455 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[1,9]<stderr>:WARNING:sglang.srt.layers.quantization.deep_gemm_wrapper.compile_utils:NVCC Compiler not found, use NVRTC for DeepGEMM JIT and may have performance loss with some cases.
[1,13]<stderr>:[2025-10-07 12:07:49 TP5] Context: self.device='cuda' self.gpu_id=1 os.environ.get('CUDA_VISIBLE_DEVICES')='5' self.tp_rank=5 self.tp_size=8
[1,13]<stderr>:[2025-10-07 12:07:49 TP5] Scheduler hit an exception: Traceback (most recent call last):
[1,13]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2555, in run_scheduler_process
[1,13]<stderr>:    scheduler = Scheduler(
[1,13]<stderr>:                ^^^^^^^^^^
[1,13]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 315, in __init__
[1,13]<stderr>:    self.tp_worker = TpWorkerClass(
[1,13]<stderr>:                     ^^^^^^^^^^^^^^
[1,13]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/tp_worker_overlap_thread.py", line 67, in __init__
[1,13]<stderr>:    self.worker = TpModelWorker(
[1,13]<stderr>:                  ^^^^^^^^^^^^^^
[1,13]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 84, in __init__
[1,13]<stderr>:    self.model_runner = ModelRunner(
[1,13]<stderr>:                        ^^^^^^^^^^^^
[1,13]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 235, in __init__
[1,13]<stderr>:    min_per_gpu_memory = self.init_torch_distributed()
[1,13]<stderr>:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,13]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 547, in init_torch_distributed
[1,13]<stderr>:    torch.get_device_module(self.device).set_device(self.gpu_id)
[1,13]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/cuda/__init__.py", line 569, in set_device
[1,13]<stderr>:    torch._C._cuda_setDevice(device)
[1,13]<stderr>:torch.AcceleratorError: CUDA error: invalid device ordinal
[1,13]<stderr>:CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
[1,13]<stderr>:For debugging consider passing CUDA_LAUNCH_BLOCKING=1
[1,13]<stderr>:Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
[1,13]<stderr>:
[1,13]<stderr>:
[1,13]<stderr>:[2025-10-07 12:07:49] Received sigquit from a child process. It usually means the child failed.
[1,9]<stderr>:W1007 12:07:49.440000 1037604 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[1,9]<stderr>:W1007 12:07:49.440000 1037604 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[1,13]<stderr>:The target application terminated. One or more process it created re-parented.
[1,13]<stderr>:Waiting for termination of re-parented processes.
[1,13]<stderr>:Use the `--wait` option to modify this behavior.
[1,0]<stderr>:W1007 12:07:50.101000 870991 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[1,0]<stderr>:W1007 12:07:50.101000 870991 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[1,15]<stderr>:WARNING:sglang.srt.layers.quantization.deep_gemm_wrapper.compile_utils:NVCC Compiler not found, use NVRTC for DeepGEMM JIT and may have performance loss with some cases.
[1,15]<stderr>:W1007 12:07:50.379000 1037475 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[1,15]<stderr>:W1007 12:07:50.379000 1037475 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[1,7]<stderr>:W1007 12:07:50.394000 870968 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[1,7]<stderr>:W1007 12:07:50.394000 870968 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[1,10]<stderr>:WARNING:sglang.srt.layers.quantization.deep_gemm_wrapper.compile_utils:NVCC Compiler not found, use NVRTC for DeepGEMM JIT and may have performance loss with some cases.
[1,12]<stderr>:WARNING:sglang.srt.layers.quantization.deep_gemm_wrapper.compile_utils:NVCC Compiler not found, use NVRTC for DeepGEMM JIT and may have performance loss with some cases.
[1,15]<stderr>:WARNING:sglang.srt.layers.quantization.deep_gemm_wrapper.compile_utils:NVCC Compiler not found, use NVRTC for DeepGEMM JIT and may have performance loss with some cases.
[1,8]<stderr>:WARNING:sglang.srt.layers.quantization.deep_gemm_wrapper.compile_utils:NVCC Compiler not found, use NVRTC for DeepGEMM JIT and may have performance loss with some cases.
[1,10]<stderr>:W1007 12:07:50.796000 1037882 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[1,10]<stderr>:W1007 12:07:50.796000 1037882 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[1,12]<stderr>:W1007 12:07:50.809000 1037730 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[1,12]<stderr>:W1007 12:07:50.809000 1037730 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[1,15]<stderr>:W1007 12:07:50.812000 1037478 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[1,15]<stderr>:W1007 12:07:50.812000 1037478 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[1,8]<stderr>:W1007 12:07:50.956000 1037173 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[1,8]<stderr>:W1007 12:07:50.956000 1037173 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[1,9]<stderr>:WARNING:sglang.srt.layers.quantization.deep_gemm_wrapper.compile_utils:NVCC Compiler not found, use NVRTC for DeepGEMM JIT and may have performance loss with some cases.
[1,14]<stderr>:WARNING:sglang.srt.layers.quantization.deep_gemm_wrapper.compile_utils:NVCC Compiler not found, use NVRTC for DeepGEMM JIT and may have performance loss with some cases.
[1,8]<stderr>:WARNING:sglang.srt.layers.quantization.deep_gemm_wrapper.compile_utils:NVCC Compiler not found, use NVRTC for DeepGEMM JIT and may have performance loss with some cases.
[1,10]<stderr>:WARNING:sglang.srt.layers.quantization.deep_gemm_wrapper.compile_utils:NVCC Compiler not found, use NVRTC for DeepGEMM JIT and may have performance loss with some cases.
[1,8]<stderr>:WARNING:sglang.srt.layers.quantization.deep_gemm_wrapper.compile_utils:NVCC Compiler not found, use NVRTC for DeepGEMM JIT and may have performance loss with some cases.
[1,9]<stderr>:W1007 12:07:51.330000 1037603 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[1,9]<stderr>:W1007 12:07:51.330000 1037603 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[1,8]<stderr>:W1007 12:07:51.386000 1037175 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[1,8]<stderr>:W1007 12:07:51.386000 1037175 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[1,14]<stderr>:W1007 12:07:51.386000 1037458 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[1,14]<stderr>:W1007 12:07:51.386000 1037458 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[1,10]<stderr>:W1007 12:07:51.401000 1037887 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[1,10]<stderr>:W1007 12:07:51.401000 1037887 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[1,8]<stderr>:W1007 12:07:51.442000 1037176 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[1,8]<stderr>:W1007 12:07:51.442000 1037176 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[1,15]<stderr>:WARNING:sglang.srt.layers.quantization.deep_gemm_wrapper.compile_utils:NVCC Compiler not found, use NVRTC for DeepGEMM JIT and may have performance loss with some cases.
[1,7]<stderr>:W1007 12:07:51.655000 870971 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[1,7]<stderr>:W1007 12:07:51.655000 870971 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[1,11]<stderr>:WARNING:sglang.srt.layers.quantization.deep_gemm_wrapper.compile_utils:NVCC Compiler not found, use NVRTC for DeepGEMM JIT and may have performance loss with some cases.
[1,15]<stderr>:W1007 12:07:51.692000 1037476 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[1,15]<stderr>:W1007 12:07:51.692000 1037476 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[1,14]<stderr>:[2025-10-07 12:07:51 TP0] Attention backend not explicitly specified. Use fa3 backend by default.
[1,14]<stderr>:[2025-10-07 12:07:51 TP0] Chunked prefix cache is turned on.
[1,14]<stderr>:[2025-10-07 12:07:51 TP0] Init torch distributed begin.
[1,15]<stderr>:WARNING:sglang.srt.layers.quantization.deep_gemm_wrapper.compile_utils:NVCC Compiler not found, use NVRTC for DeepGEMM JIT and may have performance loss with some cases.
[1,15]<stderr>:W1007 12:07:51.950000 1037477 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[1,15]<stderr>:W1007 12:07:51.950000 1037477 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[1,11]<stderr>:WARNING:sglang.srt.layers.quantization.deep_gemm_wrapper.compile_utils:NVCC Compiler not found, use NVRTC for DeepGEMM JIT and may have performance loss with some cases.
[1,11]<stderr>:W1007 12:07:51.978000 1037735 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[1,11]<stderr>:W1007 12:07:51.978000 1037735 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[1,14]<stderr>:WARNING:sglang.srt.layers.quantization.deep_gemm_wrapper.compile_utils:NVCC Compiler not found, use NVRTC for DeepGEMM JIT and may have performance loss with some cases.
[1,9]<stderr>:[2025-10-07 12:07:52 TP6] Context: self.device='cuda' self.gpu_id=2 os.environ.get('CUDA_VISIBLE_DEVICES')='1' self.tp_rank=6 self.tp_size=8
[1,9]<stderr>:[2025-10-07 12:07:52 TP6] Scheduler hit an exception: Traceback (most recent call last):
[1,9]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2555, in run_scheduler_process
[1,9]<stderr>:    scheduler = Scheduler(
[1,9]<stderr>:                ^^^^^^^^^^
[1,9]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 315, in __init__
[1,9]<stderr>:    self.tp_worker = TpWorkerClass(
[1,9]<stderr>:                     ^^^^^^^^^^^^^^
[1,9]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/tp_worker_overlap_thread.py", line 67, in __init__
[1,9]<stderr>:    self.worker = TpModelWorker(
[1,9]<stderr>:                  ^^^^^^^^^^^^^^
[1,9]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 84, in __init__
[1,9]<stderr>:    self.model_runner = ModelRunner(
[1,9]<stderr>:                        ^^^^^^^^^^^^
[1,9]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 235, in __init__
[1,9]<stderr>:    min_per_gpu_memory = self.init_torch_distributed()
[1,9]<stderr>:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,9]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 547, in init_torch_distributed
[1,9]<stderr>:    torch.get_device_module(self.device).set_device(self.gpu_id)
[1,9]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/cuda/__init__.py", line 569, in set_device
[1,9]<stderr>:    torch._C._cuda_setDevice(device)
[1,9]<stderr>:torch.AcceleratorError: CUDA error: invalid device ordinal
[1,9]<stderr>:CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
[1,9]<stderr>:For debugging consider passing CUDA_LAUNCH_BLOCKING=1
[1,9]<stderr>:Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
[1,9]<stderr>:
[1,9]<stderr>:
[1,9]<stderr>:[2025-10-07 12:07:52] Received sigquit from a child process. It usually means the child failed.
[1,11]<stderr>:W1007 12:07:52.208000 1037736 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[1,11]<stderr>:W1007 12:07:52.208000 1037736 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[1,14]<stderr>:W1007 12:07:52.209000 1037456 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[1,14]<stderr>:W1007 12:07:52.209000 1037456 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[1,14]<stderr>:[W1007 12:07:52.410231765 Utils.hpp:137] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[1,14]<stderr>:[2025-10-07 12:07:52 TP0] Scheduler hit an exception: Traceback (most recent call last):
[1,14]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2555, in run_scheduler_process
[1,14]<stderr>:    scheduler = Scheduler(
[1,14]<stderr>:                ^^^^^^^^^^
[1,14]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 315, in __init__
[1,14]<stderr>:    self.tp_worker = TpWorkerClass(
[1,14]<stderr>:                     ^^^^^^^^^^^^^^
[1,14]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/tp_worker_overlap_thread.py", line 67, in __init__
[1,14]<stderr>:    self.worker = TpModelWorker(
[1,14]<stderr>:                  ^^^^^^^^^^^^^^
[1,14]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 84, in __init__
[1,14]<stderr>:    self.model_runner = ModelRunner(
[1,14]<stderr>:                        ^^^^^^^^^^^^
[1,14]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 235, in __init__
[1,14]<stderr>:    min_per_gpu_memory = self.init_torch_distributed()
[1,14]<stderr>:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,14]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 591, in init_torch_distributed
[1,14]<stderr>:    init_distributed_environment(
[1,14]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/distributed/parallel_state.py", line 1280, in init_distributed_environment
[1,14]<stderr>:    _WORLD = init_world_group(ranks, local_rank, backend)
[1,14]<stderr>:             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,14]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/distributed/parallel_state.py", line 1101, in init_world_group
[1,14]<stderr>:    return GroupCoordinator(
[1,14]<stderr>:           ^^^^^^^^^^^^^^^^^
[1,14]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/distributed/parallel_state.py", line 250, in __init__
[1,14]<stderr>:    assert self.cpu_group is not None
[1,14]<stderr>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,14]<stderr>:AssertionError
[1,14]<stderr>:
[1,14]<stderr>:[2025-10-07 12:07:52] Received sigquit from a child process. It usually means the child failed.
[1,11]<stderr>:WARNING:sglang.srt.layers.quantization.deep_gemm_wrapper.compile_utils:NVCC Compiler not found, use NVRTC for DeepGEMM JIT and may have performance loss with some cases.
[1,11]<stderr>:WARNING:sglang.srt.layers.quantization.deep_gemm_wrapper.compile_utils:NVCC Compiler not found, use NVRTC for DeepGEMM JIT and may have performance loss with some cases.
[1,12]<stderr>:WARNING:sglang.srt.layers.quantization.deep_gemm_wrapper.compile_utils:NVCC Compiler not found, use NVRTC for DeepGEMM JIT and may have performance loss with some cases.
[1,9]<stderr>:The target application terminated. One or more process it created re-parented.
[1,9]<stderr>:Waiting for termination of re-parented processes.
[1,9]<stderr>:Use the `--wait` option to modify this behavior.
[1,10]<stderr>:WARNING:sglang.srt.layers.quantization.deep_gemm_wrapper.compile_utils:NVCC Compiler not found, use NVRTC for DeepGEMM JIT and may have performance loss with some cases.
[1,11]<stderr>:W1007 12:07:52.475000 1037734 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[1,11]<stderr>:W1007 12:07:52.475000 1037734 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[1,11]<stderr>:W1007 12:07:52.477000 1037733 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[1,11]<stderr>:W1007 12:07:52.477000 1037733 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[1,8]<stderr>:WARNING:sglang.srt.layers.quantization.deep_gemm_wrapper.compile_utils:NVCC Compiler not found, use NVRTC for DeepGEMM JIT and may have performance loss with some cases.
[1,10]<stderr>:WARNING:sglang.srt.layers.quantization.deep_gemm_wrapper.compile_utils:NVCC Compiler not found, use NVRTC for DeepGEMM JIT and may have performance loss with some cases.
[1,12]<stderr>:WARNING:sglang.srt.layers.quantization.deep_gemm_wrapper.compile_utils:NVCC Compiler not found, use NVRTC for DeepGEMM JIT and may have performance loss with some cases.
[1,10]<stderr>:W1007 12:07:52.587000 1037881 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[1,10]<stderr>:W1007 12:07:52.587000 1037881 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[1,12]<stderr>:WARNING:sglang.srt.layers.quantization.deep_gemm_wrapper.compile_utils:NVCC Compiler not found, use NVRTC for DeepGEMM JIT and may have performance loss with some cases.
[1,12]<stderr>:W1007 12:07:52.612000 1037728 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[1,12]<stderr>:W1007 12:07:52.612000 1037728 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[1,14]<stderr>:The target application terminated. One or more process it created re-parented.
[1,14]<stderr>:Waiting for termination of re-parented processes.
[1,14]<stderr>:Use the `--wait` option to modify this behavior.
[1,8]<stderr>:WARNING:sglang.srt.layers.quantization.deep_gemm_wrapper.compile_utils:NVCC Compiler not found, use NVRTC for DeepGEMM JIT and may have performance loss with some cases.
[1,10]<stderr>:W1007 12:07:52.748000 1037880 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[1,10]<stderr>:W1007 12:07:52.748000 1037880 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[1,8]<stderr>:W1007 12:07:52.773000 1037179 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[1,8]<stderr>:W1007 12:07:52.773000 1037179 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[1,12]<stderr>:W1007 12:07:52.780000 1037729 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[1,12]<stderr>:W1007 12:07:52.780000 1037729 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[1,0]<stderr>:[2025-10-07 12:07:52 TP2] Context: self.device='cuda' self.gpu_id=2 os.environ.get('CUDA_VISIBLE_DEVICES')='0' self.tp_rank=2 self.tp_size=8
[1,0]<stderr>:[2025-10-07 12:07:52 TP2] Scheduler hit an exception: Traceback (most recent call last):
[1,0]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2555, in run_scheduler_process
[1,0]<stderr>:    scheduler = Scheduler(
[1,0]<stderr>:                ^^^^^^^^^^
[1,0]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 315, in __init__
[1,0]<stderr>:    self.tp_worker = TpWorkerClass(
[1,0]<stderr>:                     ^^^^^^^^^^^^^^
[1,0]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/tp_worker_overlap_thread.py", line 67, in __init__
[1,0]<stderr>:    self.worker = TpModelWorker(
[1,0]<stderr>:                  ^^^^^^^^^^^^^^
[1,0]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 84, in __init__
[1,0]<stderr>:    self.model_runner = ModelRunner(
[1,0]<stderr>:                        ^^^^^^^^^^^^
[1,0]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 235, in __init__
[1,0]<stderr>:    min_per_gpu_memory = self.init_torch_distributed()
[1,0]<stderr>:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 547, in init_torch_distributed
[1,0]<stderr>:    torch.get_device_module(self.device).set_device(self.gpu_id)
[1,0]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/cuda/__init__.py", line 569, in set_device
[1,0]<stderr>:    torch._C._cuda_setDevice(device)
[1,0]<stderr>:torch.AcceleratorError: CUDA error: invalid device ordinal
[1,0]<stderr>:CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
[1,0]<stderr>:For debugging consider passing CUDA_LAUNCH_BLOCKING=1
[1,0]<stderr>:Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
[1,0]<stderr>:
[1,0]<stderr>:
[1,0]<stderr>:[2025-10-07 12:07:52] Received sigquit from a child process. It usually means the child failed.
[1,12]<stderr>:W1007 12:07:52.830000 1037731 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[1,12]<stderr>:W1007 12:07:52.830000 1037731 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[1,7]<stderr>:W1007 12:07:52.970000 870970 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[1,7]<stderr>:W1007 12:07:52.970000 870970 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[1,8]<stderr>:W1007 12:07:52.971000 1037174 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[1,8]<stderr>:W1007 12:07:52.971000 1037174 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[1,0]<stderr>:The target application terminated. One or more process it created re-parented.
[1,0]<stderr>:Waiting for termination of re-parented processes.
[1,0]<stderr>:Use the `--wait` option to modify this behavior.
[1,7]<stderr>:[2025-10-07 12:07:54 TP7] Context: self.device='cuda' self.gpu_id=3 os.environ.get('CUDA_VISIBLE_DEVICES')='7' self.tp_rank=7 self.tp_size=8
[1,7]<stderr>:[2025-10-07 12:07:54 TP7] Scheduler hit an exception: Traceback (most recent call last):
[1,7]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2555, in run_scheduler_process
[1,7]<stderr>:    scheduler = Scheduler(
[1,7]<stderr>:                ^^^^^^^^^^
[1,7]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 315, in __init__
[1,7]<stderr>:    self.tp_worker = TpWorkerClass(
[1,7]<stderr>:                     ^^^^^^^^^^^^^^
[1,7]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/tp_worker_overlap_thread.py", line 67, in __init__
[1,7]<stderr>:    self.worker = TpModelWorker(
[1,7]<stderr>:                  ^^^^^^^^^^^^^^
[1,7]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 84, in __init__
[1,7]<stderr>:    self.model_runner = ModelRunner(
[1,7]<stderr>:                        ^^^^^^^^^^^^
[1,7]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 235, in __init__
[1,7]<stderr>:    min_per_gpu_memory = self.init_torch_distributed()
[1,7]<stderr>:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,7]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 547, in init_torch_distributed
[1,7]<stderr>:    torch.get_device_module(self.device).set_device(self.gpu_id)
[1,7]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/cuda/__init__.py", line 569, in set_device
[1,7]<stderr>:    torch._C._cuda_setDevice(device)
[1,7]<stderr>:torch.AcceleratorError: CUDA error: invalid device ordinal
[1,7]<stderr>:CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
[1,7]<stderr>:For debugging consider passing CUDA_LAUNCH_BLOCKING=1
[1,7]<stderr>:Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
[1,7]<stderr>:
[1,7]<stderr>:
[1,7]<stderr>:[2025-10-07 12:07:54] Received sigquit from a child process. It usually means the child failed.
[1,7]<stderr>:The target application terminated. One or more process it created re-parented.
[1,7]<stderr>:Waiting for termination of re-parented processes.
[1,7]<stderr>:Use the `--wait` option to modify this behavior.
[1,5]<stderr>:W1007 12:07:54.539000 871259 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[1,5]<stderr>:W1007 12:07:54.539000 871259 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[1,5]<stderr>:W1007 12:07:54.846000 871261 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[1,5]<stderr>:W1007 12:07:54.846000 871261 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[1,15]<stderr>:[2025-10-07 12:07:54 TP7] Context: self.device='cuda' self.gpu_id=3 os.environ.get('CUDA_VISIBLE_DEVICES')='7' self.tp_rank=7 self.tp_size=8
[1,15]<stderr>:[2025-10-07 12:07:54 TP7] Scheduler hit an exception: Traceback (most recent call last):
[1,15]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2555, in run_scheduler_process
[1,15]<stderr>:    scheduler = Scheduler(
[1,15]<stderr>:                ^^^^^^^^^^
[1,15]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 315, in __init__
[1,15]<stderr>:    self.tp_worker = TpWorkerClass(
[1,15]<stderr>:                     ^^^^^^^^^^^^^^
[1,15]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/tp_worker_overlap_thread.py", line 67, in __init__
[1,15]<stderr>:    self.worker = TpModelWorker(
[1,15]<stderr>:                  ^^^^^^^^^^^^^^
[1,15]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 84, in __init__
[1,15]<stderr>:    self.model_runner = ModelRunner(
[1,15]<stderr>:                        ^^^^^^^^^^^^
[1,15]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 235, in __init__
[1,15]<stderr>:    min_per_gpu_memory = self.init_torch_distributed()
[1,15]<stderr>:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,15]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 547, in init_torch_distributed
[1,15]<stderr>:    torch.get_device_module(self.device).set_device(self.gpu_id)
[1,15]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/cuda/__init__.py", line 569, in set_device
[1,15]<stderr>:    torch._C._cuda_setDevice(device)
[1,15]<stderr>:torch.AcceleratorError: CUDA error: invalid device ordinal
[1,15]<stderr>:CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
[1,15]<stderr>:For debugging consider passing CUDA_LAUNCH_BLOCKING=1
[1,15]<stderr>:Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
[1,15]<stderr>:
[1,15]<stderr>:
[1,15]<stderr>:[2025-10-07 12:07:54] Received sigquit from a child process. It usually means the child failed.
[1,12]<stderr>:[2025-10-07 12:07:54 TP2] Context: self.device='cuda' self.gpu_id=2 os.environ.get('CUDA_VISIBLE_DEVICES')='4' self.tp_rank=2 self.tp_size=8
[1,12]<stderr>:[2025-10-07 12:07:54 TP2] Scheduler hit an exception: Traceback (most recent call last):
[1,12]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2555, in run_scheduler_process
[1,12]<stderr>:    scheduler = Scheduler(
[1,12]<stderr>:                ^^^^^^^^^^
[1,12]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 315, in __init__
[1,12]<stderr>:    self.tp_worker = TpWorkerClass(
[1,12]<stderr>:                     ^^^^^^^^^^^^^^
[1,12]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/tp_worker_overlap_thread.py", line 67, in __init__
[1,12]<stderr>:    self.worker = TpModelWorker(
[1,12]<stderr>:                  ^^^^^^^^^^^^^^
[1,12]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 84, in __init__
[1,12]<stderr>:    self.model_runner = ModelRunner(
[1,12]<stderr>:                        ^^^^^^^^^^^^
[1,12]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 235, in __init__
[1,12]<stderr>:    min_per_gpu_memory = self.init_torch_distributed()
[1,12]<stderr>:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,12]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 547, in init_torch_distributed
[1,12]<stderr>:    torch.get_device_module(self.device).set_device(self.gpu_id)
[1,12]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/cuda/__init__.py", line 569, in set_device
[1,12]<stderr>:    torch._C._cuda_setDevice(device)
[1,12]<stderr>:torch.AcceleratorError: CUDA error: invalid device ordinal
[1,12]<stderr>:CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
[1,12]<stderr>:For debugging consider passing CUDA_LAUNCH_BLOCKING=1
[1,12]<stderr>:Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
[1,12]<stderr>:
[1,12]<stderr>:
[1,12]<stderr>:[2025-10-07 12:07:54] Received sigquit from a child process. It usually means the child failed.
[1,10]<stderr>:[2025-10-07 12:07:55 TP2] Context: self.device='cuda' self.gpu_id=2 os.environ.get('CUDA_VISIBLE_DEVICES')='2' self.tp_rank=2 self.tp_size=8
[1,10]<stderr>:[2025-10-07 12:07:55 TP2] Scheduler hit an exception: Traceback (most recent call last):
[1,10]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2555, in run_scheduler_process
[1,10]<stderr>:    scheduler = Scheduler(
[1,10]<stderr>:                ^^^^^^^^^^
[1,10]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 315, in __init__
[1,10]<stderr>:    self.tp_worker = TpWorkerClass(
[1,10]<stderr>:                     ^^^^^^^^^^^^^^
[1,10]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/tp_worker_overlap_thread.py", line 67, in __init__
[1,10]<stderr>:    self.worker = TpModelWorker(
[1,10]<stderr>:                  ^^^^^^^^^^^^^^
[1,10]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 84, in __init__
[1,10]<stderr>:    self.model_runner = ModelRunner(
[1,10]<stderr>:                        ^^^^^^^^^^^^
[1,10]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 235, in __init__
[1,10]<stderr>:    min_per_gpu_memory = self.init_torch_distributed()
[1,10]<stderr>:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,10]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 547, in init_torch_distributed
[1,10]<stderr>:    torch.get_device_module(self.device).set_device(self.gpu_id)
[1,10]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/cuda/__init__.py", line 569, in set_device
[1,10]<stderr>:    torch._C._cuda_setDevice(device)
[1,10]<stderr>:torch.AcceleratorError: CUDA error: invalid device ordinal
[1,10]<stderr>:CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
[1,10]<stderr>:For debugging consider passing CUDA_LAUNCH_BLOCKING=1
[1,10]<stderr>:Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
[1,10]<stderr>:
[1,10]<stderr>:
[1,10]<stderr>:[2025-10-07 12:07:55] Received sigquit from a child process. It usually means the child failed.
[1,5]<stderr>:W1007 12:07:55.067000 871258 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[1,5]<stderr>:W1007 12:07:55.067000 871258 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[1,1]<stderr>:W1007 12:07:55.075000 871438 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[1,1]<stderr>:W1007 12:07:55.075000 871438 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[1,3]<stderr>:W1007 12:07:55.236000 871379 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[1,3]<stderr>:W1007 12:07:55.236000 871379 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[1,8]<stderr>:[2025-10-07 12:07:55 TP0] Attention backend not explicitly specified. Use fa3 backend by default.
[1,8]<stderr>:[2025-10-07 12:07:55 TP0] Chunked prefix cache is turned on.
[1,8]<stderr>:[2025-10-07 12:07:55 TP0] Init torch distributed begin.
[1,15]<stderr>:The target application terminated. One or more process it created re-parented.
[1,15]<stderr>:Waiting for termination of re-parented processes.
[1,15]<stderr>:Use the `--wait` option to modify this behavior.
[1,1]<stderr>:W1007 12:07:55.449000 871440 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[1,1]<stderr>:W1007 12:07:55.449000 871440 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[1,4]<stderr>:W1007 12:07:55.453000 871689 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[1,4]<stderr>:W1007 12:07:55.453000 871689 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[1,12]<stderr>:The target application terminated. One or more process it created re-parented.
[1,12]<stderr>:Waiting for termination of re-parented processes.
[1,12]<stderr>:Use the `--wait` option to modify this behavior.
[1,1]<stderr>:W1007 12:07:55.494000 871437 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[1,1]<stderr>:W1007 12:07:55.494000 871437 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[1,5]<stderr>:W1007 12:07:55.516000 871260 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[1,5]<stderr>:W1007 12:07:55.516000 871260 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[1,10]<stderr>:The target application terminated. One or more process it created re-parented.
[1,10]<stderr>:Waiting for termination of re-parented processes.
[1,10]<stderr>:Use the `--wait` option to modify this behavior.
[1,8]<stderr>:[2025-10-07 12:07:55 TP2] Context: self.device='cuda' self.gpu_id=2 os.environ.get('CUDA_VISIBLE_DEVICES')='0' self.tp_rank=2 self.tp_size=8
[1,8]<stderr>:[2025-10-07 12:07:55 TP2] Scheduler hit an exception: Traceback (most recent call last):
[1,8]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2555, in run_scheduler_process
[1,8]<stderr>:    scheduler = Scheduler(
[1,8]<stderr>:                ^^^^^^^^^^
[1,8]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 315, in __init__
[1,8]<stderr>:    self.tp_worker = TpWorkerClass(
[1,8]<stderr>:                     ^^^^^^^^^^^^^^
[1,8]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/tp_worker_overlap_thread.py", line 67, in __init__
[1,8]<stderr>:    self.worker = TpModelWorker(
[1,8]<stderr>:                  ^^^^^^^^^^^^^^
[1,8]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 84, in __init__
[1,8]<stderr>:    self.model_runner = ModelRunner(
[1,8]<stderr>:                        ^^^^^^^^^^^^
[1,8]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 235, in __init__
[1,8]<stderr>:    min_per_gpu_memory = self.init_torch_distributed()
[1,8]<stderr>:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,8]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 547, in init_torch_distributed
[1,8]<stderr>:    torch.get_device_module(self.device).set_device(self.gpu_id)
[1,8]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/cuda/__init__.py", line 569, in set_device
[1,8]<stderr>:    torch._C._cuda_setDevice(device)
[1,8]<stderr>:torch.AcceleratorError: CUDA error: invalid device ordinal
[1,8]<stderr>:CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
[1,8]<stderr>:For debugging consider passing CUDA_LAUNCH_BLOCKING=1
[1,8]<stderr>:Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
[1,8]<stderr>:
[1,8]<stderr>:
[1,8]<stderr>:[2025-10-07 12:07:55] Received sigquit from a child process. It usually means the child failed.
[1,3]<stderr>:W1007 12:07:55.779000 871380 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[1,3]<stderr>:W1007 12:07:55.779000 871380 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[1,2]<stderr>:W1007 12:07:55.862000 871383 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[1,2]<stderr>:W1007 12:07:55.862000 871383 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[1,6]<stderr>:W1007 12:07:55.940000 871434 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[1,6]<stderr>:W1007 12:07:55.940000 871434 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[1,2]<stderr>:W1007 12:07:56.004000 871384 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[1,2]<stderr>:W1007 12:07:56.004000 871384 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[1,6]<stderr>:W1007 12:07:56.036000 871433 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[1,6]<stderr>:W1007 12:07:56.036000 871433 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[1,6]<stderr>:W1007 12:07:56.092000 871432 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[1,6]<stderr>:W1007 12:07:56.092000 871432 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[1,2]<stderr>:W1007 12:07:56.103000 871386 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[1,2]<stderr>:W1007 12:07:56.103000 871386 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[1,2]<stderr>:W1007 12:07:56.107000 871385 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[1,2]<stderr>:W1007 12:07:56.107000 871385 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[1,3]<stderr>:W1007 12:07:56.121000 871377 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[1,3]<stderr>:W1007 12:07:56.121000 871377 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[1,4]<stderr>:W1007 12:07:56.121000 871692 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[1,4]<stderr>:W1007 12:07:56.121000 871692 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[1,11]<stderr>:[2025-10-07 12:07:56 TP6] Context: self.device='cuda' self.gpu_id=2 os.environ.get('CUDA_VISIBLE_DEVICES')='3' self.tp_rank=6 self.tp_size=8
[1,11]<stderr>:[2025-10-07 12:07:56 TP6] Scheduler hit an exception: Traceback (most recent call last):
[1,11]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2555, in run_scheduler_process
[1,11]<stderr>:    scheduler = Scheduler(
[1,11]<stderr>:                ^^^^^^^^^^
[1,11]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 315, in __init__
[1,11]<stderr>:    self.tp_worker = TpWorkerClass(
[1,11]<stderr>:                     ^^^^^^^^^^^^^^
[1,11]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/tp_worker_overlap_thread.py", line 67, in __init__
[1,11]<stderr>:    self.worker = TpModelWorker(
[1,11]<stderr>:                  ^^^^^^^^^^^^^^
[1,11]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 84, in __init__
[1,11]<stderr>:    self.model_runner = ModelRunner(
[1,11]<stderr>:                        ^^^^^^^^^^^^
[1,11]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 235, in __init__
[1,11]<stderr>:    min_per_gpu_memory = self.init_torch_distributed()
[1,11]<stderr>:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,11]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 547, in init_torch_distributed
[1,11]<stderr>:    torch.get_device_module(self.device).set_device(self.gpu_id)
[1,11]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/cuda/__init__.py", line 569, in set_device
[1,11]<stderr>:    torch._C._cuda_setDevice(device)
[1,11]<stderr>:torch.AcceleratorError: CUDA error: invalid device ordinal
[1,11]<stderr>:CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
[1,11]<stderr>:For debugging consider passing CUDA_LAUNCH_BLOCKING=1
[1,11]<stderr>:Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
[1,11]<stderr>:
[1,11]<stderr>:
[1,11]<stderr>:[2025-10-07 12:07:56] Received sigquit from a child process. It usually means the child failed.
[1,4]<stderr>:W1007 12:07:56.156000 871690 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[1,4]<stderr>:W1007 12:07:56.156000 871690 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[1,8]<stderr>:The target application terminated. One or more process it created re-parented.
[1,8]<stderr>:Waiting for termination of re-parented processes.
[1,8]<stderr>:Use the `--wait` option to modify this behavior.
[1,3]<stderr>:W1007 12:07:56.215000 871378 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[1,3]<stderr>:W1007 12:07:56.215000 871378 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[1,6]<stderr>:W1007 12:07:56.291000 871435 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[1,6]<stderr>:W1007 12:07:56.291000 871435 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[1,4]<stderr>:W1007 12:07:56.349000 871691 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[1,4]<stderr>:W1007 12:07:56.349000 871691 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[1,1]<stderr>:W1007 12:07:56.356000 871439 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[1,1]<stderr>:W1007 12:07:56.356000 871439 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
[1,11]<stderr>:The target application terminated. One or more process it created re-parented.
[1,11]<stderr>:Waiting for termination of re-parented processes.
[1,11]<stderr>:Use the `--wait` option to modify this behavior.
[1,5]<stderr>:[2025-10-07 12:07:57 TP5] Context: self.device='cuda' self.gpu_id=1 os.environ.get('CUDA_VISIBLE_DEVICES')='5' self.tp_rank=5 self.tp_size=8
[1,5]<stderr>:[2025-10-07 12:07:57 TP5] Scheduler hit an exception: Traceback (most recent call last):
[1,5]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2555, in run_scheduler_process
[1,5]<stderr>:    scheduler = Scheduler(
[1,5]<stderr>:                ^^^^^^^^^^
[1,5]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 315, in __init__
[1,5]<stderr>:    self.tp_worker = TpWorkerClass(
[1,5]<stderr>:                     ^^^^^^^^^^^^^^
[1,5]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/tp_worker_overlap_thread.py", line 67, in __init__
[1,5]<stderr>:    self.worker = TpModelWorker(
[1,5]<stderr>:                  ^^^^^^^^^^^^^^
[1,5]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 84, in __init__
[1,5]<stderr>:    self.model_runner = ModelRunner(
[1,5]<stderr>:                        ^^^^^^^^^^^^
[1,5]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 235, in __init__
[1,5]<stderr>:    min_per_gpu_memory = self.init_torch_distributed()
[1,5]<stderr>:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,5]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 547, in init_torch_distributed
[1,5]<stderr>:    torch.get_device_module(self.device).set_device(self.gpu_id)
[1,5]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/cuda/__init__.py", line 569, in set_device
[1,5]<stderr>:    torch._C._cuda_setDevice(device)
[1,5]<stderr>:torch.AcceleratorError: CUDA error: invalid device ordinal
[1,5]<stderr>:CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
[1,5]<stderr>:For debugging consider passing CUDA_LAUNCH_BLOCKING=1
[1,5]<stderr>:Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
[1,5]<stderr>:
[1,5]<stderr>:
[1,5]<stderr>:[2025-10-07 12:07:57] Received sigquit from a child process. It usually means the child failed.
[1,5]<stderr>:The target application terminated. One or more process it created re-parented.
[1,5]<stderr>:Waiting for termination of re-parented processes.
[1,5]<stderr>:Use the `--wait` option to modify this behavior.
[1,1]<stderr>:[2025-10-07 12:07:59 TP5] Context: self.device='cuda' self.gpu_id=1 os.environ.get('CUDA_VISIBLE_DEVICES')='1' self.tp_rank=5 self.tp_size=8
[1,1]<stderr>:[2025-10-07 12:07:59 TP5] Scheduler hit an exception: Traceback (most recent call last):
[1,1]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2555, in run_scheduler_process
[1,1]<stderr>:    scheduler = Scheduler(
[1,1]<stderr>:                ^^^^^^^^^^
[1,1]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 315, in __init__
[1,1]<stderr>:    self.tp_worker = TpWorkerClass(
[1,1]<stderr>:                     ^^^^^^^^^^^^^^
[1,1]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/tp_worker_overlap_thread.py", line 67, in __init__
[1,1]<stderr>:    self.worker = TpModelWorker(
[1,1]<stderr>:                  ^^^^^^^^^^^^^^
[1,1]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 84, in __init__
[1,1]<stderr>:    self.model_runner = ModelRunner(
[1,1]<stderr>:                        ^^^^^^^^^^^^
[1,1]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 235, in __init__
[1,1]<stderr>:    min_per_gpu_memory = self.init_torch_distributed()
[1,1]<stderr>:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 547, in init_torch_distributed
[1,1]<stderr>:    torch.get_device_module(self.device).set_device(self.gpu_id)
[1,1]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/cuda/__init__.py", line 569, in set_device
[1,1]<stderr>:    torch._C._cuda_setDevice(device)
[1,1]<stderr>:torch.AcceleratorError: CUDA error: invalid device ordinal
[1,1]<stderr>:CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
[1,1]<stderr>:For debugging consider passing CUDA_LAUNCH_BLOCKING=1
[1,1]<stderr>:Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
[1,1]<stderr>:
[1,1]<stderr>:
[1,1]<stderr>:[2025-10-07 12:07:59] Received sigquit from a child process. It usually means the child failed.
[1,3]<stderr>:[2025-10-07 12:08:00 TP6] Context: self.device='cuda' self.gpu_id=2 os.environ.get('CUDA_VISIBLE_DEVICES')='3' self.tp_rank=6 self.tp_size=8
[1,3]<stderr>:[2025-10-07 12:08:00 TP6] Scheduler hit an exception: Traceback (most recent call last):
[1,3]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2555, in run_scheduler_process
[1,3]<stderr>:    scheduler = Scheduler(
[1,3]<stderr>:                ^^^^^^^^^^
[1,3]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 315, in __init__
[1,3]<stderr>:    self.tp_worker = TpWorkerClass(
[1,3]<stderr>:                     ^^^^^^^^^^^^^^
[1,3]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/tp_worker_overlap_thread.py", line 67, in __init__
[1,3]<stderr>:    self.worker = TpModelWorker(
[1,3]<stderr>:                  ^^^^^^^^^^^^^^
[1,3]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 84, in __init__
[1,3]<stderr>:    self.model_runner = ModelRunner(
[1,3]<stderr>:                        ^^^^^^^^^^^^
[1,3]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 235, in __init__
[1,3]<stderr>:    min_per_gpu_memory = self.init_torch_distributed()
[1,3]<stderr>:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,3]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 547, in init_torch_distributed
[1,3]<stderr>:    torch.get_device_module(self.device).set_device(self.gpu_id)
[1,3]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/cuda/__init__.py", line 569, in set_device
[1,3]<stderr>:    torch._C._cuda_setDevice(device)
[1,3]<stderr>:torch.AcceleratorError: CUDA error: invalid device ordinal
[1,3]<stderr>:CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
[1,3]<stderr>:For debugging consider passing CUDA_LAUNCH_BLOCKING=1
[1,3]<stderr>:Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
[1,3]<stderr>:
[1,3]<stderr>:
[1,3]<stderr>:[2025-10-07 12:08:00] Received sigquit from a child process. It usually means the child failed.
[1,4]<stderr>:[2025-10-07 12:08:00 TP0] Attention backend not explicitly specified. Use fa3 backend by default.
[1,4]<stderr>:[2025-10-07 12:08:00 TP0] Chunked prefix cache is turned on.
[1,4]<stderr>:[2025-10-07 12:08:00 TP0] Init torch distributed begin.
[1,13]<stdout>:Generating '/raid/pbs.95030.pbs111/nsys-report-e916.qdstrm'
[1,6]<stderr>:[2025-10-07 12:08:00 TP2] Context: self.device='cuda' self.gpu_id=2 os.environ.get('CUDA_VISIBLE_DEVICES')='6' self.tp_rank=2 self.tp_size=8
[1,6]<stderr>:[2025-10-07 12:08:00 TP2] Scheduler hit an exception: Traceback (most recent call last):
[1,6]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2555, in run_scheduler_process
[1,6]<stderr>:    scheduler = Scheduler(
[1,6]<stderr>:                ^^^^^^^^^^
[1,6]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 315, in __init__
[1,6]<stderr>:    self.tp_worker = TpWorkerClass(
[1,6]<stderr>:                     ^^^^^^^^^^^^^^
[1,6]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/tp_worker_overlap_thread.py", line 67, in __init__
[1,6]<stderr>:    self.worker = TpModelWorker(
[1,6]<stderr>:                  ^^^^^^^^^^^^^^
[1,6]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 84, in __init__
[1,6]<stderr>:    self.model_runner = ModelRunner(
[1,6]<stderr>:                        ^^^^^^^^^^^^
[1,6]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 235, in __init__
[1,6]<stderr>:    min_per_gpu_memory = self.init_torch_distributed()
[1,6]<stderr>:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,6]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 547, in init_torch_distributed
[1,6]<stderr>:    torch.get_device_module(self.device).set_device(self.gpu_id)
[1,6]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/cuda/__init__.py", line 569, in set_device
[1,6]<stderr>:    torch._C._cuda_setDevice(device)
[1,6]<stderr>:torch.AcceleratorError: CUDA error: invalid device ordinal
[1,6]<stderr>:CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
[1,6]<stderr>:For debugging consider passing CUDA_LAUNCH_BLOCKING=1
[1,6]<stderr>:Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
[1,6]<stderr>:
[1,6]<stderr>:
[1,6]<stderr>:[2025-10-07 12:08:00] Received sigquit from a child process. It usually means the child failed.
[1,2]<stderr>:[2025-10-07 12:08:00 TP0] Attention backend not explicitly specified. Use fa3 backend by default.
[1,2]<stderr>:[2025-10-07 12:08:00 TP0] Chunked prefix cache is turned on.
[1,2]<stderr>:[2025-10-07 12:08:00 TP0] Init torch distributed begin.
[1,3]<stderr>:The target application terminated. One or more process it created re-parented.
[1,3]<stderr>:Waiting for termination of re-parented processes.
[1,3]<stderr>:Use the `--wait` option to modify this behavior.
[1,6]<stderr>:[2025-10-07 12:08:00 TP0] Attention backend not explicitly specified. Use fa3 backend by default.
[1,6]<stderr>:[2025-10-07 12:08:00 TP0] Chunked prefix cache is turned on.
[1,6]<stderr>:[2025-10-07 12:08:00 TP0] Init torch distributed begin.
[1,4]<stderr>:[2025-10-07 12:08:00 TP3] Context: self.device='cuda' self.gpu_id=3 os.environ.get('CUDA_VISIBLE_DEVICES')='4' self.tp_rank=3 self.tp_size=8
[1,6]<stderr>:[2025-10-07 12:08:00 TP1] Context: self.device='cuda' self.gpu_id=1 os.environ.get('CUDA_VISIBLE_DEVICES')='6' self.tp_rank=1 self.tp_size=8
[1,4]<stderr>:[2025-10-07 12:08:00 TP3] Scheduler hit an exception: Traceback (most recent call last):
[1,4]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2555, in run_scheduler_process
[1,4]<stderr>:    scheduler = Scheduler(
[1,4]<stderr>:                ^^^^^^^^^^
[1,4]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 315, in __init__
[1,4]<stderr>:    self.tp_worker = TpWorkerClass(
[1,4]<stderr>:                     ^^^^^^^^^^^^^^
[1,4]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/tp_worker_overlap_thread.py", line 67, in __init__
[1,4]<stderr>:    self.worker = TpModelWorker(
[1,4]<stderr>:                  ^^^^^^^^^^^^^^
[1,4]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 84, in __init__
[1,4]<stderr>:    self.model_runner = ModelRunner(
[1,4]<stderr>:                        ^^^^^^^^^^^^
[1,4]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 235, in __init__
[1,4]<stderr>:    min_per_gpu_memory = self.init_torch_distributed()
[1,4]<stderr>:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,4]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 547, in init_torch_distributed
[1,4]<stderr>:    torch.get_device_module(self.device).set_device(self.gpu_id)
[1,4]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/cuda/__init__.py", line 569, in set_device
[1,4]<stderr>:    torch._C._cuda_setDevice(device)
[1,4]<stderr>:torch.AcceleratorError: CUDA error: invalid device ordinal
[1,4]<stderr>:CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
[1,4]<stderr>:For debugging consider passing CUDA_LAUNCH_BLOCKING=1
[1,4]<stderr>:Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
[1,4]<stderr>:
[1,4]<stderr>:
[1,4]<stderr>:[2025-10-07 12:08:00] Received sigquit from a child process. It usually means the child failed.
[1,6]<stderr>:[2025-10-07 12:08:00 TP1] Scheduler hit an exception: Traceback (most recent call last):
[1,6]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2555, in run_scheduler_process
[1,6]<stderr>:    scheduler = Scheduler(
[1,6]<stderr>:                ^^^^^^^^^^
[1,6]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 315, in __init__
[1,6]<stderr>:    self.tp_worker = TpWorkerClass(
[1,6]<stderr>:                     ^^^^^^^^^^^^^^
[1,6]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/tp_worker_overlap_thread.py", line 67, in __init__
[1,6]<stderr>:    self.worker = TpModelWorker(
[1,6]<stderr>:                  ^^^^^^^^^^^^^^
[1,6]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 84, in __init__
[1,6]<stderr>:    self.model_runner = ModelRunner(
[1,6]<stderr>:                        ^^^^^^^^^^^^
[1,6]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 235, in __init__
[1,6]<stderr>:    min_per_gpu_memory = self.init_torch_distributed()
[1,6]<stderr>:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,6]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 547, in init_torch_distributed
[1,6]<stderr>:    torch.get_device_module(self.device).set_device(self.gpu_id)
[1,6]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/cuda/__init__.py", line 569, in set_device
[1,6]<stderr>:    torch._C._cuda_setDevice(device)
[1,6]<stderr>:torch.AcceleratorError: CUDA error: invalid device ordinal
[1,6]<stderr>:CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
[1,6]<stderr>:For debugging consider passing CUDA_LAUNCH_BLOCKING=1
[1,6]<stderr>:Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
[1,6]<stderr>:
[1,6]<stderr>:
[1,6]<stderr>:[2025-10-07 12:08:00] Received sigquit from a child process. It usually means the child failed.
[1,2]<stderr>:[2025-10-07 12:08:00 TP1] Context: self.device='cuda' self.gpu_id=1 os.environ.get('CUDA_VISIBLE_DEVICES')='2' self.tp_rank=1 self.tp_size=8
[1,2]<stderr>:[2025-10-07 12:08:00 TP1] Scheduler hit an exception: Traceback (most recent call last):
[1,2]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2555, in run_scheduler_process
[1,2]<stderr>:    scheduler = Scheduler(
[1,2]<stderr>:                ^^^^^^^^^^
[1,2]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 315, in __init__
[1,2]<stderr>:    self.tp_worker = TpWorkerClass(
[1,2]<stderr>:                     ^^^^^^^^^^^^^^
[1,2]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/tp_worker_overlap_thread.py", line 67, in __init__
[1,2]<stderr>:    self.worker = TpModelWorker(
[1,2]<stderr>:                  ^^^^^^^^^^^^^^
[1,2]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 84, in __init__
[1,2]<stderr>:    self.model_runner = ModelRunner(
[1,2]<stderr>:                        ^^^^^^^^^^^^
[1,2]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 235, in __init__
[1,2]<stderr>:    min_per_gpu_memory = self.init_torch_distributed()
[1,2]<stderr>:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,2]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 547, in init_torch_distributed
[1,2]<stderr>:    torch.get_device_module(self.device).set_device(self.gpu_id)
[1,2]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/cuda/__init__.py", line 569, in set_device
[1,2]<stderr>:    torch._C._cuda_setDevice(device)
[1,2]<stderr>:torch.AcceleratorError: CUDA error: invalid device ordinal
[1,2]<stderr>:CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
[1,2]<stderr>:For debugging consider passing CUDA_LAUNCH_BLOCKING=1
[1,2]<stderr>:Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
[1,2]<stderr>:
[1,2]<stderr>:
[1,2]<stderr>:[2025-10-07 12:08:00] Received sigquit from a child process. It usually means the child failed.
[1,4]<stderr>:[2025-10-07 12:08:00 TP2] Context: self.device='cuda' self.gpu_id=2 os.environ.get('CUDA_VISIBLE_DEVICES')='4' self.tp_rank=2 self.tp_size=8
[1,4]<stderr>:[2025-10-07 12:08:00 TP2] Scheduler hit an exception: Traceback (most recent call last):
[1,4]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2555, in run_scheduler_process
[1,4]<stderr>:    scheduler = Scheduler(
[1,4]<stderr>:                ^^^^^^^^^^
[1,4]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 315, in __init__
[1,4]<stderr>:    self.tp_worker = TpWorkerClass(
[1,4]<stderr>:                     ^^^^^^^^^^^^^^
[1,4]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/tp_worker_overlap_thread.py", line 67, in __init__
[1,4]<stderr>:    self.worker = TpModelWorker(
[1,4]<stderr>:                  ^^^^^^^^^^^^^^
[1,4]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 84, in __init__
[1,4]<stderr>:    self.model_runner = ModelRunner(
[1,4]<stderr>:                        ^^^^^^^^^^^^
[1,4]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 235, in __init__
[1,4]<stderr>:    min_per_gpu_memory = self.init_torch_distributed()
[1,4]<stderr>:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,4]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 547, in init_torch_distributed
[1,4]<stderr>:    torch.get_device_module(self.device).set_device(self.gpu_id)
[1,4]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/cuda/__init__.py", line 569, in set_device
[1,4]<stderr>:    torch._C._cuda_setDevice(device)
[1,4]<stderr>:torch.AcceleratorError: CUDA error: invalid device ordinal
[1,4]<stderr>:CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
[1,4]<stderr>:For debugging consider passing CUDA_LAUNCH_BLOCKING=1
[1,4]<stderr>:Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
[1,4]<stderr>:
[1,4]<stderr>:
[1,4]<stderr>:[2025-10-07 12:08:00] Received sigquit from a child process. It usually means the child failed.
[1,6]<stderr>:[2025-10-07 12:08:00 TP3] Context: self.device='cuda' self.gpu_id=3 os.environ.get('CUDA_VISIBLE_DEVICES')='6' self.tp_rank=3 self.tp_size=8
[1,6]<stderr>:[2025-10-07 12:08:00 TP3] Scheduler hit an exception: Traceback (most recent call last):
[1,6]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2555, in run_scheduler_process
[1,6]<stderr>:    scheduler = Scheduler(
[1,6]<stderr>:                ^^^^^^^^^^
[1,6]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 315, in __init__
[1,6]<stderr>:    self.tp_worker = TpWorkerClass(
[1,6]<stderr>:                     ^^^^^^^^^^^^^^
[1,6]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/tp_worker_overlap_thread.py", line 67, in __init__
[1,6]<stderr>:    self.worker = TpModelWorker(
[1,6]<stderr>:                  ^^^^^^^^^^^^^^
[1,6]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 84, in __init__
[1,6]<stderr>:    self.model_runner = ModelRunner(
[1,6]<stderr>:                        ^^^^^^^^^^^^
[1,6]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 235, in __init__
[1,6]<stderr>:    min_per_gpu_memory = self.init_torch_distributed()
[1,6]<stderr>:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,6]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 547, in init_torch_distributed
[1,6]<stderr>:    torch.get_device_module(self.device).set_device(self.gpu_id)
[1,6]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/cuda/__init__.py", line 569, in set_device
[1,6]<stderr>:    torch._C._cuda_setDevice(device)
[1,6]<stderr>:torch.AcceleratorError: CUDA error: invalid device ordinal
[1,6]<stderr>:CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
[1,6]<stderr>:For debugging consider passing CUDA_LAUNCH_BLOCKING=1
[1,6]<stderr>:Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
[1,6]<stderr>:
[1,6]<stderr>:
[1,6]<stderr>:[2025-10-07 12:08:00] Received sigquit from a child process. It usually means the child failed.
[1,2]<stderr>:[2025-10-07 12:08:00 TP2] Context: self.device='cuda' self.gpu_id=2 os.environ.get('CUDA_VISIBLE_DEVICES')='2' self.tp_rank=2 self.tp_size=8
[1,2]<stderr>:[2025-10-07 12:08:00 TP3] Context: self.device='cuda' self.gpu_id=3 os.environ.get('CUDA_VISIBLE_DEVICES')='2' self.tp_rank=3 self.tp_size=8
[1,2]<stderr>:[2025-10-07 12:08:00 TP2] Scheduler hit an exception: Traceback (most recent call last):
[1,2]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2555, in run_scheduler_process
[1,2]<stderr>:    scheduler = Scheduler(
[1,2]<stderr>:                ^^^^^^^^^^
[1,2]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 315, in __init__
[1,2]<stderr>:    self.tp_worker = TpWorkerClass(
[1,2]<stderr>:                     ^^^^^^^^^^^^^^
[1,2]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/tp_worker_overlap_thread.py", line 67, in __init__
[1,2]<stderr>:    self.worker = TpModelWorker(
[1,2]<stderr>:                  ^^^^^^^^^^^^^^
[1,2]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 84, in __init__
[1,2]<stderr>:    self.model_runner = ModelRunner(
[1,2]<stderr>:                        ^^^^^^^^^^^^
[1,2]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 235, in __init__
[1,2]<stderr>:    min_per_gpu_memory = self.init_torch_distributed()
[1,2]<stderr>:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,2]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 547, in init_torch_distributed
[1,2]<stderr>:    torch.get_device_module(self.device).set_device(self.gpu_id)
[1,2]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/cuda/__init__.py", line 569, in set_device
[1,2]<stderr>:    torch._C._cuda_setDevice(device)
[1,2]<stderr>:torch.AcceleratorError: CUDA error: invalid device ordinal
[1,2]<stderr>:CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
[1,2]<stderr>:For debugging consider passing CUDA_LAUNCH_BLOCKING=1
[1,2]<stderr>:Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
[1,2]<stderr>:
[1,2]<stderr>:
[1,2]<stderr>:[2025-10-07 12:08:00] Received sigquit from a child process. It usually means the child failed.
[1,2]<stderr>:[2025-10-07 12:08:00 TP3] Scheduler hit an exception: Traceback (most recent call last):
[1,2]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2555, in run_scheduler_process
[1,2]<stderr>:    scheduler = Scheduler(
[1,2]<stderr>:                ^^^^^^^^^^
[1,2]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 315, in __init__
[1,2]<stderr>:    self.tp_worker = TpWorkerClass(
[1,2]<stderr>:                     ^^^^^^^^^^^^^^
[1,2]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/tp_worker_overlap_thread.py", line 67, in __init__
[1,2]<stderr>:    self.worker = TpModelWorker(
[1,2]<stderr>:                  ^^^^^^^^^^^^^^
[1,2]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 84, in __init__
[1,2]<stderr>:    self.model_runner = ModelRunner(
[1,2]<stderr>:                        ^^^^^^^^^^^^
[1,2]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 235, in __init__
[1,2]<stderr>:    min_per_gpu_memory = self.init_torch_distributed()
[1,2]<stderr>:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,2]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 547, in init_torch_distributed
[1,2]<stderr>:    torch.get_device_module(self.device).set_device(self.gpu_id)
[1,2]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/cuda/__init__.py", line 569, in set_device
[1,2]<stderr>:    torch._C._cuda_setDevice(device)
[1,2]<stderr>:torch.AcceleratorError: CUDA error: invalid device ordinal
[1,2]<stderr>:CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
[1,2]<stderr>:For debugging consider passing CUDA_LAUNCH_BLOCKING=1
[1,2]<stderr>:Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
[1,2]<stderr>:
[1,2]<stderr>:
[1,4]<stderr>:[2025-10-07 12:08:00 TP1] Context: self.device='cuda' self.gpu_id=1 os.environ.get('CUDA_VISIBLE_DEVICES')='4' self.tp_rank=1 self.tp_size=8
[1,2]<stderr>:[2025-10-07 12:08:00] Received sigquit from a child process. It usually means the child failed.
[1,4]<stderr>:[2025-10-07 12:08:00 TP1] Scheduler hit an exception: Traceback (most recent call last):
[1,4]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2555, in run_scheduler_process
[1,4]<stderr>:    scheduler = Scheduler(
[1,4]<stderr>:                ^^^^^^^^^^
[1,4]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 315, in __init__
[1,4]<stderr>:    self.tp_worker = TpWorkerClass(
[1,4]<stderr>:                     ^^^^^^^^^^^^^^
[1,4]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/tp_worker_overlap_thread.py", line 67, in __init__
[1,4]<stderr>:    self.worker = TpModelWorker(
[1,4]<stderr>:                  ^^^^^^^^^^^^^^
[1,4]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 84, in __init__
[1,4]<stderr>:    self.model_runner = ModelRunner(
[1,4]<stderr>:                        ^^^^^^^^^^^^
[1,4]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 235, in __init__
[1,4]<stderr>:    min_per_gpu_memory = self.init_torch_distributed()
[1,4]<stderr>:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,4]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 547, in init_torch_distributed
[1,4]<stderr>:    torch.get_device_module(self.device).set_device(self.gpu_id)
[1,4]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/cuda/__init__.py", line 569, in set_device
[1,4]<stderr>:    torch._C._cuda_setDevice(device)
[1,4]<stderr>:torch.AcceleratorError: CUDA error: invalid device ordinal
[1,4]<stderr>:CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
[1,4]<stderr>:For debugging consider passing CUDA_LAUNCH_BLOCKING=1
[1,4]<stderr>:Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
[1,4]<stderr>:
[1,4]<stderr>:
[1,4]<stderr>:[2025-10-07 12:08:00] Received sigquit from a child process. It usually means the child failed.
[1,2]<stderr>:The target application terminated. One or more process it created re-parented.
[1,2]<stderr>:Waiting for termination of re-parented processes.
[1,2]<stderr>:Use the `--wait` option to modify this behavior.
[1,4]<stderr>:The target application terminated. One or more process it created re-parented.
[1,4]<stderr>:Waiting for termination of re-parented processes.
[1,4]<stderr>:Use the `--wait` option to modify this behavior.
[1,6]<stderr>:The target application terminated. One or more process it created re-parented.
[1,6]<stderr>:Waiting for termination of re-parented processes.
[1,6]<stderr>:Use the `--wait` option to modify this behavior.
[1,9]<stdout>:Generating '/raid/pbs.95030.pbs111/nsys-report-cfeb.qdstrm'
[1,14]<stdout>:Generating '/raid/pbs.95030.pbs111/nsys-report-176c.qdstrm'
[1,12]<stdout>:Generating '/raid/pbs.95030.pbs111/nsys-report-8813.qdstrm'
[1,10]<stdout>:Generating '/raid/pbs.95030.pbs111/nsys-report-3b0c.qdstrm'
[1,15]<stdout>:Generating '/raid/pbs.95030.pbs111/nsys-report-4a1b.qdstrm'
[1,7]<stdout>:Generating '/raid/pbs.95030.pbs111/nsys-report-3e38.qdstrm'
[1,0]<stdout>:Generating '/raid/pbs.95030.pbs111/nsys-report-1333.qdstrm'
[1,11]<stdout>:Generating '/raid/pbs.95030.pbs111/nsys-report-b9af.qdstrm'
[1,5]<stdout>:Generating '/raid/pbs.95030.pbs111/nsys-report-3296.qdstrm'
[1,8]<stdout>:Generating '/raid/pbs.95030.pbs111/nsys-report-61c8.qdstrm'
[1,1]<stdout>:Generating '/raid/pbs.95030.pbs111/nsys-report-89f0.qdstrm'
[1,8]<stdout>:[1/1] [0%                          ] sglang_95030.pbs111_rank8.nsys-rep[1,12]<stdout>:[1/1] [0%                          ] sglang_95030.pbs111_rank12.nsys-rep[1,10]<stdout>:[1/1] [0%                          ] sglang_95030.pbs111_rank10.nsys-rep[1,13]<stdout>:[1/1] [0%                          ] sglang_95030.pbs111_rank13.nsys-rep[1,14]<stdout>:[1/1] [0%                          ] sglang_95030.pbs111_rank14.nsys-rep[1,11]<stdout>:[1/1] [0%                          ] sglang_95030.pbs111_rank11.nsys-rep[1,15]<stdout>:[1/1] [0%                          ] sglang_95030.pbs111_rank15.nsys-rep[1,9]<stdout>:[1/1] [0%                          ] sglang_95030.pbs111_rank9.nsys-rep[1,0]<stdout>:[1/1] [0%                          ] sglang_95030.pbs111_rank0.nsys-rep[1,5]<stdout>:[1/1] [0%                          ] sglang_95030.pbs111_rank5.nsys-rep[1,7]<stdout>:[1/1] [0%                          ] sglang_95030.pbs111_rank7.nsys-rep[1,1]<stdout>:[1/1] [0%                          ] sglang_95030.pbs111_rank1.nsys-rep[1,0]<stdout>:[1/1] [0%                          ] sglang_95030.pbs111_rank0.nsys-rep[1,13]<stdout>:[1/1] [0%                          ] sglang_95030.pbs111_rank13.nsys-rep[1,8]<stdout>:[1/1] [0%                          ] sglang_95030.pbs111_rank8.nsys-rep[1,15]<stdout>:[1/1] [0%                          ] sglang_95030.pbs111_rank15.nsys-rep[1,9]<stdout>:[1/1] [0%                          ] sglang_95030.pbs111_rank9.nsys-rep[1,12]<stdout>:[1/1] [0%                          ] sglang_95030.pbs111_rank12.nsys-rep[1,5]<stdout>:[1/1] [0%                          ] sglang_95030.pbs111_rank5.nsys-rep[1,11]<stdout>:[1/1] [0%                          ] sglang_95030.pbs111_rank11.nsys-rep[1,10]<stdout>:[1/1] [0%                          ] sglang_95030.pbs111_rank10.nsys-rep[1,14]<stdout>:[1/1] [0%                          ] sglang_95030.pbs111_rank14.nsys-rep[1,7]<stdout>:[1/1] [0%                          ] sglang_95030.pbs111_rank7.nsys-rep[1,3]<stdout>:Generating '/raid/pbs.95030.pbs111/nsys-report-dc89.qdstrm'
[1,1]<stdout>:[1/1] [0%                          ] sglang_95030.pbs111_rank1.nsys-rep[1,1]<stdout>:[1/1] [5%                          ] sglang_95030.pbs111_rank1.nsys-rep[1,1]<stdout>:[1/1] [5%                          ] sglang_95030.pbs111_rank1.nsys-rep[1,1]<stdout>:[1/1] [6%                          ] sglang_95030.pbs111_rank1.nsys-rep[1,1]<stdout>:[1/1] [7%                          ] sglang_95030.pbs111_rank1.nsys-rep[1,1]<stdout>:[1/1] [6%                          ] sglang_95030.pbs111_rank1.nsys-rep[1,1]<stdout>:[1/1] [7%                          ] sglang_95030.pbs111_rank1.nsys-rep[1,1]<stdout>:[1/1] [8%                          ] sglang_95030.pbs111_rank1.nsys-rep[1,1]<stdout>:[1/1] [7%                          ] sglang_95030.pbs111_rank1.nsys-rep[1,1]<stdout>:[1/1] [6%                          ] sglang_95030.pbs111_rank1.nsys-rep[1,1]<stdout>:[1/1] [7%                          ] sglang_95030.pbs111_rank1.nsys-rep[1,1]<stdout>:[1/1] [8%                          ] sglang_95030.pbs111_rank1.nsys-rep[1,1]<stdout>:[1/1] [7%                          ] sglang_95030.pbs111_rank1.nsys-rep[1,1]<stdout>:[1/1] [8%                          ] sglang_95030.pbs111_rank1.nsys-rep[1,1]<stdout>:[1/1] [9%                          ] sglang_95030.pbs111_rank1.nsys-rep[1,1]<stdout>:[1/1] [8%                          ] sglang_95030.pbs111_rank1.nsys-rep[1,1]<stdout>:[1/1] [9%                          ] sglang_95030.pbs111_rank1.nsys-rep[1,1]<stdout>:[1/1] [10%                         ] sglang_95030.pbs111_rank1.nsys-rep[1,1]<stdout>:[1/1] [9%                          ] sglang_95030.pbs111_rank1.nsys-rep[1,1]<stdout>:[1/1] [8%                          ] sglang_95030.pbs111_rank1.nsys-rep[1,1]<stdout>:[1/1] [7%                          ] sglang_95030.pbs111_rank1.nsys-rep[1,1]<stdout>:[1/1] [6%                          ] sglang_95030.pbs111_rank1.nsys-rep[1,1]<stdout>:[1/1] [5%                          ] sglang_95030.pbs111_rank1.nsys-rep[1,2]<stdout>:Generating '/raid/pbs.95030.pbs111/nsys-report-40bf.qdstrm'
[1,3]<stdout>:[1/1] [0%                          ] sglang_95030.pbs111_rank3.nsys-rep[1,6]<stdout>:Generating '/raid/pbs.95030.pbs111/nsys-report-ee84.qdstrm'
[1,4]<stdout>:Generating '/raid/pbs.95030.pbs111/nsys-report-8708.qdstrm'
[1,3]<stdout>:[1/1] [0%                          ] sglang_95030.pbs111_rank3.nsys-rep[1,3]<stdout>:[1/1] [5%                          ] sglang_95030.pbs111_rank3.nsys-rep[1,3]<stdout>:[1/1] [5%                          ] sglang_95030.pbs111_rank3.nsys-rep[1,3]<stdout>:[1/1] [6%                          ] sglang_95030.pbs111_rank3.nsys-rep[1,3]<stdout>:[1/1] [5%                          ] sglang_95030.pbs111_rank3.nsys-rep[1,3]<stdout>:[1/1] [6%                          ] sglang_95030.pbs111_rank3.nsys-rep[1,3]<stdout>:[1/1] [7%                          ] sglang_95030.pbs111_rank3.nsys-rep[1,3]<stdout>:[1/1] [6%                          ] sglang_95030.pbs111_rank3.nsys-rep[1,3]<stdout>:[1/1] [7%                          ] sglang_95030.pbs111_rank3.nsys-rep[1,3]<stdout>:[1/1] [8%                          ] sglang_95030.pbs111_rank3.nsys-rep[1,3]<stdout>:[1/1] [7%                          ] sglang_95030.pbs111_rank3.nsys-rep[1,3]<stdout>:[1/1] [6%                          ] sglang_95030.pbs111_rank3.nsys-rep[1,3]<stdout>:[1/1] [7%                          ] sglang_95030.pbs111_rank3.nsys-rep[1,3]<stdout>:[1/1] [8%                          ] sglang_95030.pbs111_rank3.nsys-rep[1,3]<stdout>:[1/1] [7%                          ] sglang_95030.pbs111_rank3.nsys-rep[1,3]<stdout>:[1/1] [8%                          ] sglang_95030.pbs111_rank3.nsys-rep[1,3]<stdout>:[1/1] [9%                          ] sglang_95030.pbs111_rank3.nsys-rep[1,3]<stdout>:[1/1] [10%                         ] sglang_95030.pbs111_rank3.nsys-rep[1,3]<stdout>:[1/1] [9%                          ] sglang_95030.pbs111_rank3.nsys-rep[1,3]<stdout>:[1/1] [10%                         ] sglang_95030.pbs111_rank3.nsys-rep[1,3]<stdout>:[1/1] [9%                          ] sglang_95030.pbs111_rank3.nsys-rep[1,3]<stdout>:[1/1] [8%                          ] sglang_95030.pbs111_rank3.nsys-rep[1,3]<stdout>:[1/1] [7%                          ] sglang_95030.pbs111_rank3.nsys-rep[1,3]<stdout>:[1/1] [6%                          ] sglang_95030.pbs111_rank3.nsys-rep[1,3]<stdout>:[1/1] [5%                          ] sglang_95030.pbs111_rank3.nsys-rep[1,2]<stdout>:[1/1] [0%                          ] sglang_95030.pbs111_rank2.nsys-rep[1,6]<stdout>:[1/1] [0%                          ] sglang_95030.pbs111_rank6.nsys-rep[1,4]<stdout>:[1/1] [0%                          ] sglang_95030.pbs111_rank4.nsys-rep[1,2]<stdout>:[1/1] [0%                          ] sglang_95030.pbs111_rank2.nsys-rep[1,2]<stdout>:[1/1] [5%                          ] sglang_95030.pbs111_rank2.nsys-rep[1,2]<stdout>:[1/1] [6%                          ] sglang_95030.pbs111_rank2.nsys-rep[1,2]<stdout>:[1/1] [7%                          ] sglang_95030.pbs111_rank2.nsys-rep[1,2]<stdout>:[1/1] [8%                          ] sglang_95030.pbs111_rank2.nsys-rep[1,2]<stdout>:[1/1] [7%                          ] sglang_95030.pbs111_rank2.nsys-rep[1,2]<stdout>:[1/1] [8%                          ] sglang_95030.pbs111_rank2.nsys-rep[1,2]<stdout>:[1/1] [7%                          ] sglang_95030.pbs111_rank2.nsys-rep[1,2]<stdout>:[1/1] [8%                          ] sglang_95030.pbs111_rank2.nsys-rep[1,2]<stdout>:[1/1] [9%                          ] sglang_95030.pbs111_rank2.nsys-rep[1,2]<stdout>:[1/1] [8%                          ] sglang_95030.pbs111_rank2.nsys-rep[1,2]<stdout>:[1/1] [9%                          ] sglang_95030.pbs111_rank2.nsys-rep[1,2]<stdout>:[1/1] [10%                         ] sglang_95030.pbs111_rank2.nsys-rep[1,2]<stdout>:[1/1] [9%                          ] sglang_95030.pbs111_rank2.nsys-rep[1,9]<stdout>:[1/1] [5%                          ] sglang_95030.pbs111_rank9.nsys-rep[1,2]<stdout>:[1/1] [10%                         ] sglang_95030.pbs111_rank2.nsys-rep[1,2]<stdout>:[1/1] [9%                          ] sglang_95030.pbs111_rank2.nsys-rep[1,2]<stdout>:[1/1] [8%                          ] sglang_95030.pbs111_rank2.nsys-rep[1,13]<stdout>:[1/1] [5%                          ] sglang_95030.pbs111_rank13.nsys-rep[1,14]<stdout>:[1/1] [5%                          ] sglang_95030.pbs111_rank14.nsys-rep[1,2]<stdout>:[1/1] [7%                          ] sglang_95030.pbs111_rank2.nsys-rep[1,11]<stdout>:[1/1] [5%                          ] sglang_95030.pbs111_rank11.nsys-rep[1,2]<stdout>:[1/1] [6%                          ] sglang_95030.pbs111_rank2.nsys-rep[1,12]<stdout>:[1/1] [5%                          ] sglang_95030.pbs111_rank12.nsys-rep[1,9]<stdout>:[1/1] [6%                          ] sglang_95030.pbs111_rank9.nsys-rep[1,15]<stdout>:[1/1] [5%                          ] sglang_95030.pbs111_rank15.nsys-rep[1,2]<stdout>:[1/1] [5%                          ] sglang_95030.pbs111_rank2.nsys-rep[1,14]<stdout>:[1/1] [6%                          ] sglang_95030.pbs111_rank14.nsys-rep[1,11]<stdout>:[1/1] [6%                          ] sglang_95030.pbs111_rank11.nsys-rep[1,5]<stdout>:[1/1] [5%                          ] sglang_95030.pbs111_rank5.nsys-rep[1,12]<stdout>:[1/1] [6%                          ] sglang_95030.pbs111_rank12.nsys-rep[1,15]<stdout>:[1/1] [6%                          ] sglang_95030.pbs111_rank15.nsys-rep[1,7]<stdout>:[1/1] [5%                          ] sglang_95030.pbs111_rank7.nsys-rep[1,13]<stdout>:[1/1] [6%                          ] sglang_95030.pbs111_rank13.nsys-rep[1,8]<stdout>:[1/1] [5%                          ] sglang_95030.pbs111_rank8.nsys-rep[1,5]<stdout>:[1/1] [6%                          ] sglang_95030.pbs111_rank5.nsys-rep[1,10]<stdout>:[1/1] [5%                          ] sglang_95030.pbs111_rank10.nsys-rep[1,12]<stdout>:[1/1] [7%                          ] sglang_95030.pbs111_rank12.nsys-rep[1,14]<stdout>:[1/1] [7%                          ] sglang_95030.pbs111_rank14.nsys-rep[1,6]<stdout>:[1/1] [0%                          ] sglang_95030.pbs111_rank6.nsys-rep[1,6]<stdout>:[1/1] [5%                          ] sglang_95030.pbs111_rank6.nsys-rep[1,6]<stdout>:[1/1] [6%                          ] sglang_95030.pbs111_rank6.nsys-rep[1,6]<stdout>:[1/1] [7%                          ] sglang_95030.pbs111_rank6.nsys-rep[1,6]<stdout>:[1/1] [6%                          ] sglang_95030.pbs111_rank6.nsys-rep[1,9]<stdout>:[1/1] [7%                          ] sglang_95030.pbs111_rank9.nsys-rep[1,6]<stdout>:[1/1] [7%                          ] sglang_95030.pbs111_rank6.nsys-rep[1,6]<stdout>:[1/1] [8%                          ] sglang_95030.pbs111_rank6.nsys-rep[1,6]<stdout>:[1/1] [7%                          ] sglang_95030.pbs111_rank6.nsys-rep[1,6]<stdout>:[1/1] [8%                          ] sglang_95030.pbs111_rank6.nsys-rep[1,6]<stdout>:[1/1] [7%                          ] sglang_95030.pbs111_rank6.nsys-rep[1,7]<stdout>:[1/1] [6%                          ] sglang_95030.pbs111_rank7.nsys-rep[1,6]<stdout>:[1/1] [8%                          ] sglang_95030.pbs111_rank6.nsys-rep[1,6]<stdout>:[1/1] [7%                          ] sglang_95030.pbs111_rank6.nsys-rep[1,6]<stdout>:[1/1] [8%                          ] sglang_95030.pbs111_rank6.nsys-rep[1,6]<stdout>:[1/1] [9%                          ] sglang_95030.pbs111_rank6.nsys-rep[1,6]<stdout>:[1/1] [10%                         ] sglang_95030.pbs111_rank6.nsys-rep[1,6]<stdout>:[1/1] [11%                         ] sglang_95030.pbs111_rank6.nsys-rep[1,6]<stdout>:[1/1] [10%                         ] sglang_95030.pbs111_rank6.nsys-rep[1,6]<stdout>:[1/1] [9%                          ] sglang_95030.pbs111_rank6.nsys-rep[1,8]<stdout>:[1/1] [6%                          ] sglang_95030.pbs111_rank8.nsys-rep[1,1]<stdout>:[1/1] [5%                          ] sglang_95030.pbs111_rank1.nsys-rep[1,0]<stdout>:[1/1] [5%                          ] sglang_95030.pbs111_rank0.nsys-rep[1,10]<stdout>:[1/1] [6%                          ] sglang_95030.pbs111_rank10.nsys-rep[1,6]<stdout>:[1/1] [8%                          ] sglang_95030.pbs111_rank6.nsys-rep[1,11]<stdout>:[1/1] [7%                          ] sglang_95030.pbs111_rank11.nsys-rep[1,6]<stdout>:[1/1] [7%                          ] sglang_95030.pbs111_rank6.nsys-rep[1,9]<stdout>:[1/1] [8%                          ] sglang_95030.pbs111_rank9.nsys-rep[1,12]<stdout>:[1/1] [8%                          ] sglang_95030.pbs111_rank12.nsys-rep[1,6]<stdout>:[1/1] [6%                          ] sglang_95030.pbs111_rank6.nsys-rep[1,15]<stdout>:[1/1] [7%                          ] sglang_95030.pbs111_rank15.nsys-rep[1,1]<stdout>:[1/1] [6%                          ] sglang_95030.pbs111_rank1.nsys-rep[1,7]<stdout>:[1/1] [7%                          ] sglang_95030.pbs111_rank7.nsys-rep[1,4]<stdout>:[1/1] [0%                          ] sglang_95030.pbs111_rank4.nsys-rep[1,4]<stdout>:[1/1] [5%                          ] sglang_95030.pbs111_rank4.nsys-rep[1,10]<stdout>:[1/1] [7%                          ] sglang_95030.pbs111_rank10.nsys-rep[1,4]<stdout>:[1/1] [5%                          ] sglang_95030.pbs111_rank4.nsys-rep[1,6]<stdout>:[1/1] [5%                          ] sglang_95030.pbs111_rank6.nsys-rep[1,4]<stdout>:[1/1] [6%                          ] sglang_95030.pbs111_rank4.nsys-rep[1,4]<stdout>:[1/1] [5%                          ] sglang_95030.pbs111_rank4.nsys-rep[1,14]<stdout>:[1/1] [8%                          ] sglang_95030.pbs111_rank14.nsys-rep[1,13]<stdout>:[1/1] [7%                          ] sglang_95030.pbs111_rank13.nsys-rep[1,4]<stdout>:[1/1] [6%                          ] sglang_95030.pbs111_rank4.nsys-rep[1,4]<stdout>:[1/1] [7%                          ] sglang_95030.pbs111_rank4.nsys-rep[1,4]<stdout>:[1/1] [8%                          ] sglang_95030.pbs111_rank4.nsys-rep[1,4]<stdout>:[1/1] [7%                          ] sglang_95030.pbs111_rank4.nsys-rep[1,4]<stdout>:[1/1] [6%                          ] sglang_95030.pbs111_rank4.nsys-rep[1,4]<stdout>:[1/1] [7%                          ] sglang_95030.pbs111_rank4.nsys-rep[1,4]<stdout>:[1/1] [8%                          ] sglang_95030.pbs111_rank4.nsys-rep[1,4]<stdout>:[1/1] [9%                          ] sglang_95030.pbs111_rank4.nsys-rep[1,4]<stdout>:[1/1] [8%                          ] sglang_95030.pbs111_rank4.nsys-rep[1,4]<stdout>:[1/1] [9%                          ] sglang_95030.pbs111_rank4.nsys-rep[1,4]<stdout>:[1/1] [10%                         ] sglang_95030.pbs111_rank4.nsys-rep[1/1] [9%                          ] sglang_95030.pbs111_rank4.nsys-rep[1,4]<stdout>:[1/1] [10%                         ] sglang_95030.pbs111_rank4.nsys-rep[1,5]<stdout>:[1/1] [7%                          ] sglang_95030.pbs111_rank5.nsys-rep[1,4]<stdout>:[1/1] [9%                          ] sglang_95030.pbs111_rank4.nsys-rep[1,14]<stdout>:[1/1] [9%                          ] sglang_95030.pbs111_rank14.nsys-rep[1,13]<stdout>:[1/1] [8%                          ] sglang_95030.pbs111_rank13.nsys-rep[1,4]<stdout>:[1/1] [8%                          ] sglang_95030.pbs111_rank4.nsys-rep[1,1]<stdout>:[1/1] [7%                          ] sglang_95030.pbs111_rank1.nsys-rep[1,4]<stdout>:[1/1] [7%                          ] sglang_95030.pbs111_rank4.nsys-rep[1,11]<stdout>:[1/1] [8%                          ] sglang_95030.pbs111_rank11.nsys-rep[1,0]<stdout>:[1/1] [6%                          ] sglang_95030.pbs111_rank0.nsys-rep[1,12]<stdout>:[1/1] [9%                          ] sglang_95030.pbs111_rank12.nsys-rep[1,10]<stdout>:[1/1] [8%                          ] sglang_95030.pbs111_rank10.nsys-rep[1,4]<stdout>:[1/1] [6%                          ] sglang_95030.pbs111_rank4.nsys-rep[1,15]<stdout>:[1/1] [8%                          ] sglang_95030.pbs111_rank15.nsys-rep[1,11]<stdout>:[1/1] [9%                          ] sglang_95030.pbs111_rank11.nsys-rep[1,7]<stdout>:[1/1] [8%                          ] sglang_95030.pbs111_rank7.nsys-rep[1,4]<stdout>:[1/1] [5%                          ] sglang_95030.pbs111_rank4.nsys-rep[1,0]<stdout>:[1/1] [7%                          ] sglang_95030.pbs111_rank0.nsys-rep[1,12]<stdout>:[1/1] [10%                         ] sglang_95030.pbs111_rank12.nsys-rep[1,5]<stdout>:[1/1] [8%                          ] sglang_95030.pbs111_rank5.nsys-rep[1,15]<stdout>:[1/1] [9%                          ] sglang_95030.pbs111_rank15.nsys-rep[1,1]<stdout>:[1/1] [8%                          ] sglang_95030.pbs111_rank1.nsys-rep[1,7]<stdout>:[1/1] [9%                          ] sglang_95030.pbs111_rank7.nsys-rep[1,11]<stdout>:[1/1] [10%                         ] sglang_95030.pbs111_rank11.nsys-rep[1,8]<stdout>:[1/1] [7%                          ] sglang_95030.pbs111_rank8.nsys-rep[1,8]<stdout>:[1/1] [8%                          ] sglang_95030.pbs111_rank8.nsys-rep[1,1]<stdout>:[1/1] [9%                          ] sglang_95030.pbs111_rank1.nsys-rep[1,10]<stdout>:[1/1] [9%                          ] sglang_95030.pbs111_rank10.nsys-rep[1,8]<stdout>:[1/1] [9%                          ] sglang_95030.pbs111_rank8.nsys-rep[1,1]<stdout>:[1/1] [10%                         ] sglang_95030.pbs111_rank1.nsys-rep[1,9]<stdout>:[1/1] [9%                          ] sglang_95030.pbs111_rank9.nsys-rep[1,9]<stdout>:[1/1] [10%                         ] sglang_95030.pbs111_rank9.nsys-rep[1,13]<stdout>:[1/1] [9%                          ] sglang_95030.pbs111_rank13.nsys-rep[1,14]<stdout>:[1/1] [10%                         ] sglang_95030.pbs111_rank14.nsys-rep[1,9]<stdout>:[1/1] [11%                         ] sglang_95030.pbs111_rank9.nsys-rep[1,9]<stdout>:[1/1] [12%                         ] sglang_95030.pbs111_rank9.nsys-rep[1,13]<stdout>:[1/1] [10%                         ] sglang_95030.pbs111_rank13.nsys-rep[1,14]<stdout>:[1/1] [11%                         ] sglang_95030.pbs111_rank14.nsys-rep[1,9]<stdout>:[1/1] [13%                         ] sglang_95030.pbs111_rank9.nsys-rep[1,0]<stdout>:[1/1] [8%                          ] sglang_95030.pbs111_rank0.nsys-rep[1,13]<stdout>:[1/1] [11%                         ] sglang_95030.pbs111_rank13.nsys-rep[1,14]<stdout>:[1/1] [12%                         ] sglang_95030.pbs111_rank14.nsys-rep[1,9]<stdout>:[1/1] [=15%                        ] sglang_95030.pbs111_rank9.nsys-rep[1,5]<stdout>:[1/1] [9%                          ] sglang_95030.pbs111_rank5.nsys-rep[1,12]<stdout>:[1/1] [11%                         ] sglang_95030.pbs111_rank12.nsys-rep[1,13]<stdout>:[1/1] [12%                         ] sglang_95030.pbs111_rank13.nsys-rep[1,14]<stdout>:[1/1] [13%                         ] sglang_95030.pbs111_rank14.nsys-rep[1,15]<stdout>:[1/1] [10%                         ] sglang_95030.pbs111_rank15.nsys-rep[1,9]<stdout>:[1/1] [=16%                        ] sglang_95030.pbs111_rank9.nsys-rep[1,13]<stdout>:[1/1] [14%                         ] sglang_95030.pbs111_rank13.nsys-rep[1,5]<stdout>:[1/1] [10%                         ] sglang_95030.pbs111_rank5.nsys-rep[1,14]<stdout>:[1/1] [14%                         ] sglang_95030.pbs111_rank14.nsys-rep[1,12]<stdout>:[1/1] [12%                         ] sglang_95030.pbs111_rank12.nsys-rep[1,11]<stdout>:[1/1] [11%                         ] sglang_95030.pbs111_rank11.nsys-rep[1,9]<stdout>:[1/1] [=17%                        ] sglang_95030.pbs111_rank9.nsys-rep[1,5]<stdout>:[1/1] [11%                         ] sglang_95030.pbs111_rank5.nsys-rep[1,7]<stdout>:[1/1] [10%                         ] sglang_95030.pbs111_rank7.nsys-rep[1,13]<stdout>:[1/1] [=15%                        ] sglang_95030.pbs111_rank13.nsys-rep[1,9]<stdout>:[1/1] [==19%                       ] sglang_95030.pbs111_rank9.nsys-rep[1,12]<stdout>:[1/1] [13%                         ] sglang_95030.pbs111_rank12.nsys-rep[1,15]<stdout>:[1/1] [11%                         ] sglang_95030.pbs111_rank15.nsys-rep[1,14]<stdout>:[1/1] [=15%                        ] sglang_95030.pbs111_rank14.nsys-rep[1,0]<stdout>:[1/1] [9%                          ] sglang_95030.pbs111_rank0.nsys-rep[1,13]<stdout>:[1/1] [=16%                        ] sglang_95030.pbs111_rank13.nsys-rep[1,5]<stdout>:[1/1] [12%                         ] sglang_95030.pbs111_rank5.nsys-rep[1,12]<stdout>:[1/1] [14%                         ] sglang_95030.pbs111_rank12.nsys-rep[1,11]<stdout>:[1/1] [12%                         ] sglang_95030.pbs111_rank11.nsys-rep[1,14]<stdout>:[1/1] [=17%                        ] sglang_95030.pbs111_rank14.nsys-rep[1,7]<stdout>:[1/1] [11%                         ] sglang_95030.pbs111_rank7.nsys-rep[1,15]<stdout>:[1/1] [12%                         ] sglang_95030.pbs111_rank15.nsys-rep[1,13]<stdout>:[1/1] [==18%                       ] sglang_95030.pbs111_rank13.nsys-rep[1,0]<stdout>:[1/1] [10%                         ] sglang_95030.pbs111_rank0.nsys-rep[1,10]<stdout>:[1/1] [10%                         ] sglang_95030.pbs111_rank10.nsys-rep[1,5]<stdout>:[1/1] [13%                         ] sglang_95030.pbs111_rank5.nsys-rep[1,14]<stdout>:[1/1] [==18%                       ] sglang_95030.pbs111_rank14.nsys-rep[1,12]<stdout>:[1/1] [=15%                        ] sglang_95030.pbs111_rank12.nsys-rep[1,11]<stdout>:[1/1] [13%                         ] sglang_95030.pbs111_rank11.nsys-rep[1,15]<stdout>:[1/1] [13%                         ] sglang_95030.pbs111_rank15.nsys-rep[1,7]<stdout>:[1/1] [12%                         ] sglang_95030.pbs111_rank7.nsys-rep[1,9]<stdout>:[1/1] [==20%                       ] sglang_95030.pbs111_rank9.nsys-rep[1,0]<stdout>:[1/1] [11%                         ] sglang_95030.pbs111_rank0.nsys-rep[1,5]<stdout>:[1/1] [=15%                        ] sglang_95030.pbs111_rank5.nsys-rep[1,7]<stdout>:[1/1] [14%                         ] sglang_95030.pbs111_rank7.nsys-rep[1,9]<stdout>:[1/1] [===22%                      ] sglang_95030.pbs111_rank9.nsys-rep[1,11]<stdout>:[1/1] [14%                         ] sglang_95030.pbs111_rank11.nsys-rep[1,15]<stdout>:[1/1] [14%                         ] sglang_95030.pbs111_rank15.nsys-rep[1,0]<stdout>:[1/1] [12%                         ] sglang_95030.pbs111_rank0.nsys-rep[1,12]<stdout>:[1/1] [=16%                        ] sglang_95030.pbs111_rank12.nsys-rep[1,13]<stdout>:[1/1] [==19%                       ] sglang_95030.pbs111_rank13.nsys-rep[1,10]<stdout>:[1/1] [11%                         ] sglang_95030.pbs111_rank10.nsys-rep[1,5]<stdout>:[1/1] [=16%                        ] sglang_95030.pbs111_rank5.nsys-rep[1,1]<stdout>:[1/1] [11%                         ] sglang_95030.pbs111_rank1.nsys-rep[1,7]<stdout>:[1/1] [=15%                        ] sglang_95030.pbs111_rank7.nsys-rep[1,14]<stdout>:[1/1] [==19%                       ] sglang_95030.pbs111_rank14.nsys-rep[1,0]<stdout>:[1/1] [14%                         ] sglang_95030.pbs111_rank0.nsys-rep[1,12]<stdout>:[1/1] [==18%                       ] sglang_95030.pbs111_rank12.nsys-rep[1,11]<stdout>:[1/1] [=15%                        ] sglang_95030.pbs111_rank11.nsys-rep[1,15]<stdout>:[1/1] [=15%                        ] sglang_95030.pbs111_rank15.nsys-rep[1,13]<stdout>:[1/1] [==21%                       ] sglang_95030.pbs111_rank13.nsys-rep[1,5]<stdout>:[1/1] [=17%                        ] sglang_95030.pbs111_rank5.nsys-rep[1,10]<stdout>:[1/1] [12%                         ] sglang_95030.pbs111_rank10.nsys-rep[1,1]<stdout>:[1/1] [12%                         ] sglang_95030.pbs111_rank1.nsys-rep[1,14]<stdout>:[1/1] [==21%                       ] sglang_95030.pbs111_rank14.nsys-rep[1,7]<stdout>:[1/1] [=16%                        ] sglang_95030.pbs111_rank7.nsys-rep[1,12]<stdout>:[1/1] [==19%                       ] sglang_95030.pbs111_rank12.nsys-rep[1,0]<stdout>:[1/1] [=15%                        ] sglang_95030.pbs111_rank0.nsys-rep[1,15]<stdout>:[1/1] [=17%                        ] sglang_95030.pbs111_rank15.nsys-rep[1,9]<stdout>:[1/1] [===23%                      ] sglang_95030.pbs111_rank9.nsys-rep[1,13]<stdout>:[1/1] [===22%                      ] sglang_95030.pbs111_rank13.nsys-rep[1,5]<stdout>:[1/1] [==19%                       ] sglang_95030.pbs111_rank5.nsys-rep[1,11]<stdout>:[1/1] [=16%                        ] sglang_95030.pbs111_rank11.nsys-rep[1,1]<stdout>:[1/1] [14%                         ] sglang_95030.pbs111_rank1.nsys-rep[1,10]<stdout>:[1/1] [13%                         ] sglang_95030.pbs111_rank10.nsys-rep[1,0]<stdout>:[1/1] [=16%                        ] sglang_95030.pbs111_rank0.nsys-rep[1,7]<stdout>:[1/1] [==18%                       ] sglang_95030.pbs111_rank7.nsys-rep[1,15]<stdout>:[1/1] [==18%                       ] sglang_95030.pbs111_rank15.nsys-rep[1,11]<stdout>:[1/1] [=17%                        ] sglang_95030.pbs111_rank11.nsys-rep[1,14]<stdout>:[1/1] [===22%                      ] sglang_95030.pbs111_rank14.nsys-rep[1,8]<stdout>:[1/1] [10%                         ] sglang_95030.pbs111_rank8.nsys-rep[1,10]<stdout>:[1/1] [=15%                        ] sglang_95030.pbs111_rank10.nsys-rep[1,1]<stdout>:[1/1] [=15%                        ] sglang_95030.pbs111_rank1.nsys-rep[1,0]<stdout>:[1/1] [=17%                        ] sglang_95030.pbs111_rank0.nsys-rep[1,7]<stdout>:[1/1] [==19%                       ] sglang_95030.pbs111_rank7.nsys-rep[1,15]<stdout>:[1/1] [==19%                       ] sglang_95030.pbs111_rank15.nsys-rep[1,11]<stdout>:[1/1] [==18%                       ] sglang_95030.pbs111_rank11.nsys-rep[1,12]<stdout>:[1/1] [==20%                       ] sglang_95030.pbs111_rank12.nsys-rep[1,0]<stdout>:[1/1] [==18%                       ] sglang_95030.pbs111_rank0.nsys-rep[1,5]<stdout>:[1/1] [==20%                       ] sglang_95030.pbs111_rank5.nsys-rep[1,13]<stdout>:[1/1] [===24%                      ] sglang_95030.pbs111_rank13.nsys-rep[1,10]<stdout>:[1/1] [=16%                        ] sglang_95030.pbs111_rank10.nsys-rep[1,11]<stdout>:[1/1] [==20%                       ] sglang_95030.pbs111_rank11.nsys-rep[1,1]<stdout>:[1/1] [=16%                        ] sglang_95030.pbs111_rank1.nsys-rep[1,12]<stdout>:[1/1] [===22%                      ] sglang_95030.pbs111_rank12.nsys-rep[1,8]<stdout>:[1/1] [11%                         ] sglang_95030.pbs111_rank8.nsys-rep[1,9]<stdout>:[1/1] [====25%                     ] sglang_95030.pbs111_rank9.nsys-rep[1,10]<stdout>:[1/1] [=17%                        ] sglang_95030.pbs111_rank10.nsys-rep[1,5]<stdout>:[1/1] [===22%                      ] sglang_95030.pbs111_rank5.nsys-rep[1,7]<stdout>:[1/1] [==20%                       ] sglang_95030.pbs111_rank7.nsys-rep[1,1]<stdout>:[1/1] [=17%                        ] sglang_95030.pbs111_rank1.nsys-rep[1,15]<stdout>:[1/1] [==21%                       ] sglang_95030.pbs111_rank15.nsys-rep[1,8]<stdout>:[1/1] [12%                         ] sglang_95030.pbs111_rank8.nsys-rep[1,14]<stdout>:[1/1] [===24%                      ] sglang_95030.pbs111_rank14.nsys-rep[1,7]<stdout>:[1/1] [===22%                      ] sglang_95030.pbs111_rank7.nsys-rep[1,0]<stdout>:[1/1] [==19%                       ] sglang_95030.pbs111_rank0.nsys-rep[1,10]<stdout>:[1/1] [==18%                       ] sglang_95030.pbs111_rank10.nsys-rep[1,1]<stdout>:[1/1] [==19%                       ] sglang_95030.pbs111_rank1.nsys-rep[1,15]<stdout>:[1/1] [===22%                      ] sglang_95030.pbs111_rank15.nsys-rep[1,11]<stdout>:[1/1] [==21%                       ] sglang_95030.pbs111_rank11.nsys-rep[1,8]<stdout>:[1/1] [13%                         ] sglang_95030.pbs111_rank8.nsys-rep[1,0]<stdout>:[1/1] [==21%                       ] sglang_95030.pbs111_rank0.nsys-rep[1,12]<stdout>:[1/1] [===23%                      ] sglang_95030.pbs111_rank12.nsys-rep[1,10]<stdout>:[1/1] [==20%                       ] sglang_95030.pbs111_rank10.nsys-rep[1,11]<stdout>:[1/1] [===22%                      ] sglang_95030.pbs111_rank11.nsys-rep[1,0]<stdout>:[1/1] [===22%                      ] sglang_95030.pbs111_rank0.nsys-rep[1,8]<stdout>:[1/1] [14%                         ] sglang_95030.pbs111_rank8.nsys-rep[1,13]<stdout>:[1/1] [====25%                     ] sglang_95030.pbs111_rank13.nsys-rep[1,1]<stdout>:[1/1] [==20%                       ] sglang_95030.pbs111_rank1.nsys-rep[1,9]<stdout>:[1/1] [====26%                     ] sglang_95030.pbs111_rank9.nsys-rep[1,8]<stdout>:[1/1] [=15%                        ] sglang_95030.pbs111_rank8.nsys-rep[1,14]<stdout>:[1/1] [====25%                     ] sglang_95030.pbs111_rank14.nsys-rep[1,10]<stdout>:[1/1] [==21%                       ] sglang_95030.pbs111_rank10.nsys-rep[1,15]<stdout>:[1/1] [===23%                      ] sglang_95030.pbs111_rank15.nsys-rep[1,1]<stdout>:[1/1] [===22%                      ] sglang_95030.pbs111_rank1.nsys-rep[1,12]<stdout>:[1/1] [====25%                     ] sglang_95030.pbs111_rank12.nsys-rep[1,7]<stdout>:[1/1] [===23%                      ] sglang_95030.pbs111_rank7.nsys-rep[1,5]<stdout>:[1/1] [===23%                      ] sglang_95030.pbs111_rank5.nsys-rep[1,8]<stdout>:[1/1] [=16%                        ] sglang_95030.pbs111_rank8.nsys-rep[1,10]<stdout>:[1/1] [===23%                      ] sglang_95030.pbs111_rank10.nsys-rep[1,9]<stdout>:[1/1] [====27%                     ] sglang_95030.pbs111_rank9.nsys-rep[1,11]<stdout>:[1/1] [===24%                      ] sglang_95030.pbs111_rank11.nsys-rep[1,0]<stdout>:[1/1] [===23%                      ] sglang_95030.pbs111_rank0.nsys-rep[1,1]<stdout>:[1/1] [===23%                      ] sglang_95030.pbs111_rank1.nsys-rep[1,8]<stdout>:[1/1] [=17%                        ] sglang_95030.pbs111_rank8.nsys-rep[1,13]<stdout>:[1/1] [====26%                     ] sglang_95030.pbs111_rank13.nsys-rep[1,5]<stdout>:[1/1] [===24%                      ] sglang_95030.pbs111_rank5.nsys-rep[1,14]<stdout>:[1/1] [====26%                     ] sglang_95030.pbs111_rank14.nsys-rep[1,8]<stdout>:[1/1] [==18%                       ] sglang_95030.pbs111_rank8.nsys-rep[1,7]<stdout>:[1/1] [====25%                     ] sglang_95030.pbs111_rank7.nsys-rep[1,9]<stdout>:[1/1] [====28%                     ] sglang_95030.pbs111_rank9.nsys-rep[1,10]<stdout>:[1/1] [===24%                      ] sglang_95030.pbs111_rank10.nsys-rep[1,15]<stdout>:[1/1] [====25%                     ] sglang_95030.pbs111_rank15.nsys-rep[1,8]<stdout>:[1/1] [==19%                       ] sglang_95030.pbs111_rank8.nsys-rep[1,13]<stdout>:[1/1] [====27%                     ] sglang_95030.pbs111_rank13.nsys-rep[1,12]<stdout>:[1/1] [====26%                     ] sglang_95030.pbs111_rank12.nsys-rep[1,14]<stdout>:[1/1] [====27%                     ] sglang_95030.pbs111_rank14.nsys-rep[1,0]<stdout>:[1/1] [===24%                      ] sglang_95030.pbs111_rank0.nsys-rep[1,11]<stdout>:[1/1] [====25%                     ] sglang_95030.pbs111_rank11.nsys-rep[1,5]<stdout>:[1/1] [====26%                     ] sglang_95030.pbs111_rank5.nsys-rep[1,1]<stdout>:[1/1] [====25%                     ] sglang_95030.pbs111_rank1.nsys-rep[1,8]<stdout>:[1/1] [==20%                       ] sglang_95030.pbs111_rank8.nsys-rep[1,13]<stdout>:[1/1] [====28%                     ] sglang_95030.pbs111_rank13.nsys-rep[1,7]<stdout>:[1/1] [====26%                     ] sglang_95030.pbs111_rank7.nsys-rep[1,12]<stdout>:[1/1] [====27%                     ] sglang_95030.pbs111_rank12.nsys-rep[1,14]<stdout>:[1/1] [====28%                     ] sglang_95030.pbs111_rank14.nsys-rep[1,8]<stdout>:[1/1] [==21%                       ] sglang_95030.pbs111_rank8.nsys-rep[1,15]<stdout>:[1/1] [====26%                     ] sglang_95030.pbs111_rank15.nsys-rep[1,9]<stdout>:[1/1] [=====29%                    ] sglang_95030.pbs111_rank9.nsys-rep[1,11]<stdout>:[1/1] [====26%                     ] sglang_95030.pbs111_rank11.nsys-rep[1,7]<stdout>:[1/1] [====27%                     ] sglang_95030.pbs111_rank7.nsys-rep[1,10]<stdout>:[1/1] [====25%                     ] sglang_95030.pbs111_rank10.nsys-rep[1,12]<stdout>:[1/1] [====28%                     ] sglang_95030.pbs111_rank12.nsys-rep[1,15]<stdout>:[1/1] [====27%                     ] sglang_95030.pbs111_rank15.nsys-rep[1,1]<stdout>:[1/1] [====26%                     ] sglang_95030.pbs111_rank1.nsys-rep[1,5]<stdout>:[1/1] [====27%                     ] sglang_95030.pbs111_rank5.nsys-rep[1,8]<stdout>:[1/1] [===22%                      ] sglang_95030.pbs111_rank8.nsys-rep[1,11]<stdout>:[1/1] [====27%                     ] sglang_95030.pbs111_rank11.nsys-rep[1,9]<stdout>:[1/1] [=====30%                    ] sglang_95030.pbs111_rank9.nsys-rep[1,0]<stdout>:[1/1] [====26%                     ] sglang_95030.pbs111_rank0.nsys-rep[1,13]<stdout>:[1/1] [=====29%                    ] sglang_95030.pbs111_rank13.nsys-rep[1,14]<stdout>:[1/1] [=====29%                    ] sglang_95030.pbs111_rank14.nsys-rep[1,1]<stdout>:[1/1] [====27%                     ] sglang_95030.pbs111_rank1.nsys-rep[1,5]<stdout>:[1/1] [====28%                     ] sglang_95030.pbs111_rank5.nsys-rep[1,8]<stdout>:[1/1] [===23%                      ] sglang_95030.pbs111_rank8.nsys-rep[1,3]<stdout>:[1/1] [5%                          ] sglang_95030.pbs111_rank3.nsys-rep[1,7]<stdout>:[1/1] [====28%                     ] sglang_95030.pbs111_rank7.nsys-rep[1,12]<stdout>:[1/1] [=====29%                    ] sglang_95030.pbs111_rank12.nsys-rep[1,9]<stdout>:[1/1] [=====31%                    ] sglang_95030.pbs111_rank9.nsys-rep[1,10]<stdout>:[1/1] [====27%                     ] sglang_95030.pbs111_rank10.nsys-rep[1,0]<stdout>:[1/1] [====27%                     ] sglang_95030.pbs111_rank0.nsys-rep[1,15]<stdout>:[1/1] [====28%                     ] sglang_95030.pbs111_rank15.nsys-rep[1,13]<stdout>:[1/1] [=====30%                    ] sglang_95030.pbs111_rank13.nsys-rep[1,14]<stdout>:[1/1] [=====30%                    ] sglang_95030.pbs111_rank14.nsys-rep[1,1]<stdout>:[1/1] [====28%                     ] sglang_95030.pbs111_rank1.nsys-rep[1,11]<stdout>:[1/1] [====28%                     ] sglang_95030.pbs111_rank11.nsys-rep[1,7]<stdout>:[1/1] [=====29%                    ] sglang_95030.pbs111_rank7.nsys-rep[1,10]<stdout>:[1/1] [====28%                     ] sglang_95030.pbs111_rank10.nsys-rep[1,12]<stdout>:[1/1] [=====30%                    ] sglang_95030.pbs111_rank12.nsys-rep[1,13]<stdout>:[1/1] [=====31%                    ] sglang_95030.pbs111_rank13.nsys-rep[1,14]<stdout>:[1/1] [=====31%                    ] sglang_95030.pbs111_rank14.nsys-rep[1,15]<stdout>:[1/1] [=====29%                    ] sglang_95030.pbs111_rank15.nsys-rep[1,5]<stdout>:[1/1] [=====29%                    ] sglang_95030.pbs111_rank5.nsys-rep[1,7]<stdout>:[1/1] [=====30%                    ] sglang_95030.pbs111_rank7.nsys-rep[1,0]<stdout>:[1/1] [====28%                     ] sglang_95030.pbs111_rank0.nsys-rep[1,11]<stdout>:[1/1] [=====29%                    ] sglang_95030.pbs111_rank11.nsys-rep[1,10]<stdout>:[1/1] [=====29%                    ] sglang_95030.pbs111_rank10.nsys-rep[1,9]<stdout>:[1/1] [=====32%                    ] sglang_95030.pbs111_rank9.nsys-rep[1,8]<stdout>:[1/1] [====25%                     ] sglang_95030.pbs111_rank8.nsys-rep[1,1]<stdout>:[1/1] [=====29%                    ] sglang_95030.pbs111_rank1.nsys-rep[1,3]<stdout>:[1/1] [6%                          ] sglang_95030.pbs111_rank3.nsys-rep[1,5]<stdout>:[1/1] [=====30%                    ] sglang_95030.pbs111_rank5.nsys-rep[1,15]<stdout>:[1/1] [=====30%                    ] sglang_95030.pbs111_rank15.nsys-rep[1,8]<stdout>:[1/1] [====26%                     ] sglang_95030.pbs111_rank8.nsys-rep[1,12]<stdout>:[1/1] [=====31%                    ] sglang_95030.pbs111_rank12.nsys-rep[1,5]<stdout>:[1/1] [=====31%                    ] sglang_95030.pbs111_rank5.nsys-rep[1,9]<stdout>:[1/1] [======34%                   ] sglang_95030.pbs111_rank9.nsys-rep[1,13]<stdout>:[1/1] [=====32%                    ] sglang_95030.pbs111_rank13.nsys-rep[1,1]<stdout>:[1/1] [=====30%                    ] sglang_95030.pbs111_rank1.nsys-rep[1,14]<stdout>:[1/1] [=====32%                    ] sglang_95030.pbs111_rank14.nsys-rep[1,7]<stdout>:[1/1] [=====31%                    ] sglang_95030.pbs111_rank7.nsys-rep[1,11]<stdout>:[1/1] [=====30%                    ] sglang_95030.pbs111_rank11.nsys-rep[1,3]<stdout>:[1/1] [7%                          ] sglang_95030.pbs111_rank3.nsys-rep[1,10]<stdout>:[1/1] [=====30%                    ] sglang_95030.pbs111_rank10.nsys-rep[1,12]<stdout>:[1/1] [=====32%                    ] sglang_95030.pbs111_rank12.nsys-rep[1,0]<stdout>:[1/1] [=====29%                    ] sglang_95030.pbs111_rank0.nsys-rep[1,15]<stdout>:[1/1] [=====31%                    ] sglang_95030.pbs111_rank15.nsys-rep[1,9]<stdout>:[1/1] [======35%                   ] sglang_95030.pbs111_rank9.nsys-rep[1,7]<stdout>:[1/1] [=====32%                    ] sglang_95030.pbs111_rank7.nsys-rep[1,5]<stdout>:[1/1] [=====32%                    ] sglang_95030.pbs111_rank5.nsys-rep[1,11]<stdout>:[1/1] [=====31%                    ] sglang_95030.pbs111_rank11.nsys-rep[1,13]<stdout>:[1/1] [======34%                   ] sglang_95030.pbs111_rank13.nsys-rep[1,8]<stdout>:[1/1] [====27%                     ] sglang_95030.pbs111_rank8.nsys-rep[1,10]<stdout>:[1/1] [=====31%                    ] sglang_95030.pbs111_rank10.nsys-rep[1,9]<stdout>:[1/1] [=======36%                  ] sglang_95030.pbs111_rank9.nsys-rep[1,0]<stdout>:[1/1] [=====30%                    ] sglang_95030.pbs111_rank0.nsys-rep[1,9]<stdout>:[1/1] [=======37%                  ] sglang_95030.pbs111_rank9.nsys-rep[1,1]<stdout>:[1/1] [=====31%                    ] sglang_95030.pbs111_rank1.nsys-rep[1,14]<stdout>:[1/1] [======34%                   ] sglang_95030.pbs111_rank14.nsys-rep[1,12]<stdout>:[1/1] [======33%                   ] sglang_95030.pbs111_rank12.nsys-rep[1,15]<stdout>:[1/1] [=====32%                    ] sglang_95030.pbs111_rank15.nsys-rep[1,13]<stdout>:[1/1] [======35%                   ] sglang_95030.pbs111_rank13.nsys-rep[1,14]<stdout>:[1/1] [======35%                   ] sglang_95030.pbs111_rank14.nsys-rep[1,11]<stdout>:[1/1] [=====32%                    ] sglang_95030.pbs111_rank11.nsys-rep[1,9]<stdout>:[1/1] [=======39%                  ] sglang_95030.pbs111_rank9.nsys-rep[1,10]<stdout>:[1/1] [=====32%                    ] sglang_95030.pbs111_rank10.nsys-rep[1,1]<stdout>:[1/1] [=====32%                    ] sglang_95030.pbs111_rank1.nsys-rep[1,5]<stdout>:[1/1] [======33%                   ] sglang_95030.pbs111_rank5.nsys-rep[1,13]<stdout>:[1/1] [=======36%                  ] sglang_95030.pbs111_rank13.nsys-rep[1,14]<stdout>:[1/1] [=======36%                  ] sglang_95030.pbs111_rank14.nsys-rep[1,7]<stdout>:[1/1] [======33%                   ] sglang_95030.pbs111_rank7.nsys-rep[1,14]<stdout>:[1/1] [=======37%                  ] sglang_95030.pbs111_rank14.nsys-rep[1,3]<stdout>:[1/1] [8%                          ] sglang_95030.pbs111_rank3.nsys-rep[1,13]<stdout>:[1/1] [=======37%                  ] sglang_95030.pbs111_rank13.nsys-rep[1,14]<stdout>:[1/1] [=======39%                  ] sglang_95030.pbs111_rank14.nsys-rep[1,0]<stdout>:[1/1] [=====31%                    ] sglang_95030.pbs111_rank0.nsys-rep[1,5]<stdout>:[1/1] [======34%                   ] sglang_95030.pbs111_rank5.nsys-rep[1,15]<stdout>:[1/1] [======33%                   ] sglang_95030.pbs111_rank15.nsys-rep[1,8]<stdout>:[1/1] [====28%                     ] sglang_95030.pbs111_rank8.nsys-rep[1,9]<stdout>:[1/1] [========40%                 ] sglang_95030.pbs111_rank9.nsys-rep[1,10]<stdout>:[1/1] [======33%                   ] sglang_95030.pbs111_rank10.nsys-rep[1,3]<stdout>:[1/1] [9%                          ] sglang_95030.pbs111_rank3.nsys-rep[1,12]<stdout>:[1/1] [======34%                   ] sglang_95030.pbs111_rank12.nsys-rep[1,1]<stdout>:[1/1] [======33%                   ] sglang_95030.pbs111_rank1.nsys-rep[1,13]<stdout>:[1/1] [=======39%                  ] sglang_95030.pbs111_rank13.nsys-rep[1,7]<stdout>:[1/1] [======34%                   ] sglang_95030.pbs111_rank7.nsys-rep[1,5]<stdout>:[1/1] [======35%                   ] sglang_95030.pbs111_rank5.nsys-rep[1,5]<stdout>:[1/1] [=======36%                  ] sglang_95030.pbs111_rank5.nsys-rep[1,12]<stdout>:[1/1] [======35%                   ] sglang_95030.pbs111_rank12.nsys-rep[1,7]<stdout>:[1/1] [======35%                   ] sglang_95030.pbs111_rank7.nsys-rep[1,11]<stdout>:[1/1] [======33%                   ] sglang_95030.pbs111_rank11.nsys-rep[1,9]<stdout>:[1/1] [========42%                 ] sglang_95030.pbs111_rank9.nsys-rep[1,8]<stdout>:[1/1] [=====29%                    ] sglang_95030.pbs111_rank8.nsys-rep[1,7]<stdout>:[1/1] [=======37%                  ] sglang_95030.pbs111_rank7.nsys-rep[1,15]<stdout>:[1/1] [======34%                   ] sglang_95030.pbs111_rank15.nsys-rep[1,5]<stdout>:[1/1] [=======38%                  ] sglang_95030.pbs111_rank5.nsys-rep[1,3]<stdout>:[1/1] [10%                         ] sglang_95030.pbs111_rank3.nsys-rep[1,12]<stdout>:[1/1] [=======37%                  ] sglang_95030.pbs111_rank12.nsys-rep[1,14]<stdout>:[1/1] [========40%                 ] sglang_95030.pbs111_rank14.nsys-rep[1,11]<stdout>:[1/1] [======34%                   ] sglang_95030.pbs111_rank11.nsys-rep[1,0]<stdout>:[1/1] [=====32%                    ] sglang_95030.pbs111_rank0.nsys-rep[1,7]<stdout>:[1/1] [=======38%                  ] sglang_95030.pbs111_rank7.nsys-rep[1,15]<stdout>:[1/1] [======35%                   ] sglang_95030.pbs111_rank15.nsys-rep[1,12]<stdout>:[1/1] [=======38%                  ] sglang_95030.pbs111_rank12.nsys-rep[1,11]<stdout>:[1/1] [======35%                   ] sglang_95030.pbs111_rank11.nsys-rep[1,15]<stdout>:[1/1] [=======37%                  ] sglang_95030.pbs111_rank15.nsys-rep[1,11]<stdout>:[1/1] [=======37%                  ] sglang_95030.pbs111_rank11.nsys-rep[1,13]<stdout>:[1/1] [========40%                 ] sglang_95030.pbs111_rank13.nsys-rep[1,14]<stdout>:[1/1] [========42%                 ] sglang_95030.pbs111_rank14.nsys-rep[1,7]<stdout>:[1/1] [========40%                 ] sglang_95030.pbs111_rank7.nsys-rep[1,10]<stdout>:[1/1] [======34%                   ] sglang_95030.pbs111_rank10.nsys-rep[1,1]<stdout>:[1/1] [======34%                   ] sglang_95030.pbs111_rank1.nsys-rep[1,15]<stdout>:[1/1] [=======38%                  ] sglang_95030.pbs111_rank15.nsys-rep[1,12]<stdout>:[1/1] [========40%                 ] sglang_95030.pbs111_rank12.nsys-rep[1,11]<stdout>:[1/1] [=======38%                  ] sglang_95030.pbs111_rank11.nsys-rep[1,0]<stdout>:[1/1] [======33%                   ] sglang_95030.pbs111_rank0.nsys-rep[1,5]<stdout>:[1/1] [=======39%                  ] sglang_95030.pbs111_rank5.nsys-rep[1,10]<stdout>:[1/1] [======35%                   ] sglang_95030.pbs111_rank10.nsys-rep[1,9]<stdout>:[1/1] [=========43%                ] sglang_95030.pbs111_rank9.nsys-rep[1,1]<stdout>:[1/1] [======35%                   ] sglang_95030.pbs111_rank1.nsys-rep[1,1]<stdout>:[1/1] [=======37%                  ] sglang_95030.pbs111_rank1.nsys-rep[1,8]<stdout>:[1/1] [=====30%                    ] sglang_95030.pbs111_rank8.nsys-rep[1,10]<stdout>:[1/1] [=======36%                  ] sglang_95030.pbs111_rank10.nsys-rep[1,5]<stdout>:[1/1] [========41%                 ] sglang_95030.pbs111_rank5.nsys-rep[1,13]<stdout>:[1/1] [========42%                 ] sglang_95030.pbs111_rank13.nsys-rep[1,10]<stdout>:[1/1] [=======37%                  ] sglang_95030.pbs111_rank10.nsys-rep[1,7]<stdout>:[1/1] [========41%                 ] sglang_95030.pbs111_rank7.nsys-rep[1,1]<stdout>:[1/1] [=======38%                  ] sglang_95030.pbs111_rank1.nsys-rep[1,12]<stdout>:[1/1] [========41%                 ] sglang_95030.pbs111_rank12.nsys-rep[1,15]<stdout>:[1/1] [========40%                 ] sglang_95030.pbs111_rank15.nsys-rep[1,11]<stdout>:[1/1] [=======39%                  ] sglang_95030.pbs111_rank11.nsys-rep[1,10]<stdout>:[1/1] [=======39%                  ] sglang_95030.pbs111_rank10.nsys-rep[1,0]<stdout>:[1/1] [======34%                   ] sglang_95030.pbs111_rank0.nsys-rep[1,8]<stdout>:[1/1] [=====31%                    ] sglang_95030.pbs111_rank8.nsys-rep[1,5]<stdout>:[1/1] [========42%                 ] sglang_95030.pbs111_rank5.nsys-rep[1,7]<stdout>:[1/1] [========42%                 ] sglang_95030.pbs111_rank7.nsys-rep[1,15]<stdout>:[1/1] [========41%                 ] sglang_95030.pbs111_rank15.nsys-rep[1,1]<stdout>:[1/1] [=======39%                  ] sglang_95030.pbs111_rank1.nsys-rep[1,11]<stdout>:[1/1] [========41%                 ] sglang_95030.pbs111_rank11.nsys-rep[1,12]<stdout>:[1/1] [========42%                 ] sglang_95030.pbs111_rank12.nsys-rep[1,0]<stdout>:[1/1] [======35%                   ] sglang_95030.pbs111_rank0.nsys-rep[1,13]<stdout>:[1/1] [=========43%                ] sglang_95030.pbs111_rank13.nsys-rep[1,14]<stdout>:[1/1] [=========43%                ] sglang_95030.pbs111_rank14.nsys-rep[1,0]<stdout>:[1/1] [=======36%                  ] sglang_95030.pbs111_rank0.nsys-rep[1,10]<stdout>:[1/1] [========40%                 ] sglang_95030.pbs111_rank10.nsys-rep[1,0]<stdout>:[1/1] [=======37%                  ] sglang_95030.pbs111_rank0.nsys-rep[1,1]<stdout>:[1/1] [========41%                 ] sglang_95030.pbs111_rank1.nsys-rep[1,0]<stdout>:[1/1] [=======39%                  ] sglang_95030.pbs111_rank0.nsys-rep[1,10]<stdout>:[1/1] [========42%                 ] sglang_95030.pbs111_rank10.nsys-rep[1,15]<stdout>:[1/1] [========42%                 ] sglang_95030.pbs111_rank15.nsys-rep[1,9]<stdout>:[1/1] [=========44%                ] sglang_95030.pbs111_rank9.nsys-rep[1,1]<stdout>:[1/1] [========42%                 ] sglang_95030.pbs111_rank1.nsys-rep[1,11]<stdout>:[1/1] [========42%                 ] sglang_95030.pbs111_rank11.nsys-rep[1,0]<stdout>:[1/1] [========40%                 ] sglang_95030.pbs111_rank0.nsys-rep[1,9]<stdout>:[1/1] [=========45%                ] sglang_95030.pbs111_rank9.nsys-rep[1,14]<stdout>:[1/1] [=========44%                ] sglang_95030.pbs111_rank14.nsys-rep[1,8]<stdout>:[1/1] [=====32%                    ] sglang_95030.pbs111_rank8.nsys-rep[1,0]<stdout>:[1/1] [========41%                 ] sglang_95030.pbs111_rank0.nsys-rep[1,13]<stdout>:[1/1] [=========44%                ] sglang_95030.pbs111_rank13.nsys-rep[1,8]<stdout>:[1/1] [======33%                   ] sglang_95030.pbs111_rank8.nsys-rep[1,9]<stdout>:[1/1] [=========46%                ] sglang_95030.pbs111_rank9.nsys-rep[1,14]<stdout>:[1/1] [=========45%                ] sglang_95030.pbs111_rank14.nsys-rep[1,5]<stdout>:[1/1] [=========43%                ] sglang_95030.pbs111_rank5.nsys-rep[1,7]<stdout>:[1/1] [=========43%                ] sglang_95030.pbs111_rank7.nsys-rep[1,0]<stdout>:[1/1] [========42%                 ] sglang_95030.pbs111_rank0.nsys-rep[1,8]<stdout>:[1/1] [======34%                   ] sglang_95030.pbs111_rank8.nsys-rep[1,8]<stdout>:[1/1] [======35%                   ] sglang_95030.pbs111_rank8.nsys-rep[1,10]<stdout>:[1/1] [=========43%                ] sglang_95030.pbs111_rank10.nsys-rep[1,5]<stdout>:[1/1] [=========44%                ] sglang_95030.pbs111_rank5.nsys-rep[1,12]<stdout>:[1/1] [=========43%                ] sglang_95030.pbs111_rank12.nsys-rep[1,8]<stdout>:[1/1] [=======36%                  ] sglang_95030.pbs111_rank8.nsys-rep[1,7]<stdout>:[1/1] [=========44%                ] sglang_95030.pbs111_rank7.nsys-rep[1,15]<stdout>:[1/1] [=========43%                ] sglang_95030.pbs111_rank15.nsys-rep[1,8]<stdout>:[1/1] [=======37%                  ] sglang_95030.pbs111_rank8.nsys-rep[1,13]<stdout>:[1/1] [=========45%                ] sglang_95030.pbs111_rank13.nsys-rep[1,12]<stdout>:[1/1] [=========44%                ] sglang_95030.pbs111_rank12.nsys-rep[1,7]<stdout>:[1/1] [=========45%                ] sglang_95030.pbs111_rank7.nsys-rep[1,15]<stdout>:[1/1] [=========44%                ] sglang_95030.pbs111_rank15.nsys-rep[1,11]<stdout>:[1/1] [=========43%                ] sglang_95030.pbs111_rank11.nsys-rep[1,9]<stdout>:[1/1] [==========47%               ] sglang_95030.pbs111_rank9.nsys-rep[1,1]<stdout>:[1/1] [=========43%                ] sglang_95030.pbs111_rank1.nsys-rep[1,3]<stdout>:[1/1] [11%                         ] sglang_95030.pbs111_rank3.nsys-rep[1,14]<stdout>:[1/1] [=========46%                ] sglang_95030.pbs111_rank14.nsys-rep[1,12]<stdout>:[1/1] [=========45%                ] sglang_95030.pbs111_rank12.nsys-rep[1,0]<stdout>:[1/1] [=========43%                ] sglang_95030.pbs111_rank0.nsys-rep[1,13]<stdout>:[1/1] [=========46%                ] sglang_95030.pbs111_rank13.nsys-rep[1,8]<stdout>:[1/1] [=======38%                  ] sglang_95030.pbs111_rank8.nsys-rep[1,2]<stdout>:[1/1] [5%                          ] sglang_95030.pbs111_rank2.nsys-rep[1,1]<stdout>:[1/1] [=========44%                ] sglang_95030.pbs111_rank1.nsys-rep[1,9]<stdout>:[1/1] [==========48%               ] sglang_95030.pbs111_rank9.nsys-rep[1,14]<stdout>:[1/1] [==========47%               ] sglang_95030.pbs111_rank14.nsys-rep[1,5]<stdout>:[1/1] [=========45%                ] sglang_95030.pbs111_rank5.nsys-rep[1,10]<stdout>:[1/1] [=========44%                ] sglang_95030.pbs111_rank10.nsys-rep[1,11]<stdout>:[1/1] [=========44%                ] sglang_95030.pbs111_rank11.nsys-rep[1,13]<stdout>:[1/1] [==========47%               ] sglang_95030.pbs111_rank13.nsys-rep[1,9]<stdout>:[1/1] [==========49%               ] sglang_95030.pbs111_rank9.nsys-rep[1,7]<stdout>:[1/1] [=========46%                ] sglang_95030.pbs111_rank7.nsys-rep[1,3]<stdout>:[1/1] [12%                         ] sglang_95030.pbs111_rank3.nsys-rep[1,1]<stdout>:[1/1] [=========45%                ] sglang_95030.pbs111_rank1.nsys-rep[1,13]<stdout>:[1/1] [==========48%               ] sglang_95030.pbs111_rank13.nsys-rep[1,14]<stdout>:[1/1] [==========48%               ] sglang_95030.pbs111_rank14.nsys-rep[1,12]<stdout>:[1/1] [=========46%                ] sglang_95030.pbs111_rank12.nsys-rep[1,5]<stdout>:[1/1] [=========46%                ] sglang_95030.pbs111_rank5.nsys-rep[1,3]<stdout>:[1/1] [14%                         ] sglang_95030.pbs111_rank3.nsys-rep[1,8]<stdout>:[1/1] [========40%                 ] sglang_95030.pbs111_rank8.nsys-rep[1,14]<stdout>:[1/1] [==========49%               ] sglang_95030.pbs111_rank14.nsys-rep[1,15]<stdout>:[1/1] [=========45%                ] sglang_95030.pbs111_rank15.nsys-rep[1,11]<stdout>:[1/1] [=========45%                ] sglang_95030.pbs111_rank11.nsys-rep[1,7]<stdout>:[1/1] [==========47%               ] sglang_95030.pbs111_rank7.nsys-rep[1,3]<stdout>:[1/1] [=15%                        ] sglang_95030.pbs111_rank3.nsys-rep[1,10]<stdout>:[1/1] [=========45%                ] sglang_95030.pbs111_rank10.nsys-rep[1,5]<stdout>:[1/1] [==========47%               ] sglang_95030.pbs111_rank5.nsys-rep[1,13]<stdout>:[1/1] [==========49%               ] sglang_95030.pbs111_rank13.nsys-rep[1,2]<stdout>:[1/1] [6%                          ] sglang_95030.pbs111_rank2.nsys-rep[1,15]<stdout>:[1/1] [=========46%                ] sglang_95030.pbs111_rank15.nsys-rep[1,3]<stdout>:[1/1] [=16%                        ] sglang_95030.pbs111_rank3.nsys-rep[1,5]<stdout>:[1/1] [==========48%               ] sglang_95030.pbs111_rank5.nsys-rep[1,14]<stdout>:[1/1] [===========50%              ] sglang_95030.pbs111_rank14.nsys-rep[1,13]<stdout>:[1/1] [===========50%              ] sglang_95030.pbs111_rank13.nsys-rep[1,3]<stdout>:[1/1] [=17%                        ] sglang_95030.pbs111_rank3.nsys-rep[1,7]<stdout>:[1/1] [==========48%               ] sglang_95030.pbs111_rank7.nsys-rep[1,12]<stdout>:[1/1] [==========47%               ] sglang_95030.pbs111_rank12.nsys-rep[1,10]<stdout>:[1/1] [=========46%                ] sglang_95030.pbs111_rank10.nsys-rep[1,0]<stdout>:[1/1] [=========44%                ] sglang_95030.pbs111_rank0.nsys-rep[1,15]<stdout>:[1/1] [==========47%               ] sglang_95030.pbs111_rank15.nsys-rep[1,11]<stdout>:[1/1] [=========46%                ] sglang_95030.pbs111_rank11.nsys-rep[1,1]<stdout>:[1/1] [=========46%                ] sglang_95030.pbs111_rank1.nsys-rep[1,3]<stdout>:[1/1] [==18%                       ] sglang_95030.pbs111_rank3.nsys-rep[1,12]<stdout>:[1/1] [==========48%               ] sglang_95030.pbs111_rank12.nsys-rep[1,15]<stdout>:[1/1] [==========48%               ] sglang_95030.pbs111_rank15.nsys-rep[1,11]<stdout>:[1/1] [==========47%               ] sglang_95030.pbs111_rank11.nsys-rep[1,0]<stdout>:[1/1] [=========45%                ] sglang_95030.pbs111_rank0.nsys-rep[1,5]<stdout>:[1/1] [==========49%               ] sglang_95030.pbs111_rank5.nsys-rep[1,3]<stdout>:[1/1] [==20%                       ] sglang_95030.pbs111_rank3.nsys-rep[1,2]<stdout>:[1/1] [7%                          ] sglang_95030.pbs111_rank2.nsys-rep[1,1]<stdout>:[1/1] [==========47%               ] sglang_95030.pbs111_rank1.nsys-rep[1,12]<stdout>:[1/1] [==========49%               ] sglang_95030.pbs111_rank12.nsys-rep[1,7]<stdout>:[1/1] [===========50%              ] sglang_95030.pbs111_rank7.nsys-rep[1,6]<stdout>:[1/1] [5%                          ] sglang_95030.pbs111_rank6.nsys-rep[1,10]<stdout>:[1/1] [==========47%               ] sglang_95030.pbs111_rank10.nsys-rep[1,1]<stdout>:[1/1] [==========48%               ] sglang_95030.pbs111_rank1.nsys-rep[1,5]<stdout>:[1/1] [===========50%              ] sglang_95030.pbs111_rank5.nsys-rep[1,3]<stdout>:[1/1] [==21%                       ] sglang_95030.pbs111_rank3.nsys-rep[1,10]<stdout>:[1/1] [==========48%               ] sglang_95030.pbs111_rank10.nsys-rep[1,15]<stdout>:[1/1] [==========49%               ] sglang_95030.pbs111_rank15.nsys-rep[1,11]<stdout>:[1/1] [==========48%               ] sglang_95030.pbs111_rank11.nsys-rep[1,3]<stdout>:[1/1] [===22%                      ] sglang_95030.pbs111_rank3.nsys-rep[1,8]<stdout>:[1/1] [========41%                 ] sglang_95030.pbs111_rank8.nsys-rep[1,9]<stdout>:[1/1] [===========51%              ] sglang_95030.pbs111_rank9.nsys-rep[1,9]<stdout>:[1/1] [========================100%] sglang_95030.pbs111_rank9.nsys-rep[1,12]<stdout>:[1/1] [===========51%              ] sglang_95030.pbs111_rank12.nsys-rep[1,15]<stdout>:[1/1] [===========50%              ] sglang_95030.pbs111_rank15.nsys-rep[1,11]<stdout>:[1/1] [==========49%               ] sglang_95030.pbs111_rank11.nsys-rep[1,1]<stdout>:[1/1] [==========49%               ] sglang_95030.pbs111_rank1.nsys-rep[1,8]<stdout>:[1/1] [========42%                 ] sglang_95030.pbs111_rank8.nsys-rep[1,14]<stdout>:[1/1] [===========51%              ] sglang_95030.pbs111_rank14.nsys-rep[1/1] [========================100%] sglang_95030.pbs111_rank14.nsys-rep[1,10]<stdout>:[1/1] [===========50%              ] sglang_95030.pbs111_rank10.nsys-rep[1,0]<stdout>:[1/1] [=========46%                ] sglang_95030.pbs111_rank0.nsys-rep[1,11]<stdout>:[1/1] [===========50%              ] sglang_95030.pbs111_rank11.nsys-rep[1,13]<stdout>:[1/1] [===========51%              ] sglang_95030.pbs111_rank13.nsys-rep[1/1] [========================100%] sglang_95030.pbs111_rank13.nsys-rep[1,3]<stdout>:[1/1] [===24%                      ] sglang_95030.pbs111_rank3.nsys-rep[1,2]<stdout>:[1/1] [8%                          ] sglang_95030.pbs111_rank2.nsys-rep[1,1]<stdout>:[1/1] [===========50%              ] sglang_95030.pbs111_rank1.nsys-rep[1,10]<stdout>:[1/1] [===========51%              ] sglang_95030.pbs111_rank10.nsys-rep[1,6]<stdout>:[1/1] [6%                          ] sglang_95030.pbs111_rank6.nsys-rep[1,0]<stdout>:[1/1] [==========47%               ] sglang_95030.pbs111_rank0.nsys-rep[1,2]<stdout>:[1/1] [9%                          ] sglang_95030.pbs111_rank2.nsys-rep[1,4]<stdout>:[1/1] [5%                          ] sglang_95030.pbs111_rank4.nsys-rep[1,3]<stdout>:[1/1] [====25%                     ] sglang_95030.pbs111_rank3.nsys-rep[1,2]<stdout>:[1/1] [10%                         ] sglang_95030.pbs111_rank2.nsys-rep[1,5]<stdout>:[1/1] [===========51%              ] sglang_95030.pbs111_rank5.nsys-rep[1,5]<stdout>:[1/1] [========================100%] sglang_95030.pbs111_rank5.nsys-rep[1,8]<stdout>:[1/1] [=========43%                ] sglang_95030.pbs111_rank8.nsys-rep[1,6]<stdout>:[1/1] [7%                          ] sglang_95030.pbs111_rank6.nsys-rep[1,0]<stdout>:[1/1] [==========48%               ] sglang_95030.pbs111_rank0.nsys-rep[1,7]<stdout>:[1/1] [===========51%              ] sglang_95030.pbs111_rank7.nsys-rep[1,7]<stdout>:[1/1] [========================100%] sglang_95030.pbs111_rank7.nsys-rep[1,12]<stdout>:[1/1] [===========52%              ] sglang_95030.pbs111_rank12.nsys-rep[1/1] [========================100%] sglang_95030.pbs111_rank12.nsys-rep[1,15]<stdout>:[1/1] [===========51%              ] sglang_95030.pbs111_rank15.nsys-rep[1/1] [========================100%] sglang_95030.pbs111_rank15.nsys-rep[1,3]<stdout>:[1/1] [====26%                     ] sglang_95030.pbs111_rank3.nsys-rep[1,8]<stdout>:[1/1] [=========44%                ] sglang_95030.pbs111_rank8.nsys-rep[1,0]<stdout>:[1/1] [==========49%               ] sglang_95030.pbs111_rank0.nsys-rep[1,11]<stdout>:[1/1] [===========51%              ] sglang_95030.pbs111_rank11.nsys-rep[1/1] [========================100%] sglang_95030.pbs111_rank11.nsys-rep[1,10]<stdout>:[1/1] [===========52%              ] sglang_95030.pbs111_rank10.nsys-rep[1/1] [========================100%] sglang_95030.pbs111_rank10.nsys-rep[1,3]<stdout>:[1/1] [====27%                     ] sglang_95030.pbs111_rank3.nsys-rep[1,4]<stdout>:[1/1] [6%                          ] sglang_95030.pbs111_rank4.nsys-rep[1,0]<stdout>:[1/1] [===========50%              ] sglang_95030.pbs111_rank0.nsys-rep[1,1]<stdout>:[1/1] [===========51%              ] sglang_95030.pbs111_rank1.nsys-rep[1,1]<stdout>:[1/1] [========================100%] sglang_95030.pbs111_rank1.nsys-rep[1,8]<stdout>:[1/1] [=========45%                ] sglang_95030.pbs111_rank8.nsys-rep[1,6]<stdout>:[1/1] [8%                          ] sglang_95030.pbs111_rank6.nsys-rep[1,3]<stdout>:[1/1] [====28%                     ] sglang_95030.pbs111_rank3.nsys-rep[1,6]<stdout>:[1/1] [9%                          ] sglang_95030.pbs111_rank6.nsys-rep[1,8]<stdout>:[1/1] [=========46%                ] sglang_95030.pbs111_rank8.nsys-rep[1,4]<stdout>:[1/1] [7%                          ] sglang_95030.pbs111_rank4.nsys-rep[1,6]<stdout>:[1/1] [10%                         ] sglang_95030.pbs111_rank6.nsys-rep[1,3]<stdout>:[1/1] [=====29%                    ] sglang_95030.pbs111_rank3.nsys-rep[1,8]<stdout>:[1/1] [==========47%               ] sglang_95030.pbs111_rank8.nsys-rep[1,8]<stdout>:[1/1] [==========48%               ] sglang_95030.pbs111_rank8.nsys-rep[1,4]<stdout>:[1/1] [8%                          ] sglang_95030.pbs111_rank4.nsys-rep[1,8]<stdout>:[1/1] [==========49%               ] sglang_95030.pbs111_rank8.nsys-rep[1,0]<stdout>:[1/1] [===========51%              ] sglang_95030.pbs111_rank0.nsys-rep[1/1] [========================100%] sglang_95030.pbs111_rank0.nsys-rep[1,3]<stdout>:[1/1] [=====30%                    ] sglang_95030.pbs111_rank3.nsys-rep[1,4]<stdout>:[1/1] [9%                          ] sglang_95030.pbs111_rank4.nsys-rep[1,8]<stdout>:[1/1] [===========50%              ] sglang_95030.pbs111_rank8.nsys-rep[1,3]<stdout>:[1/1] [=====31%                    ] sglang_95030.pbs111_rank3.nsys-rep[1,3]<stdout>:[1/1] [=====32%                    ] sglang_95030.pbs111_rank3.nsys-rep[1,14]<stdout>:[1/1] [========================100%] sglang_95030.pbs111_rank14.nsys-rep
[1,14]<stdout>:Generated:
[1,14]<stdout>:	/home/users/industry/ai-hpc/apacsc34/scratch/nattanon/profiling/nsys_reports/sglang_95030.pbs111_rank14.nsys-rep
[1,9]<stdout>:[1/1] [========================100%] sglang_95030.pbs111_rank9.nsys-rep
[1,9]<stdout>:Generated:
[1,9]<stdout>:	/home/users/industry/ai-hpc/apacsc34/scratch/nattanon/profiling/nsys_reports/sglang_95030.pbs111_rank9.nsys-rep
[1,3]<stdout>:[1/1] [======33%                   ] sglang_95030.pbs111_rank3.nsys-rep[1,2]<stdout>:[1/1] [11%                         ] sglang_95030.pbs111_rank2.nsys-rep[1,13]<stdout>:[1/1] [========================100%] sglang_95030.pbs111_rank13.nsys-rep
[1,13]<stdout>:Generated:
[1,13]<stdout>:	/home/users/industry/ai-hpc/apacsc34/scratch/nattanon/profiling/nsys_reports/sglang_95030.pbs111_rank13.nsys-rep
[1,3]<stdout>:[1/1] [======34%                   ] sglang_95030.pbs111_rank3.nsys-rep[1,2]<stdout>:[1/1] [12%                         ] sglang_95030.pbs111_rank2.nsys-rep[1,5]<stdout>:[1/1] [========================100%] sglang_95030.pbs111_rank5.nsys-rep
[1,5]<stdout>:Generated:
[1,5]<stdout>:	[1,5]<stdout>:/home/users/industry/ai-hpc/apacsc34/scratch/nattanon/profiling/nsys_reports/sglang_95030.pbs111_rank5.nsys-rep
[1,3]<stdout>:[1/1] [======35%                   ] sglang_95030.pbs111_rank3.nsys-rep[1,3]<stdout>:[1/1] [=======37%                  ] sglang_95030.pbs111_rank3.nsys-rep[1,2]<stdout>:[1/1] [13%                         ] sglang_95030.pbs111_rank2.nsys-rep[1,12]<stdout>:[1/1] [========================100%] sglang_95030.pbs111_rank12.nsys-rep
[1,12]<stdout>:Generated:
[1,12]<stdout>:	/home/users/industry/ai-hpc/apacsc34/scratch/nattanon/profiling/nsys_reports/sglang_95030.pbs111_rank12.nsys-rep
[1,15]<stdout>:[1/1] [========================100%] sglang_95030.pbs111_rank15.nsys-rep
[1,15]<stdout>:Generated:
[1,15]<stdout>:	/home/users/industry/ai-hpc/apacsc34/scratch/nattanon/profiling/nsys_reports/sglang_95030.pbs111_rank15.nsys-rep
[1,3]<stdout>:[1/1] [=======38%                  ] sglang_95030.pbs111_rank3.nsys-rep[1,2]<stdout>:[1/1] [=15%                        ] sglang_95030.pbs111_rank2.nsys-rep[1,8]<stdout>:[1/1] [===========51%              ] sglang_95030.pbs111_rank8.nsys-rep[1/1] [========================100%] sglang_95030.pbs111_rank8.nsys-rep[1,11]<stdout>:[1/1] [========================100%] sglang_95030.pbs111_rank11.nsys-rep
[1,11]<stdout>:Generated:
[1,11]<stdout>:	/home/users/industry/ai-hpc/apacsc34/scratch/nattanon/profiling/nsys_reports/sglang_95030.pbs111_rank11.nsys-rep
[1,2]<stdout>:[1/1] [=16%                        ] sglang_95030.pbs111_rank2.nsys-rep[1,10]<stdout>:[1/1] [========================100%] sglang_95030.pbs111_rank10.nsys-rep
[1,10]<stdout>:Generated:
[1,10]<stdout>:	/home/users/industry/ai-hpc/apacsc34/scratch/nattanon/profiling/nsys_reports/sglang_95030.pbs111_rank10.nsys-rep
[1,2]<stdout>:[1/1] [=17%                        ] sglang_95030.pbs111_rank2.nsys-rep[1,3]<stdout>:[1/1] [=======39%                  ] sglang_95030.pbs111_rank3.nsys-rep[1,7]<stdout>:[1/1] [========================100%] sglang_95030.pbs111_rank7.nsys-rep
[1,7]<stdout>:Generated:
[1,7]<stdout>:	/home/users/industry/ai-hpc/apacsc34/scratch/nattanon/profiling/nsys_reports/sglang_95030.pbs111_rank7.nsys-rep
[1,2]<stdout>:[1/1] [==18%                       ] sglang_95030.pbs111_rank2.nsys-rep[1,2]<stdout>:[1/1] [==19%                       ] sglang_95030.pbs111_rank2.nsys-rep[1,3]<stdout>:[1/1] [========40%                 ] sglang_95030.pbs111_rank3.nsys-rep[1,1]<stdout>:[1/1] [========================100%] sglang_95030.pbs111_rank1.nsys-rep
[1,1]<stdout>:Generated:
[1,1]<stdout>:	/home/users/industry/ai-hpc/apacsc34/scratch/nattanon/profiling/nsys_reports/sglang_95030.pbs111_rank1.nsys-rep
[1,2]<stdout>:[1/1] [==21%                       ] sglang_95030.pbs111_rank2.nsys-rep[1,6]<stdout>:[1/1] [11%                         ] sglang_95030.pbs111_rank6.nsys-rep[1,2]<stdout>:[1/1] [===22%                      ] sglang_95030.pbs111_rank2.nsys-rep[1,6]<stdout>:[1/1] [12%                         ] sglang_95030.pbs111_rank6.nsys-rep[1,6]<stdout>:[1/1] [13%                         ] sglang_95030.pbs111_rank6.nsys-rep[1,3]<stdout>:[1/1] [========41%                 ] sglang_95030.pbs111_rank3.nsys-rep[1,6]<stdout>:[1/1] [=15%                        ] sglang_95030.pbs111_rank6.nsys-rep[1,2]<stdout>:[1/1] [===23%                      ] sglang_95030.pbs111_rank2.nsys-rep[1,6]<stdout>:[1/1] [=16%                        ] sglang_95030.pbs111_rank6.nsys-rep[1,4]<stdout>:[1/1] [10%                         ] sglang_95030.pbs111_rank4.nsys-rep[1,5]<stderr>:unrecognised option '--sqlite'[1,5]<stderr>:
[1,5]<stderr>:
[1,5]<stderr>:usage: nsys export [<args>] [nsys-rep-file]
[1,5]<stderr>:Try 'nsys export --help' for more information.
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
[1,6]<stdout>:[1/1] [=17%                        ] sglang_95030.pbs111_rank6.nsys-rep[1/1] [==18%                       ] sglang_95030.pbs111_rank6.nsys-rep[1/1] [==19%                       ] sglang_95030.pbs111_rank6.nsys-rep[1/1] [==21%                       ] sglang_95030.pbs111_rank6.nsys-rep[1/1] [===22%                      ] sglang_95030.pbs111_rank6.nsys-rep[1/1] [===23%                      ] sglang_95030.pbs111_rank6.nsys-rep[1/1] [====25%                     ] sglang_95030.pbs111_rank6.nsys-rep[1/1] [====26%                     ] sglang_95030.pbs111_rank6.nsys-rep[1/1] [====27%                     ] sglang_95030.pbs111_rank6.nsys-rep[1/1] [====28%                     ] sglang_95030.pbs111_rank6.nsys-rep[1,2]<stdout>:[1/1] [====25%                     ] sglang_95030.pbs111_rank2.nsys-rep[1/1] [====26%                     ] sglang_95030.pbs111_rank2.nsys-rep[1/1] [====27%                     ] sglang_95030.pbs111_rank2.nsys-rep[1/1] [====28%                     ] sglang_95030.pbs111_rank2.nsys-rep[1/1] [=====29%                    ] sglang_95030.pbs111_rank2.nsys-rep[1/1] [=====30%                    ] sglang_95030.pbs111_rank2.nsys-rep[1/1] [=====31%                    ] sglang_95030.pbs111_rank2.nsys-rep[1/1] [=====32%                    ] sglang_95030.pbs111_rank2.nsys-rep[1/1] [======33%                   ] sglang_95030.pbs111_rank2.nsys-rep[1,4]<stdout>:[1/1] [11%                         ] sglang_95030.pbs111_rank4.nsys-rep[1/1] [12%                         ] sglang_95030.pbs111_rank4.nsys-rep[1/1] [13%                         ] sglang_95030.pbs111_rank4.nsys-rep[1/1] [14%                         ] sglang_95030.pbs111_rank4.nsys-rep[1/1] [=16%                        ] sglang_95030.pbs111_rank4.nsys-rep[1/1] [=17%                        ] sglang_95030.pbs111_rank4.nsys-rep[1/1] [==18%                       ] sglang_95030.pbs111_rank4.nsys-rep[1/1] [==19%                       ] sglang_95030.pbs111_rank4.nsys-rep[1/1] [==21%                       ] sglang_95030.pbs111_rank4.nsys-rep[1/1] [===22%                      ] sglang_95030.pbs111_rank4.nsys-rep[1/1] [===23%                      ] sglang_95030.pbs111_rank4.nsys-rep[1/1] [====25%                     ] sglang_95030.pbs111_rank4.nsys-rep[1/1] [====26%                     ] sglang_95030.pbs111_rank4.nsys-rep[1,3]<stdout>:[1/1] [========42%                 ] sglang_95030.pbs111_rank3.nsys-rep[1/1] [=========43%                ] sglang_95030.pbs111_rank3.nsys-rep[1/1] [=========44%                ] sglang_95030.pbs111_rank3.nsys-rep[1/1] [=========45%                ] sglang_95030.pbs111_rank3.nsys-rep[1/1] [=========46%                ] sglang_95030.pbs111_rank3.nsys-rep[1/1] [==========47%               ] sglang_95030.pbs111_rank3.nsys-rep[1/1] [==========48%               ] sglang_95030.pbs111_rank3.nsys-rep[1/1] [==========49%               ] sglang_95030.pbs111_rank3.nsys-rep[1/1] [===========50%              ] sglang_95030.pbs111_rank3.nsys-rep[1,0]<stdout>:[1/1] [========================100%] sglang_95030.pbs111_rank0.nsys-rep
[1,0]<stdout>:Generated:
[1,0]<stdout>:	/home/users/industry/ai-hpc/apacsc34/scratch/nattanon/profiling/nsys_reports/sglang_95030.pbs111_rank0.nsys-rep
[1,8]<stdout>:[1/1] [========================100%] sglang_95030.pbs111_rank8.nsys-rep
[1,8]<stdout>:Generated:
[1,8]<stdout>:	/home/users/industry/ai-hpc/apacsc34/scratch/nattanon/profiling/nsys_reports/sglang_95030.pbs111_rank8.nsys-rep
[1,7]<stderr>:unrecognised option '--sqlite'
[1,7]<stderr>:
[1,7]<stderr>:usage: nsys export [<args>] [nsys-rep-file]
[1,7]<stderr>:Try 'nsys export --help' for more information.
[1,9]<stderr>:unrecognised option '--sqlite'
[1,9]<stderr>:
[1,9]<stderr>:usage: nsys export [<args>] [nsys-rep-file]
[1,9]<stderr>:Try 'nsys export --help' for more information.
[1,14]<stderr>:unrecognised option '--sqlite'
[1,14]<stderr>:
[1,14]<stderr>:usage: nsys export [<args>] [nsys-rep-file]
[1,14]<stderr>:Try 'nsys export --help' for more information.
[1,13]<stderr>:unrecognised option '--sqlite'
[1,13]<stderr>:
[1,13]<stderr>:usage: nsys export [<args>] [nsys-rep-file]
[1,13]<stderr>:Try 'nsys export --help' for more information.
--------------------------------------------------------------------------
mpirun detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[5856,1],5]
  Exit code:    1
--------------------------------------------------------------------------

real	3m14.805s
user	0m0.372s
sys	0m8.142s
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
			Resource Usage on 2025-10-07 12:08:26.155415:
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	JobId: 95030.pbs111
	Project: 50000128
	Exit Status: 0
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	NCPUs: Requested(224), Used(224)
	CPU Time Used: 00:27:26
	Memory: Requested(3760gb), Used(60549100kb)
	Vmem Used: 112159700kb
	Walltime: Requested(00:07:00), Used(00:03:19)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	Execution Nodes Used: (a2ap-dgx003:ncpus=112:ngpus=8:mem=1971322880kb)+(a2ap-dgx006:ncpus=112:ngpus=8:mem=1971322880kb)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	GPU Duration: 3.45mins
	GPU Power Consumed: 141.51W
	GPU Max GPU Memory Used: 544.0MB
	Memory Throughput Rate (Average): a2ap-dgx003:(gpu1:0%+gpu0:0%+gpu2:0%+gpu3:0%+gpu5:0%+gpu4:0%+gpu6:0%+gpu7:0%)+a2ap-dgx006:(gpu1:0%+gpu0:0%+gpu2:0%+gpu3:0%+gpu5:0%+gpu4:0%+gpu6:0%+gpu7:0%)
	Memory Throughput Rate (Max): a2ap-dgx003:(gpu1:0%+gpu0:0%+gpu2:0%+gpu3:0%+gpu5:0%+gpu4:0%+gpu6:0%+gpu7:0%)+a2ap-dgx006:(gpu1:0%+gpu0:0%+gpu2:0%+gpu3:0%+gpu5:0%+gpu4:0%+gpu6:0%+gpu7:0%)
	Memory Throughput Rate (Min): a2ap-dgx003:(gpu1:0%+gpu0:0%+gpu2:0%+gpu3:0%+gpu5:0%+gpu4:0%+gpu6:0%+gpu7:0%)+a2ap-dgx006:(gpu1:0%+gpu0:0%+gpu2:0%+gpu3:0%+gpu5:0%+gpu4:0%+gpu6:0%+gpu7:0%)
	GPU SM Utilization (Average): a2ap-dgx003:(gpu1:0%+gpu0:0%+gpu2:0%+gpu3:0%+gpu5:0%+gpu4:0%+gpu6:0%+gpu7:0%)+a2ap-dgx006:(gpu1:0%+gpu0:0%+gpu2:0%+gpu3:0%+gpu5:0%+gpu4:0%+gpu6:0%+gpu7:0%)
	GPU SM Utilization (Max): a2ap-dgx003:(gpu1:0%+gpu0:0%+gpu2:0%+gpu3:0%+gpu5:0%+gpu4:0%+gpu6:0%+gpu7:0%)+a2ap-dgx006:(gpu1:0%+gpu0:0%+gpu2:0%+gpu3:0%+gpu5:0%+gpu4:0%+gpu6:0%+gpu7:0%)
	GPU SM Utilization (Min): a2ap-dgx003:(gpu1:0%+gpu0:0%+gpu2:0%+gpu3:0%+gpu5:0%+gpu4:0%+gpu6:0%+gpu7:0%)+a2ap-dgx006:(gpu1:0%+gpu0:0%+gpu2:0%+gpu3:0%+gpu5:0%+gpu4:0%+gpu6:0%+gpu7:0%)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Warning: All GPUs have a percentage of 0 utilisation.
GPU application profile: Idle
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

