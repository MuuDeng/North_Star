========== OPTIMIZED TP16+DP2+MTP BENCHMARK ==========
Prepaid SU: 341.334 | 7min SU: 238.934 | Balance: 43095.540
N/A
Job ID: 97076.pbs111 | GPUs: 16 | Master: a2ap-dgx005.asp2p.nscc.sg:5000
Config: TP16 + DP2 + DP-Attention + Multi-Token Prediction (EAGLE)
Architecture: 2 DP groups × 8 TP GPUs | MLA + EP + FlashInfer
MTP Config: EAGLE steps=1, topk=1, draft_tokens=2
=========================================================
[22:03:35] Launching SGLang offline throughput benchmark...
[1,0]<stdout>:WARNING:sglang.srt.server_args:DP attention is enabled. The chunked prefill size is adjusted to 4096 to avoid MoE kernel issues. 
[1,0]<stdout>:WARNING:sglang.srt.server_args:Overlap scheduler is disabled because of using eagle speculative decoding.
[1,0]<stdout>:WARNING:sglang.srt.server_args:Mixed chunked prefill is disabled because of using eagle speculative decoding.
[1,0]<stdout>:WARNING:sglang.srt.server_args:DP attention is enabled. The chunked prefill size is adjusted to 2048 to avoid MoE kernel issues. 
[1,0]<stdout>:WARNING:sglang.srt.server_args:Overlap scheduler is disabled because of using eagle speculative decoding.
[1,0]<stdout>:WARNING:sglang.srt.server_args:DeepSeek MTP does not require setting speculative_draft_model_path.
[1,0]<stdout>:[2025-10-11 22:04:06] Using default HuggingFace chat template with detected content format: string
[1,1]<stdout>:WARNING:sglang.srt.server_args:DP attention is enabled. The chunked prefill size is adjusted to 4096 to avoid MoE kernel issues. 
[1,1]<stdout>:WARNING:sglang.srt.server_args:Overlap scheduler is disabled because of using eagle speculative decoding.
[1,1]<stdout>:WARNING:sglang.srt.server_args:Mixed chunked prefill is disabled because of using eagle speculative decoding.
[1,1]<stdout>:WARNING:sglang.srt.server_args:DP attention is enabled. The chunked prefill size is adjusted to 2048 to avoid MoE kernel issues. 
[1,1]<stdout>:WARNING:sglang.srt.server_args:Overlap scheduler is disabled because of using eagle speculative decoding.
[1,1]<stdout>:WARNING:sglang.srt.server_args:DeepSeek MTP does not require setting speculative_draft_model_path.
[1,0]<stdout>:[2025-10-11 22:04:53 DP0 TP0] MLA optimization is turned on. Use flashinfer backend.
[1,0]<stdout>:[2025-10-11 22:04:53 DP0 TP0] Chunked prefix cache is turned on.
[1,0]<stdout>:[2025-10-11 22:04:53 DP0 TP0] Init torch distributed begin.
[1,0]<stdout>:[Gloo] Rank 1 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,0]<stdout>:[Gloo] Rank 0 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,0]<stdout>:[Gloo] Rank 5 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,0]<stdout>:[Gloo] Rank 3 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,0]<stdout>:[Gloo] Rank 2 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,0]<stdout>:[Gloo] Rank 4 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,0]<stdout>:[Gloo] Rank 6 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,0]<stdout>:[Gloo] Rank 7 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,1]<stdout>:[Gloo] Rank 10 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,1]<stdout>:[Gloo] Rank 13 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,1]<stdout>:[Gloo] Rank 8 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,1]<stdout>:[Gloo] Rank 9 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,1]<stdout>:[Gloo] Rank 11 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,1]<stdout>:[Gloo] Rank 12 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,1]<stdout>:[Gloo] Rank 15 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,1]<stdout>:[Gloo] Rank 14 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,0]<stdout>:[Gloo] Rank 0 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,0]<stdout>:[2025-10-11 22:04:56 DP0 TP0] sglang is using nccl==2.27.3
[1,0]<stdout>:[Gloo] Rank 1 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,1]<stdout>:[Gloo] Rank 8 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,1]<stdout>:[Gloo] Rank 10 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,1]<stdout>:[Gloo] Rank 9 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,1]<stdout>:[Gloo] Rank 11 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,1]<stdout>:[Gloo] Rank 12 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,1]<stdout>:[Gloo] Rank 13 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,1]<stdout>:[Gloo] Rank 15 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,1]<stdout>:[Gloo] Rank 14 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,0]<stdout>:[Gloo] Rank 2 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,0]<stdout>:[Gloo] Rank 4 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,0]<stdout>:[Gloo] Rank 7 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,0]<stdout>:[Gloo] Rank 6 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,0]<stdout>:[Gloo] Rank 3 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,0]<stdout>:[Gloo] Rank 5 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,0]<stdout>:NCCL version 2.27.3+cuda12.9
[1,0]<stdout>:[2025-10-11 22:05:04 DP0 TP0] Custom allreduce is disabled because this process group spans across nodes.
[1,0]<stdout>:[2025-10-11 22:05:04 DP0 TP1] Custom allreduce is disabled because this process group spans across nodes.
[1,0]<stdout>:[2025-10-11 22:05:04 DP0 TP3] Custom allreduce is disabled because this process group spans across nodes.
[1,0]<stdout>:[2025-10-11 22:05:04 DP0 TP2] Custom allreduce is disabled because this process group spans across nodes.
[1,0]<stdout>:[2025-10-11 22:05:04 DP0 TP4] Custom allreduce is disabled because this process group spans across nodes.
[1,1]<stdout>:[2025-10-11 22:05:04 DP1 TP14] Custom allreduce is disabled because this process group spans across nodes.
[1,1]<stdout>:[2025-10-11 22:05:04 DP1 TP15] Custom allreduce is disabled because this process group spans across nodes.
[1,0]<stdout>:[2025-10-11 22:05:04 DP0 TP6] Custom allreduce is disabled because this process group spans across nodes.
[1,0]<stdout>:[2025-10-11 22:05:04 DP0 TP5] Custom allreduce is disabled because this process group spans across nodes.
[1,1]<stdout>:[2025-10-11 22:05:04 DP1 TP13] Custom allreduce is disabled because this process group spans across nodes.
[1,1]<stdout>:[2025-10-11 22:05:04 DP1 TP10] Custom allreduce is disabled because this process group spans across nodes.
[1,1]<stdout>:[2025-10-11 22:05:04 DP1 TP12] Custom allreduce is disabled because this process group spans across nodes.
[1,1]<stdout>:[2025-10-11 22:05:04 DP1 TP9] Custom allreduce is disabled because this process group spans across nodes.
[1,1]<stdout>:[2025-10-11 22:05:04 DP1 TP11] Custom allreduce is disabled because this process group spans across nodes.
[1,1]<stdout>:[2025-10-11 22:05:04 DP1 TP8] Custom allreduce is disabled because this process group spans across nodes.
[1,0]<stdout>:[2025-10-11 22:05:04 DP0 TP7] Custom allreduce is disabled because this process group spans across nodes.
[1,0]<stdout>:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1,0]<stdout>:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1,0]<stdout>:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1,0]<stdout>:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1,0]<stdout>:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1,0]<stdout>:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1,0]<stdout>:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1,0]<stdout>:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1,1]<stdout>:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1,1]<stdout>:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1,1]<stdout>:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1,1]<stdout>:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1,1]<stdout>:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1,1]<stdout>:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1,1]<stdout>:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1,1]<stdout>:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1,0]<stdout>:[Gloo] Rank 1 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,0]<stdout>:[Gloo] Rank 0 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,0]<stdout>:[Gloo] Rank 2 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,0]<stdout>:[Gloo] Rank 3 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,1]<stdout>:[Gloo] Rank 15 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,0]<stdout>:[2025-10-11 22:05:04 DP0 TP0] sglang is using nccl==2.27.3
[1,0]<stdout>:[Gloo] Rank 4 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,0]<stdout>:[Gloo] Rank 5 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,0]<stdout>:[Gloo] Rank 6 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,0]<stdout>:[Gloo] Rank 7 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,1]<stdout>:[Gloo] Rank 9 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,1]<stdout>:[Gloo] Rank 8 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,1]<stdout>:[Gloo] Rank 10 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,1]<stdout>:[Gloo] Rank 11 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,1]<stdout>:[Gloo] Rank 13 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,1]<stdout>:[Gloo] Rank 12 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,1]<stdout>:[Gloo] Rank 14 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,1]<stdout>:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1,0]<stdout>:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1,0]<stdout>:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1,0]<stdout>:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1,0]<stdout>:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1,0]<stdout>:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1,0]<stdout>:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1,0]<stdout>:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1,0]<stdout>:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1,1]<stdout>:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1,1]<stdout>:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1,1]<stdout>:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1,1]<stdout>:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1,1]<stdout>:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1,1]<stdout>:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1,1]<stdout>:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1,0]<stdout>:[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[1,0]<stdout>:[Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[1,0]<stdout>:[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[1,0]<stdout>:[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[1,0]<stdout>:[Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[1,0]<stdout>:[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[1,0]<stdout>:[Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[1,0]<stdout>:[Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[1,1]<stdout>:[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[1,1]<stdout>:[Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[1,1]<stdout>:[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[1,1]<stdout>:[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[1,1]<stdout>:[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[1,1]<stdout>:[Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[1,1]<stdout>:[Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[1,1]<stdout>:[Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[1,0]<stdout>:[2025-10-11 22:05:07 DP0 TP0] Init torch distributed ends. mem usage=2.18 GB
[1,1]<stdout>:[2025-10-11 22:05:08 DP1 TP8] Ignore import error when loading sglang.srt.models.glm4v_moe: No module named 'transformers.models.glm4v_moe'
[1,1]<stdout>:[2025-10-11 22:05:08 DP1 TP11] Ignore import error when loading sglang.srt.models.glm4v_moe: No module named 'transformers.models.glm4v_moe'
[1,1]<stdout>:[2025-10-11 22:05:08 DP1 TP9] Ignore import error when loading sglang.srt.models.glm4v_moe: No module named 'transformers.models.glm4v_moe'
[1,1]<stdout>:[2025-10-11 22:05:08 DP1 TP14] Ignore import error when loading sglang.srt.models.glm4v_moe: No module named 'transformers.models.glm4v_moe'
[1,1]<stdout>:[2025-10-11 22:05:08 DP1 TP10] Ignore import error when loading sglang.srt.models.glm4v_moe: No module named 'transformers.models.glm4v_moe'
[1,1]<stdout>:[2025-10-11 22:05:08 DP1 TP15] Ignore import error when loading sglang.srt.models.glm4v_moe: No module named 'transformers.models.glm4v_moe'
[1,1]<stdout>:[2025-10-11 22:05:08 DP1 TP12] Ignore import error when loading sglang.srt.models.glm4v_moe: No module named 'transformers.models.glm4v_moe'
[1,1]<stdout>:[2025-10-11 22:05:08 DP1 TP13] Ignore import error when loading sglang.srt.models.glm4v_moe: No module named 'transformers.models.glm4v_moe'
[1,0]<stdout>:[2025-10-11 22:05:08 DP0 TP1] Ignore import error when loading sglang.srt.models.glm4v_moe: No module named 'transformers.models.glm4v_moe'
[1,0]<stdout>:[2025-10-11 22:05:08 DP0 TP7] Ignore import error when loading sglang.srt.models.glm4v_moe: No module named 'transformers.models.glm4v_moe'
[1,0]<stdout>:[2025-10-11 22:05:08 DP0 TP2] Ignore import error when loading sglang.srt.models.glm4v_moe: No module named 'transformers.models.glm4v_moe'
[1,0]<stdout>:[2025-10-11 22:05:08 DP0 TP4] Ignore import error when loading sglang.srt.models.glm4v_moe: No module named 'transformers.models.glm4v_moe'
[1,0]<stdout>:[2025-10-11 22:05:08 DP0 TP3] Ignore import error when loading sglang.srt.models.glm4v_moe: No module named 'transformers.models.glm4v_moe'
[1,0]<stdout>:[2025-10-11 22:05:08 DP0 TP6] Ignore import error when loading sglang.srt.models.glm4v_moe: No module named 'transformers.models.glm4v_moe'
[1,0]<stdout>:[2025-10-11 22:05:08 DP0 TP5] Ignore import error when loading sglang.srt.models.glm4v_moe: No module named 'transformers.models.glm4v_moe'
[1,0]<stdout>:[2025-10-11 22:05:08 DP0 TP0] Ignore import error when loading sglang.srt.models.glm4v_moe: No module named 'transformers.models.glm4v_moe'
[1,0]<stdout>:[2025-10-11 22:05:09 DP0 TP0] Load weight begin. avail mem=76.36 GB
[1,0]<stdout>:[2025-10-11 22:05:09 DP0 TP0] Detected fp8 checkpoint.
[1,0]<stdout>:[2025-10-11 22:05:27 DP0 TP0] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=33.68 GB, mem usage=42.69 GB.
[1,0]<stdout>:[2025-10-11 22:05:37 DP0 TP0] KV Cache is allocated. #tokens: 170034, KV size: 11.13 GB
[1,0]<stdout>:[2025-10-11 22:05:37 DP0 TP0] Memory pool end. avail mem=22.50 GB
[1,0]<stdout>:[2025-10-11 22:05:38 DP0 TP6] KV Cache is allocated. #tokens: 170034, KV size: 11.13 GB
[1,1]<stdout>:[2025-10-11 22:05:38 DP1 TP15] KV Cache is allocated. #tokens: 170034, KV size: 11.13 GB
[1,1]<stdout>:[2025-10-11 22:05:38 DP1 TP8] KV Cache is allocated. #tokens: 170034, KV size: 11.13 GB
[1,1]<stdout>:[2025-10-11 22:05:38 DP1 TP13] KV Cache is allocated. #tokens: 170034, KV size: 11.13 GB
[1,0]<stdout>:[2025-10-11 22:05:38 DP0 TP5] KV Cache is allocated. #tokens: 170034, KV size: 11.13 GB
[1,1]<stdout>:[2025-10-11 22:05:38 DP1 TP12] KV Cache is allocated. #tokens: 170034, KV size: 11.13 GB
[1,1]<stdout>:[2025-10-11 22:05:38 DP1 TP14] KV Cache is allocated. #tokens: 170034, KV size: 11.13 GB
[1,1]<stdout>:[2025-10-11 22:05:38 DP1 TP10] KV Cache is allocated. #tokens: 170034, KV size: 11.13 GB
[1,1]<stdout>:[2025-10-11 22:05:38 DP1 TP11] KV Cache is allocated. #tokens: 170034, KV size: 11.13 GB
[1,0]<stdout>:[2025-10-11 22:05:38 DP0 TP2] KV Cache is allocated. #tokens: 170034, KV size: 11.13 GB
[1,0]<stdout>:[2025-10-11 22:05:38 DP0 TP1] KV Cache is allocated. #tokens: 170034, KV size: 11.13 GB
[1,0]<stdout>:[2025-10-11 22:05:38 DP0 TP4] KV Cache is allocated. #tokens: 170034, KV size: 11.13 GB
[1,0]<stdout>:[2025-10-11 22:05:38 DP0 TP7] KV Cache is allocated. #tokens: 170034, KV size: 11.13 GB
[1,1]<stdout>:[2025-10-11 22:05:38 DP1 TP9] KV Cache is allocated. #tokens: 170034, KV size: 11.13 GB
[1,0]<stdout>:[2025-10-11 22:05:38 DP0 TP3] KV Cache is allocated. #tokens: 170034, KV size: 11.13 GB
[1,0]<stdout>:[2025-10-11 22:05:39 DP0 TP0] Capture cuda graph begin. This can take up to several minutes. avail mem=22.04 GB
[1,0]<stdout>:[2025-10-11 22:05:40 DP0 TP0] Capture cuda graph bs [8, 16, 24, 32, 40, 48, 56, 64]
[1,0]<stdout>:  0% 0/8 [00:00<?, ?it/s][1,0]<stdout>:Capturing batches (bs=64 avail_mem=21.95 GB):   0% 0/8 [00:00<?, ?it/s][1,1]<stdout>:[2025-10-11 22:05:41 DP1 TP13] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=2112, K=7168, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-11 22:05:41 DP1 TP14] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=2112, K=7168, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-11 22:05:41 DP1 TP8] Entering DeepGEMM JIT Pre-Compile session. It may takes a long time (typically 10-20 mins) if you have not run `sglang.compile_deep_gemm`. It is recommended to run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to reduce the overhead if you have not run it before. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[1,1]<stdout>:[2025-10-11 22:05:41 DP1 TP12] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=2112, K=7168, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-11 22:05:41 DP1 TP11] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=2112, K=7168, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-11 22:05:41 DP1 TP10] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=2112, K=7168, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-11 22:05:41 DP1 TP15] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=2112, K=7168, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-11 22:05:41 DP1 TP8] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=2112, K=7168, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-11 22:05:41 DP1 TP9] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=2112, K=7168, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-11 22:05:41 DP0 TP3] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=2112, K=7168, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-11 22:05:41 DP0 TP5] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=2112, K=7168, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-11 22:05:41 DP0 TP7] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=2112, K=7168, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-11 22:05:41 DP0 TP0] Entering DeepGEMM JIT Pre-Compile session. It may takes a long time (typically 10-20 mins) if you have not run `sglang.compile_deep_gemm`. It is recommended to run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to reduce the overhead if you have not run it before. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[1,0]<stdout>:[2025-10-11 22:05:41 DP0 TP6] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=2112, K=7168, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-11 22:05:41 DP0 TP4] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=2112, K=7168, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-11 22:05:41 DP0 TP2] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=2112, K=7168, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-11 22:05:41 DP0 TP1] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=2112, K=7168, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-11 22:05:41 DP0 TP0] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=2112, K=7168, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:  0% 0/33 [00:00<?, ?it/s]  0% 0/33 [00:00<?, ?it/s][1,1]<stdout>:  0% 0/33 [00:00<?, ?it/s][1,1]<stdout>:  0% 0/33 [00:00<?, ?it/s][1,1]<stdout>:  0% 0/33 [00:00<?, ?it/s][1,1]<stdout>:  0% 0/33 [00:00<?, ?it/s][1,1]<stdout>:  0% 0/33 [00:00<?, ?it/s][1,1]<stdout>:  0% 0/33 [00:00<?, ?it/s][1,1]<stdout>:100% 33/33 [00:00<00:00, 2474.60it/s]
[1,1]<stdout>:100% 33/33 [00:00<00:00, 4039.22it/s]
[1,1]<stdout>:100% 33/33 [00:00<00:00, 3236.04it/s][1,1]<stdout>:
[1,1]<stdout>:100% 33/33 [00:00<00:00, 2224.92it/s]
[1,1]<stdout>:100% 33/33 [00:00<00:00, 2471.60it/s]
[1,1]<stdout>:100% 33/33 [00:00<00:00, 2569.47it/s]
[1,1]<stdout>:100% 33/33 [00:00<00:00, 2186.53it/s]
[1,1]<stdout>:100% 33/33 [00:00<00:00, 3012.10it/s]
[1,0]<stdout>:  0% 0/33 [00:00<?, ?it/s][1,0]<stdout>:  0% 0/33 [00:00<?, ?it/s][1,0]<stdout>:  0% 0/33 [00:00<?, ?it/s][1,0]<stdout>:  0% 0/33 [00:00<?, ?it/s][1,0]<stdout>:
[1,0]<stdout>:  0% 0/33 [00:00<?, ?it/s][1,0]<stdout>:[A[1,0]<stdout>:  0% 0/33 [00:00<?, ?it/s][1,0]<stdout>:  0% 0/33 [00:00<?, ?it/s][1,0]<stdout>:  0% 0/33 [00:00<?, ?it/s][1,0]<stdout>:100% 33/33 [00:00<00:00, 3103.76it/s]
[1,0]<stdout>:100% 33/33 [00:00<00:00, 2505.15it/s]
[1,0]<stdout>:100% 33/33 [00:00<00:00, 3162.55it/s]100% 33/33 [00:00<00:00, 4015.09it/s][1,0]<stdout>:
[1,0]<stdout>:
[1,0]<stdout>:100% 33/33 [00:00<00:00, 2451.81it/s]
[1,0]<stdout>:100% 33/33 [00:00<00:00, 2076.86it/s]
[1,0]<stdout>:100% 33/33 [00:00<00:00, 4429.47it/s]
[1,0]<stdout>:100% 33/33 [00:00<00:00, 4661.44it/s]
[1,1]<stdout>:[2025-10-11 22:05:42 DP1 TP15] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=3072, K=1536, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-11 22:05:42 DP1 TP13] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=3072, K=1536, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-11 22:05:42 DP1 TP8] Entering DeepGEMM JIT Pre-Compile session. It may takes a long time (typically 10-20 mins) if you have not run `sglang.compile_deep_gemm`. It is recommended to run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to reduce the overhead if you have not run it before. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[1,1]<stdout>:[2025-10-11 22:05:42 DP1 TP8] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=3072, K=1536, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-11 22:05:42 DP1 TP9] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=3072, K=1536, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-11 22:05:42 DP1 TP14] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=3072, K=1536, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-11 22:05:42 DP1 TP11] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=3072, K=1536, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-11 22:05:42 DP1 TP12] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=3072, K=1536, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-11 22:05:42 DP1 TP10] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=3072, K=1536, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-11 22:05:42 DP0 TP5] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=3072, K=1536, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-11 22:05:42 DP0 TP3] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=3072, K=1536, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-11 22:05:42 DP0 TP7] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=3072, K=1536, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-11 22:05:42 DP0 TP4] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=3072, K=1536, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-11 22:05:42 DP0 TP1] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=3072, K=1536, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-11 22:05:42 DP0 TP6] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=3072, K=1536, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-11 22:05:42 DP0 TP0] Entering DeepGEMM JIT Pre-Compile session. It may takes a long time (typically 10-20 mins) if you have not run `sglang.compile_deep_gemm`. It is recommended to run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to reduce the overhead if you have not run it before. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[1,0]<stdout>:[2025-10-11 22:05:42 DP0 TP0] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=3072, K=1536, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-11 22:05:42 DP0 TP2] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=3072, K=1536, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:  0% 0/29 [00:00<?, ?it/s][1,1]<stdout>:100% 29/29 [00:00<00:00, 10524.77it/s]
[1,1]<stdout>:  0% 0/29 [00:00<?, ?it/s][1,1]<stdout>:  0% 0/29 [00:00<?, ?it/s][1,1]<stdout>:100% 29/29 [00:00<00:00, 12208.65it/s]
[1,1]<stdout>:  0% 0/29 [00:00<?, ?it/s][1,1]<stdout>:100% 29/29 [00:00<00:00, 14401.47it/s]
[1,1]<stdout>:100% 29/29 [00:00<00:00, 14145.23it/s]
[1,1]<stdout>:  0% 0/29 [00:00<?, ?it/s][1,1]<stdout>:100% 29/29 [00:00<00:00, 13998.71it/s]
[1,1]<stdout>:  0% 0/29 [00:00<?, ?it/s][1,1]<stdout>:  0% 0/29 [00:00<?, ?it/s][1,1]<stdout>:100% 29/29 [00:00<00:00, 14014.84it/s]
[1,1]<stdout>:100% 29/29 [00:00<00:00, 12863.24it/s]
[1,1]<stdout>:  0% 0/29 [00:00<?, ?it/s][1,1]<stdout>:100% 29/29 [00:00<00:00, 12787.51it/s]
[1,0]<stdout>:  0% 0/29 [00:00<?, ?it/s][1,0]<stdout>:100% 29/29 [00:00<00:00, 10647.31it/s]
[1,0]<stdout>:  0% 0/29 [00:00<?, ?it/s][1,0]<stdout>:100% 29/29 [00:00<00:00, 14667.17it/s]
[1,0]<stdout>:  0% 0/29 [00:00<?, ?it/s][1,0]<stdout>:100% 29/29 [00:00<00:00, 13222.61it/s]
[1,0]<stdout>:  0% 0/29 [00:00<?, ?it/s][1,0]<stdout>:100% 29/29 [00:00<00:00, 13452.20it/s]
[1,0]<stdout>:  0% 0/29 [00:00<?, ?it/s][1,0]<stdout>:  0% 0/29 [00:00<?, ?it/s][1,0]<stdout>:100% 29/29 [00:00<00:00, 12939.87it/s]
[1,0]<stdout>:100% 29/29 [00:00<00:00, 12477.93it/s]
[1,1]<stdout>:[2025-10-11 22:05:43 DP1 TP11] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=7168, K=2048, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-11 22:05:43 DP1 TP14] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=7168, K=2048, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-11 22:05:43 DP1 TP9] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=7168, K=2048, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-11 22:05:43 DP1 TP10] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=7168, K=2048, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-11 22:05:43 DP1 TP15] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=7168, K=2048, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-11 22:05:43 DP1 TP13] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=7168, K=2048, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-11 22:05:43 DP1 TP12] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=7168, K=2048, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-11 22:05:43 DP1 TP8] Entering DeepGEMM JIT Pre-Compile session. It may takes a long time (typically 10-20 mins) if you have not run `sglang.compile_deep_gemm`. It is recommended to run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to reduce the overhead if you have not run it before. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[1,1]<stdout>:[2025-10-11 22:05:43 DP1 TP8] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=7168, K=2048, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-11 22:05:43 DP0 TP4] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=7168, K=2048, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-11 22:05:43 DP0 TP3] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=7168, K=2048, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-11 22:05:43 DP0 TP7] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=7168, K=2048, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-11 22:05:43 DP0 TP6] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=7168, K=2048, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-11 22:05:43 DP0 TP2] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=7168, K=2048, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-11 22:05:43 DP0 TP1] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=7168, K=2048, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:  0% 0/29 [00:00<?, ?it/s][1,0]<stdout>:100% 29/29 [00:00<00:00, 12928.87it/s][1,0]<stdout>:
[1,0]<stdout>:
[1,0]<stdout>:  0% 0/29 [00:00<?, ?it/s][1,0]<stdout>:[A[1,0]<stdout>:100% 29/29 [00:00<00:00, 14155.10it/s]
[1,0]<stdout>:[2025-10-11 22:05:43 DP0 TP5] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=7168, K=2048, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-11 22:05:43 DP0 TP0] Entering DeepGEMM JIT Pre-Compile session. It may takes a long time (typically 10-20 mins) if you have not run `sglang.compile_deep_gemm`. It is recommended to run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to reduce the overhead if you have not run it before. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[1,0]<stdout>:[2025-10-11 22:05:43 DP0 TP0] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=7168, K=2048, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:  0% 0/16 [00:00<?, ?it/s][1,1]<stdout>:  0% 0/16 [00:00<?, ?it/s][1,1]<stdout>:100% 16/16 [00:00<00:00, 8817.35it/s]
[1,1]<stdout>:100% 16/16 [00:00<00:00, 10859.04it/s]
[1,1]<stdout>:  0% 0/16 [00:00<?, ?it/s][1,1]<stdout>:  0% 0/16 [00:00<?, ?it/s][1,1]<stdout>:  0% 0/16 [00:00<?, ?it/s][1,1]<stdout>:100% 16/16 [00:00<00:00, 14860.24it/s]
[1,1]<stdout>:100% 16/16 [00:00<00:00, 12228.29it/s]
[1,1]<stdout>:  0% 0/16 [00:00<?, ?it/s][1,1]<stdout>:100% 16/16 [00:00<00:00, 13066.37it/s]
[1,1]<stdout>:100% 16/16 [00:00<00:00, 13831.18it/s]
[1,1]<stdout>:  0% 0/16 [00:00<?, ?it/s][1,1]<stdout>:100% 16/16 [00:00<00:00, 13960.65it/s]
[1,1]<stdout>:  0% 0/16 [00:00<?, ?it/s][1,1]<stdout>:NCCL version 2.27.3+cuda12.9
[1,1]<stdout>:100% 16/16 [00:00<00:00, 14585.71it/s][1,1]<stdout>:
[1,0]<stdout>:  0% 0/16 [00:00<?, ?it/s][1,0]<stdout>:100% 16/16 [00:00<00:00, 8561.99it/s]
[1,0]<stdout>:  0% 0/16 [00:00<?, ?it/s][1,0]<stdout>:100% 16/16 [00:00<00:00, 15293.72it/s]
[1,0]<stdout>:  0% 0/16 [00:00<?, ?it/s][1,0]<stdout>:100% 16/16 [00:00<00:00, 14713.63it/s]
[1,0]<stdout>:  0% 0/16 [00:00<?, ?it/s][1,0]<stdout>:  0% 0/16 [00:00<?, ?it/s][1,0]<stdout>:100% 16/16 [00:00<00:00, 16802.42it/s]
[1,0]<stdout>:  0% 0/16 [00:00<?, ?it/s][1,0]<stdout>:100% 16/16 [00:00<00:00, 12890.68it/s]
[1,0]<stdout>:100% 16/16 [00:00<00:00, 16710.37it/s]
[1,0]<stdout>:
[1,0]<stdout>:  0% 0/16 [00:00<?, ?it/s][A[1,0]<stdout>:100% 16/16 [00:00<00:00, 14557.24it/s]
[1,0]<stdout>:  0% 0/16 [00:00<?, ?it/s][1,0]<stdout>:100% 16/16 [00:00<00:00, 16904.00it/s]
[1,1]<stdout>:[2025-10-11 22:05:46 DP1 TP14] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=2304, K=7168, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-11 22:05:46 DP1 TP15] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=2304, K=7168, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-11 22:05:46 DP1 TP12] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=2304, K=7168, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-11 22:05:46 DP1 TP13] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=2304, K=7168, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-11 22:05:46 DP1 TP9] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=2304, K=7168, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-11 22:05:46 DP1 TP11] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=2304, K=7168, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-11 22:05:46 DP1 TP8] Entering DeepGEMM JIT Pre-Compile session. It may takes a long time (typically 10-20 mins) if you have not run `sglang.compile_deep_gemm`. It is recommended to run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to reduce the overhead if you have not run it before. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[1,1]<stdout>:[2025-10-11 22:05:46 DP1 TP8] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=2304, K=7168, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-11 22:05:46 DP1 TP10] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=2304, K=7168, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-11 22:05:46 DP0 TP6] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=2304, K=7168, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-11 22:05:46 DP0 TP0] Entering DeepGEMM JIT Pre-Compile session. It may takes a long time (typically 10-20 mins) if you have not run `sglang.compile_deep_gemm`. It is recommended to run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to reduce the overhead if you have not run it before. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[1,0]<stdout>:[2025-10-11 22:05:46 DP0 TP0] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=2304, K=7168, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-11 22:05:46 DP0 TP4] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=2304, K=7168, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-11 22:05:46 DP0 TP2] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=2304, K=7168, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-11 22:05:46 DP0 TP1] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=2304, K=7168, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-11 22:05:46 DP0 TP7] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=2304, K=7168, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-11 22:05:46 DP0 TP3] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=2304, K=7168, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-11 22:05:46 DP0 TP5] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=2304, K=7168, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:  0% 0/32 [00:00<?, ?it/s][1,1]<stdout>:  0% 0/32 [00:00<?, ?it/s][1,1]<stdout>:100% 32/32 [00:00<00:00, 9650.40it/s][1,1]<stdout>:
[1,1]<stdout>:100% 32/32 [00:00<00:00, 13046.05it/s]
[1,1]<stdout>:  0% 0/32 [00:00<?, ?it/s][1,1]<stdout>:  0% 0/32 [00:00<?, ?it/s][1,1]<stdout>:100% 32/32 [00:00<00:00, 13881.24it/s]
[1,1]<stdout>:100% 32/32 [00:00<00:00, 13815.51it/s]
[1,1]<stdout>:  0% 0/32 [00:00<?, ?it/s][1,1]<stdout>:  0% 0/32 [00:00<?, ?it/s][1,1]<stdout>:100% 32/32 [00:00<00:00, 12987.97it/s]
[1,1]<stdout>:100% 32/32 [00:00<00:00, 11782.79it/s]
[1,1]<stdout>:[2025-10-11 22:05:46 DP1 TP12] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=7168, K=1152, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-11 22:05:46 DP1 TP11] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=7168, K=1152, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-11 22:05:46 DP1 TP8] Entering DeepGEMM JIT Pre-Compile session. It may takes a long time (typically 10-20 mins) if you have not run `sglang.compile_deep_gemm`. It is recommended to run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to reduce the overhead if you have not run it before. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[1,1]<stdout>:[2025-10-11 22:05:46 DP1 TP8] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=7168, K=1152, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-11 22:05:46 DP1 TP14] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=7168, K=1152, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-11 22:05:46 DP1 TP15] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=7168, K=1152, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:  0% 0/32 [00:00<?, ?it/s][1,1]<stdout>:[2025-10-11 22:05:46 DP1 TP10] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=7168, K=1152, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:100% 32/32 [00:00<00:00, 13590.29it/s]
[1,1]<stdout>:  0% 0/32 [00:00<?, ?it/s][1,1]<stdout>:100% 32/32 [00:00<00:00, 13374.96it/s]
[1,1]<stdout>:[2025-10-11 22:05:46 DP1 TP9] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=7168, K=1152, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-11 22:05:46 DP1 TP13] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=7168, K=1152, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:  0% 0/32 [00:00<?, ?it/s][1,0]<stdout>:
[1,0]<stdout>:  0% 0/32 [00:00<?, ?it/s][A[1,0]<stdout>:100% 32/32 [00:00<00:00, 10758.94it/s]100% 32/32 [00:00<00:00, 11767.29it/s]
[1,0]<stdout>:
[1,0]<stdout>:  0% 0/32 [00:00<?, ?it/s][1,0]<stdout>:  0% 0/32 [00:00<?, ?it/s][1,0]<stdout>:  0% 0/32 [00:00<?, ?it/s][1,0]<stdout>:100% 32/32 [00:00<00:00, 16340.12it/s]
[1,0]<stdout>:100% 32/32 [00:00<00:00, 12689.58it/s]
[1,0]<stdout>:100% 32/32 [00:00<00:00, 12311.29it/s][1,0]<stdout>:
[1,0]<stdout>:  0% 0/32 [00:00<?, ?it/s][1,0]<stdout>:[2025-10-11 22:05:47 DP0 TP6] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=7168, K=1152, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-11 22:05:47 DP0 TP0] Entering DeepGEMM JIT Pre-Compile session. It may takes a long time (typically 10-20 mins) if you have not run `sglang.compile_deep_gemm`. It is recommended to run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to reduce the overhead if you have not run it before. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[1,0]<stdout>:[2025-10-11 22:05:47 DP0 TP4] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=7168, K=1152, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-11 22:05:47 DP0 TP0] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=7168, K=1152, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-11 22:05:47 DP0 TP2] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=7168, K=1152, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-11 22:05:47 DP0 TP1] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=7168, K=1152, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:100% 32/32 [00:00<00:00, 12993.00it/s][1,0]<stdout>:
[1,0]<stdout>:  0% 0/32 [00:00<?, ?it/s][1,0]<stdout>:100% 32/32 [00:00<00:00, 12714.83it/s][1,0]<stdout>:
[1,0]<stdout>:  0% 0/32 [00:00<?, ?it/s][1,0]<stdout>:100% 32/32 [00:00<00:00, 12724.47it/s]
[1,0]<stdout>:[2025-10-11 22:05:47 DP0 TP5] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=7168, K=1152, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-11 22:05:47 DP0 TP7] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=7168, K=1152, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-11 22:05:47 DP0 TP3] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=7168, K=1152, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:  0% 0/16 [00:00<?, ?it/s][1,1]<stdout>:100% 16/16 [00:00<00:00, 9016.37it/s]
[1,1]<stdout>:  0% 0/16 [00:00<?, ?it/s][1,1]<stdout>:  0% 0/16 [00:00<?, ?it/s][1,1]<stdout>:100% 16/16 [00:00<00:00, 17093.44it/s]
[1,1]<stdout>:100% 16/16 [00:00<00:00, 16016.44it/s][1,1]<stdout>:
[1,1]<stdout>:  0% 0/16 [00:00<?, ?it/s][1,1]<stdout>:100% 16/16 [00:00<00:00, 16237.32it/s]
[1,1]<stdout>:  0% 0/16 [00:00<?, ?it/s][1,1]<stdout>:  0% 0/16 [00:00<?, ?it/s][1,1]<stdout>:100% 16/16 [00:00<00:00, 15782.89it/s][1,1]<stdout>:
[1,1]<stdout>:100% 16/16 [00:00<00:00, 15853.74it/s][1,1]<stdout>:
[1,1]<stdout>:  0% 0/16 [00:00<?, ?it/s][1,1]<stdout>:100% 16/16 [00:00<00:00, 12900.59it/s]
[1,1]<stdout>:  0% 0/16 [00:00<?, ?it/s][1,1]<stdout>:100% 16/16 [00:00<00:00, 13814.09it/s]
[1,0]<stdout>:  0% 0/16 [00:00<?, ?it/s][1,0]<stdout>:100% 16/16 [00:00<00:00, 7826.11it/s]
[1,0]<stdout>:
[1,0]<stdout>:  0% 0/16 [00:00<?, ?it/s][A[1,0]<stdout>:100% 16/16 [00:00<00:00, 13437.90it/s]
[1,0]<stdout>:  0% 0/16 [00:00<?, ?it/s][1,0]<stdout>:  0% 0/16 [00:00<?, ?it/s][1,0]<stdout>:100% 16/16 [00:00<00:00, 13187.04it/s]
[1,0]<stdout>:  0% 0/16 [00:00<?, ?it/s]100% 16/16 [00:00<00:00, 16004.98it/s][1,0]<stdout>:
[1,0]<stdout>:  0% 0/16 [00:00<?, ?it/s][1,0]<stdout>:100% 16/16 [00:00<00:00, 13834.03it/s]
[1,0]<stdout>:100% 16/16 [00:00<00:00, 13546.40it/s]
[1,0]<stdout>:  0% 0/16 [00:00<?, ?it/s][1,0]<stdout>:100% 16/16 [00:00<00:00, 13897.05it/s]
[1,0]<stdout>:  0% 0/16 [00:00<?, ?it/s][1,0]<stdout>:100% 16/16 [00:00<00:00, 14327.26it/s][1,0]<stdout>:
[1,0]<stdout>:[2025-10-11 22:05:50 DP0 TP6] Config file not found at /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_4_0/E=257,N=128,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json. Fallback to triton version 3.2.0 and use MoE kernel config from /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_2_0/E=257,N=128,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json. Performance might be sub-optimal!
[1,0]<stdout>:[2025-10-11 22:05:50 DP0 TP4] Config file not found at /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_4_0/E=257,N=128,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json. Fallback to triton version 3.2.0 and use MoE kernel config from /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_2_0/E=257,N=128,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json. Performance might be sub-optimal!
[1,0]<stdout>:[2025-10-11 22:05:50 DP0 TP1] Config file not found at /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_4_0/E=257,N=128,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json. Fallback to triton version 3.2.0 and use MoE kernel config from /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_2_0/E=257,N=128,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json. Performance might be sub-optimal!
[1,1]<stdout>:[2025-10-11 22:05:50 DP1 TP10] Config file not found at /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_4_0/E=257,N=128,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json. Fallback to triton version 3.2.0 and use MoE kernel config from /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_2_0/E=257,N=128,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json. Performance might be sub-optimal!
[1,1]<stdout>:[2025-10-11 22:05:50 DP1 TP13] Config file not found at /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_4_0/E=257,N=128,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json. Fallback to triton version 3.2.0 and use MoE kernel config from /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_2_0/E=257,N=128,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json. Performance might be sub-optimal!
[1,1]<stdout>:[2025-10-11 22:05:50 DP1 TP9] Config file not found at /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_4_0/E=257,N=128,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json. Fallback to triton version 3.2.0 and use MoE kernel config from /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_2_0/E=257,N=128,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json. Performance might be sub-optimal!
[1,1]<stdout>:[2025-10-11 22:05:50 DP1 TP14] Config file not found at /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_4_0/E=257,N=128,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json. Fallback to triton version 3.2.0 and use MoE kernel config from /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_2_0/E=257,N=128,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json. Performance might be sub-optimal!
[1,1]<stdout>:[2025-10-11 22:05:50 DP1 TP8] Config file not found at /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_4_0/E=257,N=128,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json. Fallback to triton version 3.2.0 and use MoE kernel config from /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_2_0/E=257,N=128,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json. Performance might be sub-optimal!
[1,1]<stdout>:[2025-10-11 22:05:50 DP1 TP15] Config file not found at /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_4_0/E=257,N=128,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json. Fallback to triton version 3.2.0 and use MoE kernel config from /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_2_0/E=257,N=128,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json. Performance might be sub-optimal!
[1,1]<stdout>:[2025-10-11 22:05:50 DP1 TP12] Config file not found at /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_4_0/E=257,N=128,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json. Fallback to triton version 3.2.0 and use MoE kernel config from /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_2_0/E=257,N=128,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json. Performance might be sub-optimal!
[1,1]<stdout>:[2025-10-11 22:05:50 DP1 TP11] Config file not found at /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_4_0/E=257,N=128,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json. Fallback to triton version 3.2.0 and use MoE kernel config from /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_2_0/E=257,N=128,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json. Performance might be sub-optimal!
[1,0]<stdout>:[2025-10-11 22:05:50 DP0 TP0] Config file not found at /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_4_0/E=257,N=128,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json. Fallback to triton version 3.2.0 and use MoE kernel config from /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_2_0/E=257,N=128,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json. Performance might be sub-optimal!
[1,0]<stdout>:[2025-10-11 22:05:50 DP0 TP7] Config file not found at /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_4_0/E=257,N=128,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json. Fallback to triton version 3.2.0 and use MoE kernel config from /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_2_0/E=257,N=128,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json. Performance might be sub-optimal!
[1,0]<stdout>:[2025-10-11 22:05:50 DP0 TP2] Config file not found at /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_4_0/E=257,N=128,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json. Fallback to triton version 3.2.0 and use MoE kernel config from /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_2_0/E=257,N=128,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json. Performance might be sub-optimal!
[1,0]<stdout>:[2025-10-11 22:05:50 DP0 TP5] Config file not found at /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_4_0/E=257,N=128,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json. Fallback to triton version 3.2.0 and use MoE kernel config from /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_2_0/E=257,N=128,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json. Performance might be sub-optimal!
[1,0]<stdout>:[2025-10-11 22:05:50 DP0 TP3] Config file not found at /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_4_0/E=257,N=128,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json. Fallback to triton version 3.2.0 and use MoE kernel config from /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_2_0/E=257,N=128,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json. Performance might be sub-optimal!
[1,0]<stdout>:Capturing batches (bs=64 avail_mem=21.95 GB):  12% 1/8 [00:11<01:18, 11.27s/it][1,0]<stdout>:Capturing batches (bs=56 avail_mem=20.25 GB):  12% 1/8 [00:11<01:18, 11.27s/it][1,0]<stdout>:Capturing batches (bs=56 avail_mem=20.25 GB):  25% 2/8 [00:12<00:31,  5.23s/it][1,0]<stdout>:Capturing batches (bs=48 avail_mem=20.21 GB):  25% 2/8 [00:12<00:31,  5.23s/it][1,0]<stdout>:Capturing batches (bs=48 avail_mem=20.21 GB):  38% 3/8 [00:13<00:15,  3.18s/it][1,0]<stdout>:Capturing batches (bs=40 avail_mem=20.15 GB):  38% 3/8 [00:13<00:15,  3.18s/it][1,0]<stdout>:Capturing batches (bs=40 avail_mem=20.15 GB):  50% 4/8 [00:13<00:08,  2.24s/it][1,0]<stdout>:Capturing batches (bs=32 avail_mem=20.12 GB):  50% 4/8 [00:13<00:08,  2.24s/it][1,1]<stdout>:AUTOTUNE bmm(16x64x128, 16x128x512)
[1,1]<stdout>:strides: [192, 3072, 1], [65536, 1, 128]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  triton_bmm_7 0.0073 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,1]<stdout>:  triton_bmm_13 0.0076 ms 96.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_1 0.0077 ms 95.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,1]<stdout>:  triton_bmm_3 0.0078 ms 94.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_bmm_14 0.0078 ms 93.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,1]<stdout>:  bmm 0.0081 ms 90.9% 
[1,1]<stdout>:  triton_bmm_16 0.0081 ms 90.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_bmm_2 0.0082 ms 89.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_bmm_8 0.0084 ms 87.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_bmm_4 0.0087 ms 84.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.3739 seconds and 0.7245 seconds precompiling for 18 choices
[1,1]<stdout>:AUTOTUNE bmm(16x64x128, 16x128x512)
[1,1]<stdout>:strides: [192, 3072, 1], [65536, 1, 128]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  triton_bmm_7 0.0072 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,1]<stdout>:  triton_bmm_13 0.0074 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_11 0.0076 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_1 0.0076 ms 95.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,1]<stdout>:  triton_bmm_9 0.0077 ms 93.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_14 0.0078 ms 92.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,1]<stdout>:  triton_bmm_16 0.0080 ms 90.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_bmm_10 0.0081 ms 89.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,1]<stdout>:  triton_bmm_12 0.0082 ms 88.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,1]<stdout>:  triton_bmm_8 0.0083 ms 87.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.3425 seconds and 0.1776 seconds precompiling for 18 choices
[1,0]<stdout>:AUTOTUNE bmm(16x64x128, 16x128x512)
[1,0]<stdout>:strides: [192, 3072, 1], [65536, 1, 128]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  triton_bmm_7 0.0074 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,0]<stdout>:  triton_bmm_13 0.0075 ms 98.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_bmm_11 0.0077 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_bmm_14 0.0077 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,0]<stdout>:  triton_bmm_1 0.0078 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,0]<stdout>:  triton_bmm_16 0.0078 ms 94.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  bmm 0.0079 ms 93.9% 
[1,0]<stdout>:  triton_bmm_9 0.0079 ms 93.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_bmm_3 0.0079 ms 93.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_bmm_10 0.0080 ms 92.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.3688 seconds and 0.7571 seconds precompiling for 18 choices
[1,0]<stdout>:AUTOTUNE bmm(16x64x128, 16x128x512)
[1,0]<stdout>:strides: [192, 3072, 1], [65536, 1, 128]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  triton_bmm_7 0.0073 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,0]<stdout>:  triton_bmm_13 0.0074 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_bmm_11 0.0075 ms 98.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_bmm_14 0.0077 ms 94.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,0]<stdout>:  triton_bmm_3 0.0078 ms 93.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_bmm_9 0.0079 ms 93.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_bmm_10 0.0079 ms 92.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,0]<stdout>:  triton_bmm_16 0.0079 ms 92.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_bmm_12 0.0080 ms 92.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,0]<stdout>:  triton_bmm_2 0.0080 ms 91.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.3697 seconds and 0.7854 seconds precompiling for 18 choices
[1,0]<stdout>:AUTOTUNE bmm(16x64x128, 16x128x512)
[1,0]<stdout>:strides: [192, 3072, 1], [65536, 1, 128]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  triton_bmm_7 0.0075 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,0]<stdout>:  triton_bmm_13 0.0076 ms 97.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_bmm_1 0.0078 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,0]<stdout>:  triton_bmm_3 0.0078 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_bmm_11 0.0078 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  bmm 0.0079 ms 94.4% 
[1,0]<stdout>:  triton_bmm_16 0.0079 ms 94.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_bmm_9 0.0080 ms 93.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_bmm_14 0.0080 ms 93.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,0]<stdout>:  triton_bmm_12 0.0082 ms 91.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.3451 seconds and 0.5664 seconds precompiling for 18 choices
[1,0]<stdout>:AUTOTUNE bmm(16x64x128, 16x128x512)
[1,0]<stdout>:strides: [192, 3072, 1], [65536, 1, 128]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  triton_bmm_7 0.0072 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,0]<stdout>:  triton_bmm_13 0.0074 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_bmm_11 0.0075 ms 96.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  bmm 0.0076 ms 95.4% 
[1,0]<stdout>:  triton_bmm_1 0.0076 ms 95.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,0]<stdout>:  triton_bmm_3 0.0076 ms 95.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_bmm_9 0.0077 ms 93.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_bmm_14 0.0078 ms 93.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,0]<stdout>:  triton_bmm_2 0.0079 ms 91.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_bmm_10 0.0080 ms 90.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.3635 seconds and 0.6994 seconds precompiling for 18 choices
[1,0]<stdout>:AUTOTUNE bmm(16x64x128, 16x128x512)
[1,0]<stdout>:strides: [192, 3072, 1], [65536, 1, 128]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  triton_bmm_7 0.0076 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,0]<stdout>:  triton_bmm_11 0.0078 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_bmm_1 0.0079 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,0]<stdout>:  triton_bmm_13 0.0080 ms 94.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  bmm 0.0081 ms 93.7% 
[1,0]<stdout>:  triton_bmm_3 0.0081 ms 93.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_bmm_9 0.0081 ms 93.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_bmm_14 0.0081 ms 93.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,0]<stdout>:  triton_bmm_10 0.0082 ms 91.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,0]<stdout>:  triton_bmm_16 0.0083 ms 91.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.3696 seconds and 0.7705 seconds precompiling for 18 choices
[1,0]<stdout>:AUTOTUNE bmm(16x64x128, 16x128x512)
[1,0]<stdout>:strides: [192, 3072, 1], [65536, 1, 128]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  triton_bmm_7 0.0073 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,0]<stdout>:  triton_bmm_13 0.0074 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_bmm_11 0.0076 ms 96.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_bmm_1 0.0077 ms 95.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,0]<stdout>:  bmm 0.0077 ms 95.0% 
[1,0]<stdout>:  triton_bmm_14 0.0077 ms 94.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,0]<stdout>:  triton_bmm_3 0.0079 ms 92.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_bmm_9 0.0079 ms 92.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_bmm_2 0.0081 ms 90.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_bmm_10 0.0082 ms 89.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.3088 seconds and 1.0700 seconds precompiling for 18 choices
[1,0]<stdout>:AUTOTUNE bmm(16x64x128, 16x128x512)
[1,0]<stdout>:strides: [192, 3072, 1], [65536, 1, 128]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  triton_bmm_1 0.0074 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,0]<stdout>:  triton_bmm_7 0.0074 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,0]<stdout>:  triton_bmm_13 0.0075 ms 98.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_bmm_11 0.0076 ms 96.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_bmm_3 0.0076 ms 96.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_bmm_14 0.0077 ms 95.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,0]<stdout>:  triton_bmm_9 0.0078 ms 94.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_bmm_16 0.0078 ms 94.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_bmm_2 0.0080 ms 92.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_bmm_10 0.0080 ms 91.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.3925 seconds and 1.0479 seconds precompiling for 18 choices
[1,0]<stdout>:AUTOTUNE bmm(16x64x128, 16x128x512)
[1,0]<stdout>:strides: [192, 3072, 1], [65536, 1, 128]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  triton_bmm_7 0.0073 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,0]<stdout>:  triton_bmm_13 0.0073 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_bmm_11 0.0076 ms 96.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  bmm 0.0077 ms 95.0% 
[1,0]<stdout>:  triton_bmm_1 0.0077 ms 95.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,0]<stdout>:  triton_bmm_3 0.0077 ms 95.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_bmm_14 0.0077 ms 95.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,0]<stdout>:  triton_bmm_9 0.0078 ms 94.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_bmm_16 0.0079 ms 93.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_bmm_10 0.0079 ms 92.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.3796 seconds and 0.0422 seconds precompiling for 18 choices
[1,1]<stdout>:AUTOTUNE bmm(16x64x128, 16x128x512)
[1,1]<stdout>:strides: [192, 3072, 1], [65536, 1, 128]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  triton_bmm_7 0.0073 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,1]<stdout>:  triton_bmm_3 0.0075 ms 97.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_bmm_13 0.0076 ms 96.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_11 0.0076 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_1 0.0077 ms 95.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,1]<stdout>:  triton_bmm_14 0.0077 ms 95.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,1]<stdout>:  bmm 0.0077 ms 94.2% 
[1,1]<stdout>:  triton_bmm_10 0.0079 ms 92.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,1]<stdout>:  triton_bmm_2 0.0080 ms 91.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_bmm_9 0.0080 ms 91.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.3255 seconds and 1.2325 seconds precompiling for 18 choices
[1,1]<stdout>:AUTOTUNE bmm(16x64x128, 16x128x512)
[1,1]<stdout>:strides: [192, 3072, 1], [65536, 1, 128]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  triton_bmm_7 0.0075 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,1]<stdout>:  triton_bmm_13 0.0076 ms 99.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_11 0.0077 ms 97.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_1 0.0078 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,1]<stdout>:  triton_bmm_14 0.0078 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,1]<stdout>:  triton_bmm_3 0.0079 ms 95.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  bmm 0.0079 ms 94.8% 
[1,1]<stdout>:  triton_bmm_9 0.0079 ms 94.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_16 0.0079 ms 94.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_bmm_2 0.0080 ms 93.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.2956 seconds and 1.1154 seconds precompiling for 18 choices
[1,1]<stdout>:AUTOTUNE bmm(16x64x128, 16x128x512)
[1,1]<stdout>:strides: [192, 3072, 1], [65536, 1, 128]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  triton_bmm_7 0.0076 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,1]<stdout>:  triton_bmm_13 0.0077 ms 97.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_14 0.0078 ms 97.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,1]<stdout>:  triton_bmm_1 0.0079 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,1]<stdout>:  triton_bmm_11 0.0079 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_3 0.0080 ms 94.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_bmm_9 0.0081 ms 93.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  bmm 0.0081 ms 93.3% 
[1,1]<stdout>:  triton_bmm_2 0.0082 ms 92.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_bmm_16 0.0082 ms 92.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.2966 seconds and 1.1621 seconds precompiling for 18 choices
[1,1]<stdout>:AUTOTUNE bmm(16x64x128, 16x128x512)
[1,1]<stdout>:strides: [192, 3072, 1], [65536, 1, 128]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  triton_bmm_7 0.0073 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,1]<stdout>:  triton_bmm_13 0.0075 ms 97.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_1 0.0076 ms 96.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,1]<stdout>:  triton_bmm_11 0.0076 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_3 0.0077 ms 95.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  bmm 0.0077 ms 95.0% 
[1,1]<stdout>:  triton_bmm_16 0.0078 ms 94.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_bmm_9 0.0078 ms 93.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_14 0.0079 ms 92.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,1]<stdout>:  triton_bmm_2 0.0081 ms 90.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.2973 seconds and 1.0852 seconds precompiling for 18 choices
[1,1]<stdout>:AUTOTUNE bmm(16x64x128, 16x128x512)
[1,1]<stdout>:strides: [192, 3072, 1], [65536, 1, 128]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  triton_bmm_7 0.0074 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,1]<stdout>:  triton_bmm_13 0.0076 ms 97.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_11 0.0077 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_1 0.0078 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,1]<stdout>:  triton_bmm_3 0.0078 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  bmm 0.0079 ms 93.9% 
[1,1]<stdout>:  triton_bmm_16 0.0080 ms 92.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_bmm_9 0.0080 ms 92.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_10 0.0081 ms 91.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,1]<stdout>:  triton_bmm_14 0.0081 ms 91.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.3208 seconds and 1.2153 seconds precompiling for 18 choices
[1,1]<stdout>:AUTOTUNE bmm(16x64x128, 16x128x512)
[1,1]<stdout>:strides: [192, 3072, 1], [65536, 1, 128]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  triton_bmm_7 0.0073 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,1]<stdout>:  triton_bmm_13 0.0075 ms 97.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_11 0.0076 ms 96.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_14 0.0076 ms 96.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,1]<stdout>:  triton_bmm_1 0.0077 ms 95.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,1]<stdout>:  triton_bmm_3 0.0078 ms 93.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  bmm 0.0079 ms 93.1% 
[1,1]<stdout>:  triton_bmm_9 0.0079 ms 92.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_10 0.0080 ms 91.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,1]<stdout>:  triton_bmm_2 0.0081 ms 90.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.3659 seconds and 1.1730 seconds precompiling for 18 choices
[1,1]<stdout>:/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py:1575: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
[1,1]<stdout>:  torch._dynamo.utils.warn_once(msg)
[1,1]<stdout>:/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py:1575: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
[1,1]<stdout>:  torch._dynamo.utils.warn_once(msg)
[1,1]<stdout>:/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py:1575: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
[1,1]<stdout>:  torch._dynamo.utils.warn_once(msg)
[1,1]<stdout>:/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py:1575: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
[1,1]<stdout>:  torch._dynamo.utils.warn_once(msg)
[1,1]<stdout>:/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py:1575: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
[1,1]<stdout>:  torch._dynamo.utils.warn_once(msg)
[1,0]<stdout>:/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py:1575: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
[1,0]<stdout>:  torch._dynamo.utils.warn_once(msg)
[1,0]<stdout>:/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py:1575: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
[1,0]<stdout>:  torch._dynamo.utils.warn_once(msg)
[1,1]<stdout>:/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py:1575: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
[1,1]<stdout>:  torch._dynamo.utils.warn_once(msg)
[1,1]<stdout>:/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py:1575: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
[1,1]<stdout>:  torch._dynamo.utils.warn_once(msg)
[1,0]<stdout>:/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py:1575: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
[1,0]<stdout>:  torch._dynamo.utils.warn_once(msg)
[1,0]<stdout>:/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py:1575: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
[1,0]<stdout>:  torch._dynamo.utils.warn_once(msg)
[1,0]<stdout>:/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py:1575: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
[1,0]<stdout>:  torch._dynamo.utils.warn_once(msg)
[1,0]<stdout>:/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py:1575: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
[1,0]<stdout>:  torch._dynamo.utils.warn_once(msg)
[1,0]<stdout>:/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py:1575: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
[1,0]<stdout>:  torch._dynamo.utils.warn_once(msg)
[1,0]<stdout>:/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py:1575: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
[1,0]<stdout>:  torch._dynamo.utils.warn_once(msg)
[1,1]<stdout>:/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py:1575: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
[1,1]<stdout>:  torch._dynamo.utils.warn_once(msg)
[1,1]<stdout>:AUTOTUNE mm(128x7168, 7168x256)
[1,1]<stdout>:strides: [7168, 1], [1, 7168]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  mm 0.0133 ms 100.0% 
[1,1]<stdout>:  triton_mm_21 0.0191 ms 70.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_25 0.0224 ms 59.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_29 0.0262 ms 50.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,1]<stdout>:  triton_mm_35 0.0411 ms 32.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_20 0.0447 ms 29.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_19 0.0462 ms 28.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_24 0.0498 ms 26.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,1]<stdout>:  triton_mm_28 0.0500 ms 26.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_mm_18 0.0532 ms 25.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.4480 seconds and 1.7215 seconds precompiling for 20 choices
[1,1]<stdout>:AUTOTUNE mm(128x7168, 7168x256)
[1,1]<stdout>:strides: [7168, 1], [1, 7168]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  mm 0.0137 ms 100.0% 
[1,1]<stdout>:  triton_mm_21 0.0191 ms 71.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_25 0.0224 ms 61.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_29 0.0263 ms 52.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,1]<stdout>:  triton_mm_35 0.0411 ms 33.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_20 0.0445 ms 30.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_19 0.0463 ms 29.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_24 0.0495 ms 27.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,1]<stdout>:  triton_mm_28 0.0498 ms 27.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_mm_18 0.0531 ms 25.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.5134 seconds and 1.9229 seconds precompiling for 20 choices
[1,1]<stdout>:AUTOTUNE mm(128x7168, 7168x256)
[1,1]<stdout>:strides: [7168, 1], [1, 7168]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  mm 0.0134 ms 100.0% 
[1,1]<stdout>:  triton_mm_21 0.0191 ms 70.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_25 0.0224 ms 59.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_29 0.0263 ms 50.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,1]<stdout>:  triton_mm_35 0.0412 ms 32.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_20 0.0451 ms 29.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_19 0.0468 ms 28.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_28 0.0499 ms 26.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_mm_24 0.0500 ms 26.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,1]<stdout>:  triton_mm_18 0.0536 ms 25.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.5114 seconds and 1.9039 seconds precompiling for 20 choices
[1,1]<stdout>:AUTOTUNE mm(128x7168, 7168x256)
[1,1]<stdout>:strides: [7168, 1], [1, 7168]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  mm 0.0140 ms 100.0% 
[1,1]<stdout>:  triton_mm_21 0.0193 ms 72.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_25 0.0228 ms 61.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_29 0.0266 ms 52.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,1]<stdout>:  triton_mm_35 0.0410 ms 34.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_20 0.0451 ms 31.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_19 0.0465 ms 30.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_24 0.0497 ms 28.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,1]<stdout>:  triton_mm_28 0.0501 ms 27.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_mm_18 0.0531 ms 26.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.5114 seconds and 1.7176 seconds precompiling for 20 choices
[1,1]<stdout>:AUTOTUNE mm(128x7168, 7168x256)
[1,1]<stdout>:strides: [7168, 1], [1, 7168]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  mm 0.0132 ms 100.0% 
[1,1]<stdout>:  triton_mm_21 0.0193 ms 68.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_25 0.0226 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_29 0.0264 ms 50.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,1]<stdout>:  triton_mm_35 0.0410 ms 32.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_20 0.0449 ms 29.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_19 0.0466 ms 28.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_24 0.0496 ms 26.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,1]<stdout>:  triton_mm_28 0.0498 ms 26.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_mm_18 0.0532 ms 24.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.5160 seconds and 1.9166 seconds precompiling for 20 choices
[1,1]<stdout>:AUTOTUNE mm(128x7168, 7168x256)
[1,1]<stdout>:strides: [7168, 1], [1, 7168]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  mm 0.0137 ms 100.0% 
[1,1]<stdout>:  triton_mm_21 0.0192 ms 71.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_25 0.0223 ms 61.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_29 0.0264 ms 52.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,1]<stdout>:  triton_mm_35 0.0412 ms 33.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_20 0.0453 ms 30.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_19 0.0465 ms 29.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_28 0.0497 ms 27.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_mm_24 0.0499 ms 27.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,1]<stdout>:  triton_mm_18 0.0535 ms 25.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.5171 seconds and 1.9338 seconds precompiling for 20 choices
[1,1]<stdout>:AUTOTUNE mm(128x7168, 7168x256)
[1,1]<stdout>:strides: [7168, 1], [1, 7168]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  mm 0.0132 ms 100.0% 
[1,1]<stdout>:  triton_mm_21 0.0190 ms 69.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_25 0.0224 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_29 0.0263 ms 50.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,1]<stdout>:  triton_mm_35 0.0411 ms 32.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_20 0.0449 ms 29.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_19 0.0463 ms 28.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_24 0.0496 ms 26.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,1]<stdout>:  triton_mm_28 0.0500 ms 26.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_mm_18 0.0532 ms 24.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.5176 seconds and 1.9289 seconds precompiling for 20 choices
[1,1]<stdout>:AUTOTUNE mm(128x7168, 7168x256)
[1,1]<stdout>:strides: [7168, 1], [1, 7168]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  mm 0.0138 ms 100.0% 
[1,1]<stdout>:  triton_mm_21 0.0191 ms 71.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_25 0.0224 ms 61.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_29 0.0262 ms 52.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,1]<stdout>:  triton_mm_35 0.0410 ms 33.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_20 0.0451 ms 30.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_19 0.0465 ms 29.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_28 0.0492 ms 28.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_mm_24 0.0500 ms 27.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,1]<stdout>:  triton_mm_18 0.0534 ms 25.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.5168 seconds and 1.9221 seconds precompiling for 20 choices
[1,0]<stdout>:AUTOTUNE mm(128x7168, 7168x256)
[1,0]<stdout>:strides: [7168, 1], [1, 7168]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  mm 0.0136 ms 100.0% 
[1,0]<stdout>:  triton_mm_21 0.0197 ms 69.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_25 0.0229 ms 59.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_29 0.0267 ms 51.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,0]<stdout>:  triton_mm_35 0.0411 ms 33.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_20 0.0447 ms 30.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_19 0.0472 ms 28.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_24 0.0495 ms 27.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,0]<stdout>:  triton_mm_28 0.0499 ms 27.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_mm_18 0.0530 ms 25.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.4325 seconds and 2.8736 seconds precompiling for 20 choices
[1,0]<stdout>:AUTOTUNE mm(128x7168, 7168x256)
[1,0]<stdout>:strides: [7168, 1], [1, 7168]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  mm 0.0133 ms 100.0% 
[1,0]<stdout>:  triton_mm_21 0.0191 ms 69.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_25 0.0225 ms 59.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_29 0.0264 ms 50.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,0]<stdout>:  triton_mm_35 0.0410 ms 32.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_20 0.0442 ms 30.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_19 0.0461 ms 29.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_24 0.0490 ms 27.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,0]<stdout>:  triton_mm_28 0.0493 ms 27.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_mm_18 0.0526 ms 25.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.4124 seconds and 3.0368 seconds precompiling for 20 choices
[1,0]<stdout>:AUTOTUNE mm(128x7168, 7168x256)
[1,0]<stdout>:strides: [7168, 1], [1, 7168]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  mm 0.0136 ms 100.0% 
[1,0]<stdout>:  triton_mm_21 0.0196 ms 69.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_25 0.0229 ms 59.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_29 0.0268 ms 51.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,0]<stdout>:  triton_mm_35 0.0412 ms 33.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_20 0.0446 ms 30.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_19 0.0468 ms 29.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_24 0.0495 ms 27.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,0]<stdout>:  triton_mm_28 0.0498 ms 27.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_mm_18 0.0528 ms 25.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.4112 seconds and 3.0414 seconds precompiling for 20 choices
[1,0]<stdout>:AUTOTUNE mm(128x7168, 7168x256)
[1,0]<stdout>:strides: [7168, 1], [1, 7168]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  mm 0.0134 ms 100.0% 
[1,0]<stdout>:  triton_mm_21 0.0195 ms 68.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_25 0.0228 ms 58.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_29 0.0264 ms 50.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,0]<stdout>:  triton_mm_35 0.0410 ms 32.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_20 0.0446 ms 30.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_19 0.0465 ms 28.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_28 0.0489 ms 27.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_mm_24 0.0493 ms 27.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,0]<stdout>:  triton_mm_18 0.0526 ms 25.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.4129 seconds and 3.0063 seconds precompiling for 20 choices
[1,0]<stdout>:AUTOTUNE mm(128x7168, 7168x256)
[1,0]<stdout>:strides: [7168, 1], [1, 7168]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  triton_mm_21 0.0192 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_25 0.0226 ms 85.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_29 0.0264 ms 72.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,0]<stdout>:  triton_mm_35 0.0407 ms 47.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_20 0.0443 ms 43.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_19 0.0462 ms 41.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_24 0.0492 ms 39.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,0]<stdout>:  triton_mm_28 0.0496 ms 38.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_mm_18 0.0529 ms 36.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,0]<stdout>:  triton_mm_34 0.0546 ms 35.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.4253 seconds and 3.0214 seconds precompiling for 20 choices
[1,0]<stdout>:AUTOTUNE mm(128x7168, 7168x256)
[1,0]<stdout>:strides: [7168, 1], [1, 7168]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  triton_mm_21 0.0194 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_25 0.0227 ms 85.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_29 0.0266 ms 72.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,0]<stdout>:  triton_mm_35 0.0413 ms 47.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_20 0.0446 ms 43.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_19 0.0468 ms 41.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_24 0.0496 ms 39.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,0]<stdout>:  triton_mm_28 0.0496 ms 39.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_mm_18 0.0532 ms 36.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,0]<stdout>:  triton_mm_34 0.0551 ms 35.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.4260 seconds and 2.7248 seconds precompiling for 20 choices
[1,0]<stdout>:AUTOTUNE mm(128x7168, 7168x256)
[1,0]<stdout>:strides: [7168, 1], [1, 7168]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  mm 0.0136 ms 100.0% 
[1,0]<stdout>:  triton_mm_21 0.0196 ms 69.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_25 0.0227 ms 59.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_29 0.0266 ms 51.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,0]<stdout>:  triton_mm_35 0.0410 ms 33.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_20 0.0444 ms 30.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_19 0.0471 ms 28.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_24 0.0488 ms 27.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,0]<stdout>:  triton_mm_28 0.0489 ms 27.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_mm_18 0.0532 ms 25.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.4265 seconds and 3.0088 seconds precompiling for 20 choices
[1,1]<stdout>:AUTOTUNE mm(128x7168, 7168x8080)
[1,1]<stdout>:strides: [7168, 1], [1, 7168]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  mm 0.0538 ms 100.0% 
[1,1]<stdout>:  triton_mm_48 0.0542 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,1]<stdout>:  triton_mm_54 0.0589 ms 91.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_44 0.0598 ms 90.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_43 0.0741 ms 72.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,1]<stdout>:  triton_mm_47 0.0744 ms 72.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_mm_53 0.0777 ms 69.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_mm_40 0.0786 ms 68.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_50 0.0909 ms 59.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,1]<stdout>:  triton_mm_46 0.0919 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.6293 seconds and 0.9123 seconds precompiling for 20 choices
[1,1]<stdout>:AUTOTUNE mm(128x7168, 7168x8080)
[1,1]<stdout>:strides: [7168, 1], [1, 7168]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  mm 0.0539 ms 100.0% 
[1,1]<stdout>:  triton_mm_48 0.0543 ms 99.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,1]<stdout>:  triton_mm_54 0.0596 ms 90.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_44 0.0600 ms 89.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_43 0.0744 ms 72.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,1]<stdout>:  triton_mm_47 0.0747 ms 72.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_mm_53 0.0779 ms 69.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_mm_40 0.0791 ms 68.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_50 0.0915 ms 59.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,1]<stdout>:  triton_mm_46 0.0922 ms 58.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.6697 seconds and 0.9409 seconds precompiling for 20 choices
[1,1]<stdout>:AUTOTUNE mm(128x7168, 7168x8080)
[1,1]<stdout>:strides: [7168, 1], [1, 7168]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  triton_mm_48 0.0543 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,1]<stdout>:  triton_mm_54 0.0592 ms 91.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_44 0.0599 ms 90.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  mm 0.0607 ms 89.4% 
[1,1]<stdout>:  triton_mm_43 0.0737 ms 73.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,1]<stdout>:  triton_mm_47 0.0740 ms 73.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_mm_53 0.0771 ms 70.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_mm_40 0.0791 ms 68.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_50 0.0910 ms 59.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,1]<stdout>:  triton_mm_46 0.0915 ms 59.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.6680 seconds and 0.9222 seconds precompiling for 20 choices
[1,1]<stdout>:AUTOTUNE mm(128x7168, 7168x8080)
[1,1]<stdout>:strides: [7168, 1], [1, 7168]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  triton_mm_48 0.0536 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,1]<stdout>:  triton_mm_54 0.0589 ms 91.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_44 0.0595 ms 90.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_43 0.0738 ms 72.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,1]<stdout>:  triton_mm_47 0.0741 ms 72.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_mm_53 0.0773 ms 69.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_mm_40 0.0788 ms 68.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_50 0.0905 ms 59.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,1]<stdout>:  triton_mm_46 0.0911 ms 58.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,1]<stdout>:  mm 0.1017 ms 52.7% 
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.6693 seconds and 0.9215 seconds precompiling for 20 choices
[1,1]<stdout>:AUTOTUNE mm(128x7168, 7168x8080)
[1,1]<stdout>:strides: [7168, 1], [1, 7168]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  triton_mm_48 0.0538 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,1]<stdout>:  triton_mm_54 0.0589 ms 91.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_44 0.0597 ms 90.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_43 0.0741 ms 72.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,1]<stdout>:  triton_mm_47 0.0744 ms 72.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_mm_53 0.0778 ms 69.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_mm_40 0.0790 ms 68.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  mm 0.0841 ms 64.0% 
[1,1]<stdout>:  triton_mm_50 0.0915 ms 58.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,1]<stdout>:  triton_mm_46 0.0916 ms 58.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.6706 seconds and 0.9425 seconds precompiling for 20 choices
[1,1]<stdout>:AUTOTUNE mm(128x7168, 7168x8080)
[1,1]<stdout>:strides: [7168, 1], [1, 7168]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  triton_mm_48 0.0542 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,1]<stdout>:  mm 0.0548 ms 98.8% 
[1,1]<stdout>:  triton_mm_54 0.0591 ms 91.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_44 0.0596 ms 90.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_43 0.0740 ms 73.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,1]<stdout>:  triton_mm_47 0.0742 ms 73.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_mm_53 0.0772 ms 70.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_mm_40 0.0791 ms 68.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_50 0.0909 ms 59.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,1]<stdout>:  triton_mm_46 0.0916 ms 59.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.6685 seconds and 0.8028 seconds precompiling for 20 choices
[1,1]<stdout>:AUTOTUNE mm(128x7168, 7168x8080)
[1,1]<stdout>:strides: [7168, 1], [1, 7168]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  triton_mm_48 0.0537 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,1]<stdout>:  mm 0.0538 ms 99.9% 
[1,1]<stdout>:  triton_mm_54 0.0588 ms 91.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_44 0.0597 ms 89.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_43 0.0739 ms 72.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,1]<stdout>:  triton_mm_47 0.0739 ms 72.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_mm_53 0.0771 ms 69.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_mm_40 0.0788 ms 68.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_46 0.0907 ms 59.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,1]<stdout>:  triton_mm_50 0.0908 ms 59.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.6701 seconds and 0.9384 seconds precompiling for 20 choices
[1,0]<stdout>:AUTOTUNE mm(128x7168, 7168x8080)
[1,0]<stdout>:strides: [7168, 1], [1, 7168]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  mm 0.0540 ms 100.0% 
[1,0]<stdout>:  triton_mm_48 0.0542 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,0]<stdout>:  triton_mm_54 0.0595 ms 90.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_44 0.0600 ms 90.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_43 0.0743 ms 72.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,0]<stdout>:  triton_mm_47 0.0749 ms 72.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_mm_53 0.0780 ms 69.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_mm_40 0.0790 ms 68.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_50 0.0914 ms 59.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,0]<stdout>:  triton_mm_46 0.0920 ms 58.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.5589 seconds and 1.0037 seconds precompiling for 20 choices
[1,0]<stdout>:AUTOTUNE mm(128x7168, 7168x8080)
[1,0]<stdout>:strides: [7168, 1], [1, 7168]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  mm 0.0539 ms 100.0% 
[1,0]<stdout>:  triton_mm_48 0.0542 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,0]<stdout>:  triton_mm_54 0.0593 ms 90.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_44 0.0601 ms 89.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_43 0.0739 ms 72.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,0]<stdout>:  triton_mm_47 0.0740 ms 72.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_mm_53 0.0772 ms 69.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_mm_40 0.0795 ms 67.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_50 0.0909 ms 59.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,0]<stdout>:  triton_mm_46 0.0921 ms 58.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.4942 seconds and 1.2369 seconds precompiling for 20 choices
[1,0]<stdout>:AUTOTUNE mm(128x7168, 7168x8080)
[1,0]<stdout>:strides: [7168, 1], [1, 7168]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  triton_mm_48 0.0538 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,0]<stdout>:  mm 0.0538 ms 99.9% 
[1,0]<stdout>:  triton_mm_54 0.0588 ms 91.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_44 0.0594 ms 90.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_43 0.0735 ms 73.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,0]<stdout>:  triton_mm_47 0.0738 ms 72.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_mm_53 0.0773 ms 69.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_mm_40 0.0788 ms 68.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_50 0.0904 ms 59.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,0]<stdout>:  triton_mm_46 0.0912 ms 59.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.4826 seconds and 1.2716 seconds precompiling for 20 choices
[1,0]<stdout>:AUTOTUNE mm(128x7168, 7168x8080)
[1,0]<stdout>:strides: [7168, 1], [1, 7168]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  mm 0.0537 ms 100.0% 
[1,0]<stdout>:  triton_mm_48 0.0540 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,0]<stdout>:  triton_mm_54 0.0590 ms 91.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_44 0.0600 ms 89.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_47 0.0744 ms 72.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_mm_43 0.0745 ms 72.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,0]<stdout>:  triton_mm_53 0.0775 ms 69.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_mm_40 0.0796 ms 67.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_50 0.0907 ms 59.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,0]<stdout>:  triton_mm_46 0.0915 ms 58.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.4903 seconds and 1.2896 seconds precompiling for 20 choices
[1,0]<stdout>:AUTOTUNE mm(128x7168, 7168x8080)
[1,0]<stdout>:strides: [7168, 1], [1, 7168]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  mm 0.0534 ms 100.0% 
[1,0]<stdout>:  triton_mm_48 0.0535 ms 99.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,0]<stdout>:  triton_mm_54 0.0591 ms 90.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_44 0.0597 ms 89.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_47 0.0736 ms 72.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_mm_43 0.0740 ms 72.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,0]<stdout>:  triton_mm_53 0.0773 ms 69.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_mm_40 0.0790 ms 67.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_50 0.0909 ms 58.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,0]<stdout>:  triton_mm_46 0.0915 ms 58.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.4858 seconds and 1.1511 seconds precompiling for 20 choices
[1,0]<stdout>:AUTOTUNE mm(128x7168, 7168x8080)
[1,0]<stdout>:strides: [7168, 1], [1, 7168]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  triton_mm_48 0.0539 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,0]<stdout>:  triton_mm_54 0.0591 ms 91.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_44 0.0596 ms 90.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_47 0.0739 ms 73.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_mm_43 0.0741 ms 72.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,0]<stdout>:  triton_mm_53 0.0773 ms 69.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_mm_40 0.0790 ms 68.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_50 0.0903 ms 59.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,0]<stdout>:  triton_mm_46 0.0912 ms 59.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,0]<stdout>:  mm 0.0918 ms 58.7% 
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.5193 seconds and 1.1431 seconds precompiling for 20 choices
[1,0]<stdout>:AUTOTUNE mm(128x7168, 7168x8080)
[1,0]<stdout>:strides: [7168, 1], [1, 7168]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  triton_mm_48 0.0538 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,0]<stdout>:  mm 0.0540 ms 99.8% 
[1,0]<stdout>:  triton_mm_54 0.0594 ms 90.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_44 0.0601 ms 89.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_43 0.0740 ms 72.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,0]<stdout>:  triton_mm_47 0.0744 ms 72.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_mm_53 0.0775 ms 69.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_mm_40 0.0795 ms 67.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_50 0.0910 ms 59.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,0]<stdout>:  triton_mm_46 0.0919 ms 58.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 2.2002 seconds and 0.0084 seconds precompiling for 20 choices
[1,1]<stdout>:AUTOTUNE mm(128x7168, 7168x8080)
[1,1]<stdout>:strides: [7168, 1], [1, 7168]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  mm 0.0538 ms 100.0% 
[1,1]<stdout>:  triton_mm_48 0.0538 ms 99.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,1]<stdout>:  triton_mm_54 0.0594 ms 90.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_44 0.0599 ms 89.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_47 0.0744 ms 72.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_mm_43 0.0745 ms 72.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,1]<stdout>:  triton_mm_53 0.0776 ms 69.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_mm_40 0.0793 ms 67.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_50 0.0907 ms 59.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,1]<stdout>:  triton_mm_46 0.0918 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 2.3926 seconds and 0.0050 seconds precompiling for 20 choices
[1,0]<stdout>:Capturing batches (bs=32 avail_mem=20.12 GB):  62% 5/8 [01:05<01:00, 20.23s/it][1,0]<stdout>:Capturing batches (bs=24 avail_mem=20.06 GB):  62% 5/8 [01:05<01:00, 20.23s/it][1,1]<stdout>:[rank11]:W1011 22:06:50.983000 3125268 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/_inductor/codecache.py:1026] [14/5] fx graph cache unable to load compiled graph
[1,1]<stdout>:[rank11]:W1011 22:06:50.983000 3125268 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/_inductor/codecache.py:1026] [14/5] Traceback (most recent call last):
[1,1]<stdout>:[rank11]:W1011 22:06:50.983000 3125268 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/_inductor/codecache.py:1026] [14/5]   File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_inductor/codecache.py", line 1022, in iterate_over_candidates
[1,1]<stdout>:[rank11]:W1011 22:06:50.983000 3125268 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/_inductor/codecache.py:1026] [14/5]     with open(os.path.join(subdir, path), "rb") as f:
[1,1]<stdout>:[rank11]:W1011 22:06:50.983000 3125268 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/_inductor/codecache.py:1026] [14/5]          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:[rank11]:W1011 22:06:50.983000 3125268 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/_inductor/codecache.py:1026] [14/5] FileNotFoundError: [Errno 2] No such file or directory: '/home/users/industry/ai-hpc/apacsc34/scratch/.torchinductor_cache/fxgraph/h7/fh7z6lqq5ommoooshe27623qfsnj4r74cj4jbjjm67oa4ga6reba/.3698710.140737350502208.tmp'
[1,1]<stdout>:AUTOTUNE bmm(16x48x128, 16x128x512)
[1,1]<stdout>:strides: [192, 3072, 1], [65536, 1, 128]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  triton_bmm_62 0.0073 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,1]<stdout>:  triton_bmm_68 0.0074 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_66 0.0075 ms 97.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_56 0.0076 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,1]<stdout>:  triton_bmm_58 0.0076 ms 96.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_bmm_64 0.0077 ms 95.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_69 0.0078 ms 93.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,1]<stdout>:  bmm 0.0079 ms 92.3% 
[1,1]<stdout>:  triton_bmm_71 0.0079 ms 92.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_bmm_67 0.0080 ms 91.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.4480 seconds and 0.6258 seconds precompiling for 18 choices
[1,1]<stdout>:AUTOTUNE bmm(16x48x128, 16x128x512)
[1,1]<stdout>:strides: [192, 3072, 1], [65536, 1, 128]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  triton_bmm_62 0.0075 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,1]<stdout>:  triton_bmm_68 0.0076 ms 97.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_66 0.0077 ms 97.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_56 0.0078 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,1]<stdout>:  triton_bmm_58 0.0079 ms 95.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_bmm_64 0.0079 ms 95.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_65 0.0080 ms 93.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,1]<stdout>:  triton_bmm_67 0.0081 ms 92.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,1]<stdout>:  triton_bmm_57 0.0081 ms 92.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  bmm 0.0082 ms 91.8% 
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.4541 seconds and 0.6204 seconds precompiling for 18 choices
[1,1]<stdout>:AUTOTUNE bmm(16x48x128, 16x128x512)
[1,1]<stdout>:strides: [192, 3072, 1], [65536, 1, 128]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  triton_bmm_62 0.0075 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,1]<stdout>:  triton_bmm_68 0.0076 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_66 0.0076 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_69 0.0078 ms 95.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,1]<stdout>:  triton_bmm_58 0.0079 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_bmm_56 0.0079 ms 94.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,1]<stdout>:  triton_bmm_64 0.0079 ms 94.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_57 0.0079 ms 94.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_bmm_65 0.0079 ms 94.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,1]<stdout>:  triton_bmm_71 0.0079 ms 94.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.4079 seconds and 0.5526 seconds precompiling for 18 choices
[1,1]<stdout>:AUTOTUNE bmm(16x48x128, 16x128x512)
[1,1]<stdout>:strides: [192, 3072, 1], [65536, 1, 128]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  triton_bmm_62 0.0073 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,1]<stdout>:  triton_bmm_68 0.0073 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_66 0.0076 ms 96.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_56 0.0077 ms 95.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,1]<stdout>:  triton_bmm_58 0.0077 ms 94.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_bmm_64 0.0077 ms 94.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  bmm 0.0078 ms 93.9% 
[1,1]<stdout>:  triton_bmm_69 0.0078 ms 93.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,1]<stdout>:  triton_bmm_71 0.0080 ms 92.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_bmm_65 0.0080 ms 91.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.4097 seconds and 0.6220 seconds precompiling for 18 choices
[1,1]<stdout>:AUTOTUNE bmm(16x48x128, 16x128x512)
[1,1]<stdout>:strides: [192, 3072, 1], [65536, 1, 128]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  triton_bmm_62 0.0073 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,1]<stdout>:  triton_bmm_66 0.0075 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_68 0.0075 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_56 0.0077 ms 94.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,1]<stdout>:  triton_bmm_58 0.0077 ms 94.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_bmm_64 0.0077 ms 94.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_69 0.0077 ms 94.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,1]<stdout>:  triton_bmm_71 0.0078 ms 93.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_bmm_65 0.0078 ms 93.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,1]<stdout>:  triton_bmm_57 0.0079 ms 91.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.4107 seconds and 0.0027 seconds precompiling for 18 choices
[1,1]<stdout>:AUTOTUNE bmm(16x48x128, 16x128x512)
[1,1]<stdout>:strides: [192, 3072, 1], [65536, 1, 128]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  triton_bmm_62 0.0074 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,1]<stdout>:  triton_bmm_66 0.0075 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_58 0.0076 ms 97.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_bmm_56 0.0076 ms 96.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,1]<stdout>:  triton_bmm_64 0.0077 ms 95.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_57 0.0078 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_bmm_65 0.0078 ms 94.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,1]<stdout>:  bmm 0.0079 ms 93.5% 
[1,1]<stdout>:  triton_bmm_68 0.0081 ms 91.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_63 0.0082 ms 90.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.4738 seconds and 0.6455 seconds precompiling for 18 choices
[1,1]<stdout>:AUTOTUNE bmm(16x48x128, 16x128x512)
[1,1]<stdout>:strides: [192, 3072, 1], [65536, 1, 128]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  triton_bmm_69 0.0073 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,1]<stdout>:  triton_bmm_71 0.0075 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_bmm_62 0.0076 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,1]<stdout>:  triton_bmm_57 0.0076 ms 96.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_bmm_65 0.0076 ms 96.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,1]<stdout>:  triton_bmm_67 0.0076 ms 96.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,1]<stdout>:  triton_bmm_68 0.0077 ms 95.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_66 0.0078 ms 94.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_63 0.0078 ms 93.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_bmm_56 0.0079 ms 92.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.4307 seconds and 0.4348 seconds precompiling for 18 choices
[1,0]<stdout>:AUTOTUNE bmm(16x48x128, 16x128x512)
[1,0]<stdout>:strides: [192, 3072, 1], [65536, 1, 128]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  triton_bmm_69 0.0076 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,0]<stdout>:  triton_bmm_65 0.0078 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,0]<stdout>:  triton_bmm_67 0.0078 ms 97.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,0]<stdout>:  triton_bmm_71 0.0078 ms 97.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_bmm_62 0.0078 ms 96.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,0]<stdout>:  triton_bmm_57 0.0079 ms 95.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_bmm_66 0.0080 ms 94.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_bmm_68 0.0080 ms 94.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  bmm 0.0081 ms 93.3% 
[1,0]<stdout>:  triton_bmm_56 0.0081 ms 93.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.4065 seconds and 0.9786 seconds precompiling for 18 choices
[1,0]<stdout>:AUTOTUNE bmm(16x48x128, 16x128x512)
[1,0]<stdout>:strides: [192, 3072, 1], [65536, 1, 128]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  triton_bmm_69 0.0077 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,0]<stdout>:  triton_bmm_66 0.0079 ms 98.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_bmm_65 0.0079 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,0]<stdout>:  triton_bmm_62 0.0080 ms 96.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,0]<stdout>:  triton_bmm_71 0.0080 ms 96.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_bmm_67 0.0080 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,0]<stdout>:  triton_bmm_56 0.0081 ms 96.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,0]<stdout>:  triton_bmm_57 0.0081 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_bmm_68 0.0083 ms 93.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_bmm_63 0.0083 ms 93.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.3977 seconds and 1.0047 seconds precompiling for 18 choices
[1,1]<stdout>:AUTOTUNE bmm(16x48x128, 16x128x512)
[1,1]<stdout>:strides: [192, 3072, 1], [65536, 1, 128]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  triton_bmm_62 0.0074 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,1]<stdout>:  triton_bmm_68 0.0076 ms 98.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_66 0.0076 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_56 0.0078 ms 95.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,1]<stdout>:  triton_bmm_58 0.0078 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_bmm_64 0.0078 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_69 0.0079 ms 93.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,1]<stdout>:  bmm 0.0080 ms 93.2% 
[1,1]<stdout>:  triton_bmm_71 0.0080 ms 92.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_bmm_65 0.0082 ms 91.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.4445 seconds and 0.3292 seconds precompiling for 18 choices
[1,0]<stdout>:AUTOTUNE bmm(16x48x128, 16x128x512)
[1,0]<stdout>:strides: [192, 3072, 1], [65536, 1, 128]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  triton_bmm_62 0.0074 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,0]<stdout>:  triton_bmm_69 0.0075 ms 98.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,0]<stdout>:  triton_bmm_57 0.0076 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_bmm_65 0.0076 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,0]<stdout>:  triton_bmm_68 0.0076 ms 96.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_bmm_66 0.0077 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_bmm_71 0.0077 ms 95.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_bmm_67 0.0078 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,0]<stdout>:  triton_bmm_58 0.0079 ms 93.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_bmm_64 0.0079 ms 93.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.3389 seconds and 1.1029 seconds precompiling for 18 choices
[1,0]<stdout>:AUTOTUNE bmm(16x48x128, 16x128x512)
[1,0]<stdout>:strides: [192, 3072, 1], [65536, 1, 128]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  triton_bmm_69 0.0076 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,0]<stdout>:  triton_bmm_65 0.0076 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,0]<stdout>:  triton_bmm_71 0.0078 ms 97.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_bmm_62 0.0078 ms 96.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,0]<stdout>:  triton_bmm_66 0.0078 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_bmm_57 0.0079 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_bmm_67 0.0079 ms 95.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,0]<stdout>:  triton_bmm_68 0.0079 ms 95.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_bmm_56 0.0080 ms 94.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,0]<stdout>:  triton_bmm_63 0.0081 ms 92.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.3596 seconds and 1.1023 seconds precompiling for 18 choices
[1,0]<stdout>:AUTOTUNE bmm(16x48x128, 16x128x512)
[1,0]<stdout>:strides: [192, 3072, 1], [65536, 1, 128]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  triton_bmm_62 0.0075 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,0]<stdout>:  triton_bmm_69 0.0075 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,0]<stdout>:  triton_bmm_65 0.0076 ms 98.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,0]<stdout>:  triton_bmm_71 0.0076 ms 98.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_bmm_68 0.0076 ms 97.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_bmm_56 0.0077 ms 97.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,0]<stdout>:  triton_bmm_57 0.0077 ms 97.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_bmm_66 0.0077 ms 97.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_bmm_67 0.0077 ms 96.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,0]<stdout>:  triton_bmm_58 0.0078 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.3438 seconds and 1.1078 seconds precompiling for 18 choices
[1,0]<stdout>:AUTOTUNE bmm(16x48x128, 16x128x512)
[1,0]<stdout>:strides: [192, 3072, 1], [65536, 1, 128]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  triton_bmm_62 0.0074 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,0]<stdout>:  triton_bmm_68 0.0074 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_bmm_66 0.0074 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_bmm_56 0.0076 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,0]<stdout>:  triton_bmm_58 0.0076 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_bmm_64 0.0078 ms 95.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_bmm_69 0.0078 ms 94.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,0]<stdout>:  bmm 0.0079 ms 93.9% 
[1,0]<stdout>:  triton_bmm_65 0.0080 ms 92.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,0]<stdout>:  triton_bmm_57 0.0081 ms 91.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.3621 seconds and 1.1163 seconds precompiling for 18 choices
[1,0]<stdout>:AUTOTUNE bmm(16x48x128, 16x128x512)
[1,0]<stdout>:strides: [192, 3072, 1], [65536, 1, 128]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  triton_bmm_62 0.0073 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,0]<stdout>:  triton_bmm_68 0.0073 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_bmm_56 0.0074 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,0]<stdout>:  triton_bmm_66 0.0075 ms 96.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_bmm_64 0.0077 ms 94.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_bmm_65 0.0077 ms 94.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,0]<stdout>:  triton_bmm_58 0.0077 ms 94.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_bmm_69 0.0077 ms 94.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,0]<stdout>:  triton_bmm_71 0.0078 ms 92.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  bmm 0.0079 ms 92.3% 
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.3452 seconds and 1.0678 seconds precompiling for 18 choices
[1,0]<stdout>:AUTOTUNE bmm(16x48x128, 16x128x512)
[1,0]<stdout>:strides: [192, 3072, 1], [65536, 1, 128]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  triton_bmm_62 0.0074 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,0]<stdout>:  triton_bmm_68 0.0075 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_bmm_66 0.0076 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_bmm_64 0.0077 ms 96.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_bmm_56 0.0077 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,0]<stdout>:  triton_bmm_58 0.0078 ms 95.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_bmm_69 0.0078 ms 95.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,0]<stdout>:  triton_bmm_65 0.0079 ms 93.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,0]<stdout>:  triton_bmm_67 0.0079 ms 93.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,0]<stdout>:  bmm 0.0080 ms 92.8% 
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.3526 seconds and 1.0662 seconds precompiling for 18 choices
[1,0]<stdout>:AUTOTUNE mm(96x7168, 7168x256)
[1,0]<stdout>:strides: [7168, 1], [1, 7168]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  mm 0.0135 ms 100.0% 
[1,0]<stdout>:  triton_mm_76 0.0196 ms 69.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_80 0.0230 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_84 0.0267 ms 50.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,0]<stdout>:  triton_mm_90 0.0411 ms 32.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_75 0.0445 ms 30.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_74 0.0472 ms 28.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_79 0.0497 ms 27.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,0]<stdout>:  triton_mm_83 0.0497 ms 27.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_mm_73 0.0531 ms 25.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.5394 seconds and 1.6144 seconds precompiling for 20 choices
[1,1]<stdout>:AUTOTUNE mm(96x7168, 7168x256)
[1,1]<stdout>:strides: [7168, 1], [1, 7168]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  mm 0.0136 ms 100.0% 
[1,1]<stdout>:  triton_mm_76 0.0195 ms 69.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_80 0.0230 ms 59.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_84 0.0268 ms 50.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,1]<stdout>:  triton_mm_90 0.0414 ms 32.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_75 0.0443 ms 30.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_74 0.0466 ms 29.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_79 0.0494 ms 27.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,1]<stdout>:  triton_mm_83 0.0496 ms 27.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_mm_73 0.0532 ms 25.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.5007 seconds and 1.6431 seconds precompiling for 20 choices
[1,1]<stdout>:AUTOTUNE mm(96x7168, 7168x256)
[1,1]<stdout>:strides: [7168, 1], [1, 7168]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  mm 0.0133 ms 100.0% 
[1,1]<stdout>:  triton_mm_76 0.0194 ms 68.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_80 0.0227 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_84 0.0265 ms 50.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,1]<stdout>:  triton_mm_90 0.0404 ms 33.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_75 0.0443 ms 30.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_74 0.0456 ms 29.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_83 0.0488 ms 27.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_mm_79 0.0493 ms 27.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,1]<stdout>:  triton_mm_73 0.0524 ms 25.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.4875 seconds and 1.8512 seconds precompiling for 20 choices
[1,1]<stdout>:AUTOTUNE mm(96x7168, 7168x256)
[1,1]<stdout>:strides: [7168, 1], [1, 7168]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  mm 0.0133 ms 100.0% 
[1,1]<stdout>:  triton_mm_76 0.0194 ms 68.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_80 0.0226 ms 58.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_84 0.0266 ms 50.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,1]<stdout>:  triton_mm_90 0.0412 ms 32.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_75 0.0445 ms 29.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_74 0.0471 ms 28.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_79 0.0492 ms 27.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,1]<stdout>:  triton_mm_83 0.0498 ms 26.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_mm_73 0.0530 ms 25.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.4874 seconds and 1.7341 seconds precompiling for 20 choices
[1,1]<stdout>:AUTOTUNE mm(96x7168, 7168x256)
[1,1]<stdout>:strides: [7168, 1], [1, 7168]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  mm 0.0132 ms 100.0% 
[1,1]<stdout>:  triton_mm_76 0.0193 ms 68.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_80 0.0227 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_84 0.0267 ms 49.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,1]<stdout>:  triton_mm_90 0.0410 ms 32.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_75 0.0446 ms 29.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_74 0.0471 ms 28.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_79 0.0493 ms 26.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,1]<stdout>:  triton_mm_83 0.0499 ms 26.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_mm_73 0.0532 ms 24.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.4880 seconds and 1.8571 seconds precompiling for 20 choices
[1,1]<stdout>:AUTOTUNE mm(96x7168, 7168x256)
[1,1]<stdout>:strides: [7168, 1], [1, 7168]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  mm 0.0132 ms 100.0% 
[1,1]<stdout>:  triton_mm_76 0.0190 ms 69.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_80 0.0225 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_84 0.0261 ms 50.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,1]<stdout>:  triton_mm_90 0.0406 ms 32.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_75 0.0446 ms 29.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_74 0.0462 ms 28.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_79 0.0497 ms 26.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,1]<stdout>:  triton_mm_83 0.0498 ms 26.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_mm_73 0.0531 ms 24.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.4888 seconds and 1.8024 seconds precompiling for 20 choices
[1,1]<stdout>:AUTOTUNE mm(96x7168, 7168x256)
[1,1]<stdout>:strides: [7168, 1], [1, 7168]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  mm 0.0133 ms 100.0% 
[1,1]<stdout>:  triton_mm_76 0.0196 ms 67.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_80 0.0227 ms 58.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_84 0.0268 ms 49.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,1]<stdout>:  triton_mm_90 0.0409 ms 32.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_75 0.0445 ms 29.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_74 0.0463 ms 28.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_79 0.0492 ms 27.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,1]<stdout>:  triton_mm_83 0.0493 ms 26.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_mm_73 0.0527 ms 25.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.4942 seconds and 1.8491 seconds precompiling for 20 choices
[1,1]<stdout>:AUTOTUNE mm(96x7168, 7168x256)
[1,1]<stdout>:strides: [7168, 1], [1, 7168]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  mm 0.0132 ms 100.0% 
[1,1]<stdout>:  triton_mm_76 0.0194 ms 68.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_80 0.0228 ms 58.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_84 0.0266 ms 49.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,1]<stdout>:  triton_mm_90 0.0411 ms 32.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_75 0.0443 ms 29.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_74 0.0468 ms 28.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_79 0.0494 ms 26.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,1]<stdout>:  triton_mm_83 0.0494 ms 26.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_mm_73 0.0532 ms 24.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.4959 seconds and 1.7843 seconds precompiling for 20 choices
[1,1]<stdout>:AUTOTUNE mm(96x7168, 7168x256)
[1,1]<stdout>:strides: [7168, 1], [1, 7168]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  mm 0.0133 ms 100.0% 
[1,1]<stdout>:  triton_mm_76 0.0194 ms 68.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_80 0.0229 ms 58.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_84 0.0266 ms 50.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,1]<stdout>:  triton_mm_90 0.0411 ms 32.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_75 0.0444 ms 30.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_74 0.0468 ms 28.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_79 0.0494 ms 27.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,1]<stdout>:  triton_mm_83 0.0496 ms 26.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_mm_73 0.0527 ms 25.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.5004 seconds and 1.8542 seconds precompiling for 20 choices
[1,0]<stdout>:AUTOTUNE mm(96x7168, 7168x256)
[1,0]<stdout>:strides: [7168, 1], [1, 7168]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  mm 0.0134 ms 100.0% 
[1,0]<stdout>:  triton_mm_76 0.0195 ms 68.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_80 0.0228 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_84 0.0264 ms 50.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,0]<stdout>:  triton_mm_90 0.0407 ms 32.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_75 0.0443 ms 30.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_74 0.0468 ms 28.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_83 0.0489 ms 27.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_mm_79 0.0491 ms 27.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,0]<stdout>:  triton_mm_73 0.0532 ms 25.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.4288 seconds and 1.9994 seconds precompiling for 20 choices
[1,0]<stdout>:AUTOTUNE mm(96x7168, 7168x256)
[1,0]<stdout>:strides: [7168, 1], [1, 7168]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  mm 0.0131 ms 100.0% 
[1,0]<stdout>:  triton_mm_76 0.0189 ms 69.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_80 0.0224 ms 58.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_84 0.0262 ms 49.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,0]<stdout>:  triton_mm_90 0.0409 ms 32.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_75 0.0443 ms 29.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_74 0.0461 ms 28.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_79 0.0491 ms 26.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,0]<stdout>:  triton_mm_83 0.0497 ms 26.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_mm_73 0.0525 ms 24.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.4163 seconds and 2.0858 seconds precompiling for 20 choices
[1,0]<stdout>:AUTOTUNE mm(96x7168, 7168x256)
[1,0]<stdout>:strides: [7168, 1], [1, 7168]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  mm 0.0131 ms 100.0% 
[1,0]<stdout>:  triton_mm_76 0.0192 ms 68.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_80 0.0228 ms 57.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_84 0.0264 ms 49.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,0]<stdout>:  triton_mm_90 0.0411 ms 31.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_75 0.0442 ms 29.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_74 0.0467 ms 28.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_79 0.0494 ms 26.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,0]<stdout>:  triton_mm_83 0.0496 ms 26.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_mm_73 0.0524 ms 25.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.4157 seconds and 2.0800 seconds precompiling for 20 choices
[1,0]<stdout>:AUTOTUNE mm(96x7168, 7168x256)
[1,0]<stdout>:strides: [7168, 1], [1, 7168]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  mm 0.0130 ms 100.0% 
[1,0]<stdout>:  triton_mm_76 0.0191 ms 68.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_80 0.0227 ms 57.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_84 0.0265 ms 49.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,0]<stdout>:  triton_mm_90 0.0406 ms 32.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_75 0.0439 ms 29.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_74 0.0465 ms 28.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_83 0.0488 ms 26.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_mm_79 0.0492 ms 26.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,0]<stdout>:  triton_mm_73 0.0526 ms 24.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.4184 seconds and 2.0697 seconds precompiling for 20 choices
[1,0]<stdout>:AUTOTUNE mm(96x7168, 7168x256)
[1,0]<stdout>:strides: [7168, 1], [1, 7168]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  mm 0.0134 ms 100.0% 
[1,0]<stdout>:  triton_mm_76 0.0190 ms 70.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_80 0.0225 ms 59.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_84 0.0262 ms 51.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,0]<stdout>:  triton_mm_90 0.0407 ms 32.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_75 0.0447 ms 30.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_74 0.0465 ms 28.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_79 0.0498 ms 26.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,0]<stdout>:  triton_mm_83 0.0500 ms 26.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_mm_73 0.0533 ms 25.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.4183 seconds and 2.0733 seconds precompiling for 20 choices
[1,0]<stdout>:AUTOTUNE mm(96x7168, 7168x256)
[1,0]<stdout>:strides: [7168, 1], [1, 7168]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  mm 0.0133 ms 100.0% 
[1,0]<stdout>:  triton_mm_76 0.0190 ms 69.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_80 0.0226 ms 58.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_84 0.0259 ms 51.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,0]<stdout>:  triton_mm_90 0.0404 ms 32.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_75 0.0445 ms 29.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_74 0.0453 ms 29.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_83 0.0491 ms 27.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_mm_79 0.0499 ms 26.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,0]<stdout>:  triton_mm_73 0.0523 ms 25.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.4182 seconds and 2.0715 seconds precompiling for 20 choices
[1,0]<stdout>:AUTOTUNE mm(96x7168, 7168x256)
[1,0]<stdout>:strides: [7168, 1], [1, 7168]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  mm 0.0134 ms 100.0% 
[1,0]<stdout>:  triton_mm_76 0.0196 ms 68.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_80 0.0230 ms 58.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_84 0.0267 ms 50.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,0]<stdout>:  triton_mm_90 0.0416 ms 32.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_75 0.0445 ms 30.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_74 0.0470 ms 28.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_83 0.0494 ms 27.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_mm_79 0.0494 ms 27.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,0]<stdout>:  triton_mm_73 0.0527 ms 25.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.4220 seconds and 2.0876 seconds precompiling for 20 choices
[1,1]<stdout>:AUTOTUNE mm(96x7168, 7168x8080)
[1,1]<stdout>:strides: [7168, 1], [1, 7168]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  mm 0.0534 ms 100.0% 
[1,1]<stdout>:  triton_mm_103 0.0541 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,1]<stdout>:  triton_mm_109 0.0595 ms 89.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_99 0.0604 ms 88.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_98 0.0737 ms 72.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,1]<stdout>:  triton_mm_102 0.0740 ms 72.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_mm_108 0.0778 ms 68.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_mm_95 0.0781 ms 68.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_92 0.0822 ms 65.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,1]<stdout>:  triton_mm_105 0.0910 ms 58.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.5730 seconds and 0.9742 seconds precompiling for 20 choices
[1,1]<stdout>:AUTOTUNE mm(96x7168, 7168x8080)
[1,1]<stdout>:strides: [7168, 1], [1, 7168]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  mm 0.0534 ms 100.0% 
[1,1]<stdout>:  triton_mm_103 0.0541 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,1]<stdout>:  triton_mm_109 0.0591 ms 90.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_99 0.0604 ms 88.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_98 0.0736 ms 72.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,1]<stdout>:  triton_mm_102 0.0741 ms 72.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_mm_108 0.0770 ms 69.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_mm_95 0.0782 ms 68.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_92 0.0816 ms 65.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,1]<stdout>:  triton_mm_105 0.0909 ms 58.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.5681 seconds and 1.1110 seconds precompiling for 20 choices
[1,1]<stdout>:AUTOTUNE mm(96x7168, 7168x8080)
[1,1]<stdout>:strides: [7168, 1], [1, 7168]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  mm 0.0533 ms 100.0% 
[1,1]<stdout>:  triton_mm_103 0.0539 ms 98.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,1]<stdout>:  triton_mm_109 0.0589 ms 90.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_99 0.0606 ms 88.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_102 0.0735 ms 72.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_mm_98 0.0739 ms 72.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,1]<stdout>:  triton_mm_108 0.0770 ms 69.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_mm_95 0.0780 ms 68.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_92 0.0821 ms 64.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,1]<stdout>:  triton_mm_105 0.0911 ms 58.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.5710 seconds and 1.1056 seconds precompiling for 20 choices
[1,1]<stdout>:AUTOTUNE mm(96x7168, 7168x8080)
[1,1]<stdout>:strides: [7168, 1], [1, 7168]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  mm 0.0538 ms 100.0% 
[1,1]<stdout>:  triton_mm_103 0.0546 ms 98.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,1]<stdout>:  triton_mm_109 0.0597 ms 90.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_99 0.0604 ms 88.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_98 0.0742 ms 72.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,1]<stdout>:  triton_mm_102 0.0743 ms 72.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_mm_108 0.0776 ms 69.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_mm_95 0.0786 ms 68.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_92 0.0820 ms 65.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,1]<stdout>:  triton_mm_105 0.0923 ms 58.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.5712 seconds and 1.1269 seconds precompiling for 20 choices
[1,1]<stdout>:AUTOTUNE mm(96x7168, 7168x8080)
[1,1]<stdout>:strides: [7168, 1], [1, 7168]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  mm 0.0537 ms 100.0% 
[1,1]<stdout>:  triton_mm_103 0.0543 ms 98.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,1]<stdout>:  triton_mm_109 0.0591 ms 90.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_99 0.0604 ms 88.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_98 0.0738 ms 72.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,1]<stdout>:  triton_mm_102 0.0743 ms 72.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_mm_108 0.0769 ms 69.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_mm_95 0.0785 ms 68.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_92 0.0820 ms 65.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,1]<stdout>:  triton_mm_105 0.0912 ms 58.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.5693 seconds and 1.0984 seconds precompiling for 20 choices
[1,1]<stdout>:AUTOTUNE mm(96x7168, 7168x8080)
[1,1]<stdout>:strides: [7168, 1], [1, 7168]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  mm 0.0539 ms 100.0% 
[1,1]<stdout>:  triton_mm_103 0.0545 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,1]<stdout>:  triton_mm_109 0.0597 ms 90.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_99 0.0608 ms 88.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_98 0.0738 ms 73.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,1]<stdout>:  triton_mm_102 0.0741 ms 72.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_mm_108 0.0773 ms 69.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_mm_95 0.0784 ms 68.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_92 0.0813 ms 66.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,1]<stdout>:  triton_mm_105 0.0913 ms 59.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.5692 seconds and 1.1165 seconds precompiling for 20 choices
[1,1]<stdout>:AUTOTUNE mm(96x7168, 7168x8080)
[1,1]<stdout>:strides: [7168, 1], [1, 7168]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  triton_mm_103 0.0540 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,1]<stdout>:  mm 0.0540 ms 99.9% 
[1,1]<stdout>:  triton_mm_109 0.0590 ms 91.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_99 0.0602 ms 89.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_98 0.0738 ms 73.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,1]<stdout>:  triton_mm_102 0.0738 ms 73.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_mm_108 0.0771 ms 70.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_mm_95 0.0787 ms 68.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_92 0.0816 ms 66.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,1]<stdout>:  triton_mm_105 0.0910 ms 59.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.5727 seconds and 1.0998 seconds precompiling for 20 choices
[1,0]<stdout>:AUTOTUNE mm(96x7168, 7168x8080)
[1,0]<stdout>:strides: [7168, 1], [1, 7168]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  triton_mm_103 0.0540 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,0]<stdout>:  triton_mm_109 0.0588 ms 91.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_99 0.0603 ms 89.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_102 0.0741 ms 72.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_mm_98 0.0743 ms 72.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,0]<stdout>:  triton_mm_108 0.0774 ms 69.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_mm_95 0.0783 ms 69.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_92 0.0827 ms 65.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,0]<stdout>:  mm 0.0860 ms 62.8% 
[1,0]<stdout>:  triton_mm_105 0.0903 ms 59.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.6016 seconds and 1.1524 seconds precompiling for 20 choices
[1,0]<stdout>:AUTOTUNE mm(96x7168, 7168x8080)
[1,0]<stdout>:strides: [7168, 1], [1, 7168]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  triton_mm_103 0.0541 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,0]<stdout>:  mm 0.0549 ms 98.5% 
[1,0]<stdout>:  triton_mm_109 0.0592 ms 91.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_99 0.0600 ms 90.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_98 0.0740 ms 73.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,0]<stdout>:  triton_mm_102 0.0740 ms 73.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_mm_108 0.0774 ms 69.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_mm_95 0.0780 ms 69.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_92 0.0820 ms 66.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,0]<stdout>:  triton_mm_105 0.0909 ms 59.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.5515 seconds and 1.1970 seconds precompiling for 20 choices
[1,0]<stdout>:AUTOTUNE mm(96x7168, 7168x8080)
[1,0]<stdout>:strides: [7168, 1], [1, 7168]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  triton_mm_103 0.0538 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,0]<stdout>:  mm 0.0542 ms 99.2% 
[1,0]<stdout>:  triton_mm_109 0.0586 ms 91.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_99 0.0606 ms 88.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_102 0.0734 ms 73.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_mm_98 0.0735 ms 73.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,0]<stdout>:  triton_mm_108 0.0767 ms 70.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_mm_95 0.0781 ms 68.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_92 0.0812 ms 66.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,0]<stdout>:  triton_mm_105 0.0908 ms 59.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.5531 seconds and 1.2154 seconds precompiling for 20 choices
[1,0]<stdout>:AUTOTUNE mm(96x7168, 7168x8080)
[1,0]<stdout>:strides: [7168, 1], [1, 7168]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  triton_mm_103 0.0536 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,0]<stdout>:  mm 0.0537 ms 99.8% 
[1,0]<stdout>:  triton_mm_109 0.0586 ms 91.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_99 0.0603 ms 88.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_98 0.0737 ms 72.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,0]<stdout>:  triton_mm_102 0.0739 ms 72.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_mm_108 0.0772 ms 69.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_mm_95 0.0777 ms 69.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_92 0.0812 ms 66.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,0]<stdout>:  triton_mm_105 0.0906 ms 59.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.5769 seconds and 1.2757 seconds precompiling for 20 choices
[1,0]<stdout>:AUTOTUNE mm(96x7168, 7168x8080)
[1,0]<stdout>:strides: [7168, 1], [1, 7168]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  triton_mm_103 0.0541 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,0]<stdout>:  mm 0.0543 ms 99.8% 
[1,0]<stdout>:  triton_mm_109 0.0589 ms 92.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_99 0.0604 ms 89.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_102 0.0741 ms 73.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_mm_98 0.0745 ms 72.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,0]<stdout>:  triton_mm_108 0.0774 ms 69.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_mm_95 0.0778 ms 69.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_92 0.0820 ms 66.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,0]<stdout>:  triton_mm_105 0.0908 ms 59.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.5116 seconds and 1.2797 seconds precompiling for 20 choices
[1,0]<stdout>:AUTOTUNE mm(96x7168, 7168x8080)
[1,0]<stdout>:strides: [7168, 1], [1, 7168]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  mm 0.0537 ms 100.0% 
[1,0]<stdout>:  triton_mm_103 0.0539 ms 99.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,0]<stdout>:  triton_mm_109 0.0586 ms 91.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_99 0.0601 ms 89.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_98 0.0740 ms 72.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,0]<stdout>:  triton_mm_102 0.0742 ms 72.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_mm_108 0.0772 ms 69.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_mm_95 0.0782 ms 68.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_92 0.0832 ms 64.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,0]<stdout>:  triton_mm_105 0.0909 ms 59.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.5175 seconds and 1.3709 seconds precompiling for 20 choices
[1,0]<stdout>:AUTOTUNE mm(96x7168, 7168x8080)
[1,0]<stdout>:strides: [7168, 1], [1, 7168]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  mm 0.0535 ms 100.0% 
[1,0]<stdout>:  triton_mm_103 0.0544 ms 98.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,0]<stdout>:  triton_mm_109 0.0594 ms 90.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_99 0.0606 ms 88.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_98 0.0739 ms 72.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,0]<stdout>:  triton_mm_102 0.0741 ms 72.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_mm_108 0.0773 ms 69.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_mm_95 0.0785 ms 68.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_92 0.0827 ms 64.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,0]<stdout>:  triton_mm_105 0.0915 ms 58.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.5589 seconds and 1.2635 seconds precompiling for 20 choices
[1,0]<stdout>:AUTOTUNE mm(96x7168, 7168x8080)
[1,0]<stdout>:strides: [7168, 1], [1, 7168]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  mm 0.0534 ms 100.0% 
[1,0]<stdout>:  triton_mm_103 0.0546 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,0]<stdout>:  triton_mm_109 0.0597 ms 89.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_99 0.0616 ms 86.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_98 0.0740 ms 72.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,0]<stdout>:  triton_mm_102 0.0743 ms 71.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_mm_108 0.0777 ms 68.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_mm_95 0.0787 ms 67.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_92 0.0818 ms 65.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,0]<stdout>:  triton_mm_105 0.0921 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 1.9003 seconds and 0.9601 seconds precompiling for 20 choices
[1,1]<stdout>:AUTOTUNE mm(96x7168, 7168x8080)
[1,1]<stdout>:strides: [7168, 1], [1, 7168]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  triton_mm_103 0.0540 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,1]<stdout>:  mm 0.0541 ms 99.7% 
[1,1]<stdout>:  triton_mm_109 0.0588 ms 91.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_99 0.0606 ms 89.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_98 0.0739 ms 73.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,1]<stdout>:  triton_mm_102 0.0744 ms 72.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_mm_108 0.0773 ms 69.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_mm_95 0.0775 ms 69.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_92 0.0829 ms 65.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,1]<stdout>:  triton_mm_105 0.0908 ms 59.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 2.1955 seconds and 0.8259 seconds precompiling for 20 choices
[1,0]<stdout>:[rank3]:W1011 22:07:28.557000 3698710 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/_inductor/codecache.py:1026] [70/1] fx graph cache unable to load compiled graph
[1,0]<stdout>:[rank3]:W1011 22:07:28.557000 3698710 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/_inductor/codecache.py:1026] [70/1] Traceback (most recent call last):
[1,0]<stdout>:[rank3]:W1011 22:07:28.557000 3698710 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/_inductor/codecache.py:1026] [70/1]   File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_inductor/codecache.py", line 1022, in iterate_over_candidates
[1,0]<stdout>:[rank3]:W1011 22:07:28.557000 3698710 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/_inductor/codecache.py:1026] [70/1]     with open(os.path.join(subdir, path), "rb") as f:
[1,0]<stdout>:[rank3]:W1011 22:07:28.557000 3698710 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/_inductor/codecache.py:1026] [70/1]          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:[rank3]:W1011 22:07:28.557000 3698710 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/_inductor/codecache.py:1026] [70/1] FileNotFoundError: [Errno 2] No such file or directory: '/home/users/industry/ai-hpc/apacsc34/scratch/.torchinductor_cache/fxgraph/bc/fbcmarlp3lmeilnqyejteelmqmoqkwjmhc3ermnmhqltzw6wrt6p/.3125268.140737350502208.tmp'
[1,0]<stdout>:Capturing batches (bs=24 avail_mem=20.06 GB):  75% 6/8 [01:49<00:56, 28.21s/it][1,0]<stdout>:Capturing batches (bs=16 avail_mem=20.00 GB):  75% 6/8 [01:49<00:56, 28.21s/it][1,1]<stdout>:[rank10]:W1011 22:07:36.857000 3125267 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/_inductor/codecache.py:1026] [14/11] fx graph cache unable to load compiled graph
[1,1]<stdout>:[rank10]:W1011 22:07:36.857000 3125267 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/_inductor/codecache.py:1026] [14/11] Traceback (most recent call last):
[1,1]<stdout>:[rank10]:W1011 22:07:36.857000 3125267 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/_inductor/codecache.py:1026] [14/11]   File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_inductor/codecache.py", line 1022, in iterate_over_candidates
[1,1]<stdout>:[rank10]:W1011 22:07:36.857000 3125267 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/_inductor/codecache.py:1026] [14/11]     with open(os.path.join(subdir, path), "rb") as f:
[1,1]<stdout>:[rank10]:W1011 22:07:36.857000 3125267 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/_inductor/codecache.py:1026] [14/11]          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:[rank10]:W1011 22:07:36.857000 3125267 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/_inductor/codecache.py:1026] [14/11] FileNotFoundError: [Errno 2] No such file or directory: '/home/users/industry/ai-hpc/apacsc34/scratch/.torchinductor_cache/fxgraph/7i/f7ip34qxaxuohuoidovy3cctfbfznxezg5prq676lgvnx7qkhft2/.3698709.140737350502208.tmp'
[1,1]<stdout>:AUTOTUNE bmm(16x32x128, 16x128x512)
[1,1]<stdout>:strides: [192, 3072, 1], [65536, 1, 128]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  triton_bmm_112 0.0075 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_bmm_118 0.0075 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_bmm_124 0.0076 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,1]<stdout>:  triton_bmm_114 0.0076 ms 98.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_bmm_117 0.0076 ms 98.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,1]<stdout>:  triton_bmm_113 0.0076 ms 97.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_bmm_121 0.0077 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_111 0.0078 ms 95.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,1]<stdout>:  triton_bmm_123 0.0078 ms 95.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_120 0.0078 ms 95.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.3225 seconds and 0.6148 seconds precompiling for 18 choices
[1,1]<stdout>:AUTOTUNE bmm(16x32x128, 16x128x512)
[1,1]<stdout>:strides: [192, 3072, 1], [65536, 1, 128]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  triton_bmm_118 0.0073 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_bmm_112 0.0074 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_bmm_113 0.0074 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_bmm_114 0.0075 ms 98.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_bmm_117 0.0075 ms 98.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,1]<stdout>:  triton_bmm_124 0.0075 ms 97.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,1]<stdout>:  triton_bmm_126 0.0075 ms 97.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_bmm_111 0.0076 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,1]<stdout>:  triton_bmm_123 0.0076 ms 96.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_120 0.0077 ms 95.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.3360 seconds and 0.6726 seconds precompiling for 18 choices
[1,1]<stdout>:AUTOTUNE bmm(16x32x128, 16x128x512)
[1,1]<stdout>:strides: [192, 3072, 1], [65536, 1, 128]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  triton_bmm_118 0.0073 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_bmm_113 0.0074 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_bmm_117 0.0074 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,1]<stdout>:  triton_bmm_112 0.0074 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_bmm_114 0.0074 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_bmm_123 0.0075 ms 97.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_124 0.0075 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,1]<stdout>:  triton_bmm_111 0.0076 ms 96.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,1]<stdout>:  triton_bmm_121 0.0076 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_126 0.0077 ms 95.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.3274 seconds and 0.7244 seconds precompiling for 18 choices
[1,1]<stdout>:AUTOTUNE bmm(16x32x128, 16x128x512)
[1,1]<stdout>:strides: [192, 3072, 1], [65536, 1, 128]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  triton_bmm_118 0.0073 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_bmm_112 0.0073 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_bmm_126 0.0074 ms 98.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_bmm_124 0.0075 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,1]<stdout>:  triton_bmm_114 0.0075 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_bmm_113 0.0076 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_bmm_111 0.0076 ms 95.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,1]<stdout>:  triton_bmm_120 0.0077 ms 94.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,1]<stdout>:  triton_bmm_117 0.0077 ms 93.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,1]<stdout>:  triton_bmm_122 0.0077 ms 93.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.3518 seconds and 0.6852 seconds precompiling for 18 choices
[1,0]<stdout>:AUTOTUNE bmm(16x32x128, 16x128x512)
[1,0]<stdout>:strides: [192, 3072, 1], [65536, 1, 128]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  triton_bmm_112 0.0072 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_bmm_118 0.0073 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_bmm_124 0.0075 ms 96.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,0]<stdout>:  triton_bmm_126 0.0076 ms 95.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_bmm_113 0.0076 ms 94.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_bmm_117 0.0076 ms 94.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,0]<stdout>:  triton_bmm_120 0.0077 ms 93.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,0]<stdout>:  triton_bmm_122 0.0078 ms 92.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,0]<stdout>:  triton_bmm_121 0.0079 ms 91.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_bmm_123 0.0079 ms 91.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.4931 seconds and 0.6687 seconds precompiling for 18 choices
[1,0]<stdout>:AUTOTUNE bmm(16x32x128, 16x128x512)
[1,0]<stdout>:strides: [192, 3072, 1], [65536, 1, 128]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  triton_bmm_117 0.0071 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,0]<stdout>:  triton_bmm_113 0.0072 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_bmm_123 0.0074 ms 96.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_bmm_121 0.0075 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_bmm_112 0.0076 ms 94.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_bmm_114 0.0076 ms 94.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_bmm_118 0.0076 ms 94.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_bmm_119 0.0077 ms 92.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_bmm_120 0.0078 ms 91.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,0]<stdout>:  triton_bmm_122 0.0078 ms 91.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.4066 seconds and 0.7298 seconds precompiling for 18 choices
[1,0]<stdout>:AUTOTUNE bmm(16x32x128, 16x128x512)
[1,0]<stdout>:strides: [192, 3072, 1], [65536, 1, 128]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  triton_bmm_113 0.0072 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_bmm_112 0.0073 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_bmm_118 0.0073 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_bmm_111 0.0074 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,0]<stdout>:  triton_bmm_124 0.0074 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,0]<stdout>:  triton_bmm_114 0.0074 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_bmm_120 0.0076 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,0]<stdout>:  triton_bmm_123 0.0076 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_bmm_126 0.0076 ms 94.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_bmm_117 0.0076 ms 94.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.2866 seconds and 0.8982 seconds precompiling for 18 choices
[1,1]<stdout>:AUTOTUNE bmm(16x32x128, 16x128x512)
[1,1]<stdout>:strides: [192, 3072, 1], [65536, 1, 128]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  triton_bmm_112 0.0073 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_bmm_118 0.0074 ms 98.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_bmm_117 0.0074 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,1]<stdout>:  triton_bmm_114 0.0075 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_bmm_113 0.0075 ms 96.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_bmm_124 0.0075 ms 96.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,1]<stdout>:  triton_bmm_111 0.0076 ms 95.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,1]<stdout>:  triton_bmm_121 0.0076 ms 95.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_120 0.0077 ms 94.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,1]<stdout>:  triton_bmm_126 0.0078 ms 93.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.3389 seconds and 0.7557 seconds precompiling for 18 choices
[1,1]<stdout>:AUTOTUNE bmm(16x32x128, 16x128x512)
[1,1]<stdout>:strides: [192, 3072, 1], [65536, 1, 128]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  triton_bmm_112 0.0071 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_bmm_118 0.0072 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_bmm_124 0.0073 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,1]<stdout>:  triton_bmm_114 0.0074 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_bmm_111 0.0075 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,1]<stdout>:  triton_bmm_113 0.0075 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_bmm_117 0.0075 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,1]<stdout>:  triton_bmm_123 0.0076 ms 93.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_126 0.0076 ms 93.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_bmm_120 0.0076 ms 92.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.3360 seconds and 0.7698 seconds precompiling for 18 choices
[1,1]<stdout>:AUTOTUNE bmm(16x32x128, 16x128x512)
[1,1]<stdout>:strides: [192, 3072, 1], [65536, 1, 128]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  triton_bmm_118 0.0071 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_bmm_112 0.0072 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_bmm_113 0.0072 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_bmm_124 0.0072 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,1]<stdout>:  triton_bmm_114 0.0073 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_bmm_117 0.0074 ms 96.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,1]<stdout>:  triton_bmm_120 0.0076 ms 93.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,1]<stdout>:  triton_bmm_121 0.0076 ms 93.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_122 0.0076 ms 93.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,1]<stdout>:  triton_bmm_126 0.0076 ms 92.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.3306 seconds and 0.7700 seconds precompiling for 18 choices
[1,1]<stdout>:AUTOTUNE bmm(16x32x128, 16x128x512)
[1,1]<stdout>:strides: [192, 3072, 1], [65536, 1, 128]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  triton_bmm_112 0.0075 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_bmm_114 0.0076 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_bmm_118 0.0077 ms 97.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_bmm_113 0.0077 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_bmm_117 0.0077 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,1]<stdout>:  triton_bmm_124 0.0077 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,1]<stdout>:  triton_bmm_120 0.0079 ms 95.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,1]<stdout>:  triton_bmm_123 0.0080 ms 94.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_111 0.0080 ms 94.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,1]<stdout>:  bmm 0.0083 ms 91.1% 
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.3386 seconds and 0.7059 seconds precompiling for 18 choices
[1,0]<stdout>:AUTOTUNE bmm(16x32x128, 16x128x512)
[1,0]<stdout>:strides: [192, 3072, 1], [65536, 1, 128]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  triton_bmm_113 0.0072 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_bmm_117 0.0073 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,0]<stdout>:  triton_bmm_111 0.0075 ms 96.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,0]<stdout>:  triton_bmm_123 0.0075 ms 96.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_bmm_114 0.0076 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_bmm_121 0.0076 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_bmm_124 0.0076 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,0]<stdout>:  triton_bmm_118 0.0076 ms 94.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_bmm_112 0.0077 ms 93.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_bmm_119 0.0077 ms 93.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.2674 seconds and 0.9581 seconds precompiling for 18 choices
[1,0]<stdout>:AUTOTUNE bmm(16x32x128, 16x128x512)
[1,0]<stdout>:strides: [192, 3072, 1], [65536, 1, 128]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  triton_bmm_118 0.0075 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_bmm_117 0.0076 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,0]<stdout>:  triton_bmm_112 0.0076 ms 99.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_bmm_114 0.0076 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_bmm_124 0.0076 ms 98.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,0]<stdout>:  triton_bmm_126 0.0077 ms 97.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_bmm_113 0.0078 ms 96.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_bmm_120 0.0078 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,0]<stdout>:  triton_bmm_111 0.0079 ms 94.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,0]<stdout>:  triton_bmm_122 0.0080 ms 94.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.2606 seconds and 0.9499 seconds precompiling for 18 choices
[1,0]<stdout>:AUTOTUNE bmm(16x32x128, 16x128x512)
[1,0]<stdout>:strides: [192, 3072, 1], [65536, 1, 128]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  triton_bmm_117 0.0073 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,0]<stdout>:  triton_bmm_118 0.0073 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_bmm_112 0.0074 ms 98.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_bmm_114 0.0076 ms 96.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_bmm_124 0.0076 ms 96.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,0]<stdout>:  triton_bmm_113 0.0076 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_bmm_123 0.0077 ms 95.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_bmm_120 0.0077 ms 94.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,0]<stdout>:  triton_bmm_126 0.0077 ms 94.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_bmm_122 0.0078 ms 93.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.2693 seconds and 1.0025 seconds precompiling for 18 choices
[1,0]<stdout>:AUTOTUNE bmm(16x32x128, 16x128x512)
[1,0]<stdout>:strides: [192, 3072, 1], [65536, 1, 128]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  triton_bmm_112 0.0071 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_bmm_118 0.0072 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_bmm_113 0.0073 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_bmm_114 0.0073 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_bmm_117 0.0075 ms 95.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,0]<stdout>:  triton_bmm_124 0.0075 ms 95.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,0]<stdout>:  triton_bmm_120 0.0076 ms 94.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,0]<stdout>:  triton_bmm_126 0.0076 ms 93.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_bmm_111 0.0077 ms 92.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,0]<stdout>:  triton_bmm_122 0.0077 ms 92.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.2730 seconds and 0.9932 seconds precompiling for 18 choices
[1,0]<stdout>:[rank0]:W1011 22:07:40.694000 3698707 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/_inductor/codecache.py:1026] [10/12_1] fx graph cache unable to load compiled graph
[1,0]<stdout>:[rank0]:W1011 22:07:40.694000 3698707 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/_inductor/codecache.py:1026] [10/12_1] Traceback (most recent call last):
[1,0]<stdout>:[rank0]:W1011 22:07:40.694000 3698707 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/_inductor/codecache.py:1026] [10/12_1]   File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_inductor/codecache.py", line 1022, in iterate_over_candidates
[1,0]<stdout>:[rank0]:W1011 22:07:40.694000 3698707 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/_inductor/codecache.py:1026] [10/12_1]     with open(os.path.join(subdir, path), "rb") as f:
[1,0]<stdout>:[rank0]:W1011 22:07:40.694000 3698707 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/_inductor/codecache.py:1026] [10/12_1]          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:[rank0]:W1011 22:07:40.694000 3698707 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/_inductor/codecache.py:1026] [10/12_1] FileNotFoundError: [Errno 2] No such file or directory: '/home/users/industry/ai-hpc/apacsc34/scratch/.torchinductor_cache/fxgraph/qk/fqkhpcfu4jbh7iu5muk2hvhbfpqfo43eywey7mvozmu726bikj6k/.3125265.140737350502208.tmp'
[1,0]<stdout>:AUTOTUNE mm(64x7168, 7168x256)
[1,0]<stdout>:strides: [7168, 1], [1, 7168]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  mm 0.0138 ms 100.0% 
[1,0]<stdout>:  triton_mm_131 0.0196 ms 70.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_135 0.0231 ms 59.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_139 0.0268 ms 51.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,0]<stdout>:  triton_mm_143 0.0323 ms 42.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_130 0.0451 ms 30.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_129 0.0475 ms 29.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_134 0.0497 ms 27.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,0]<stdout>:  triton_mm_138 0.0499 ms 27.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_mm_128 0.0543 ms 25.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.4329 seconds and 1.6700 seconds precompiling for 18 choices
[1,1]<stdout>:AUTOTUNE mm(64x7168, 7168x256)
[1,1]<stdout>:strides: [7168, 1], [1, 7168]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  mm 0.0129 ms 100.0% 
[1,1]<stdout>:  triton_mm_131 0.0190 ms 67.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_135 0.0223 ms 57.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_139 0.0263 ms 48.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,1]<stdout>:  triton_mm_143 0.0318 ms 40.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_130 0.0447 ms 28.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_129 0.0467 ms 27.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_134 0.0495 ms 26.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,1]<stdout>:  triton_mm_138 0.0499 ms 25.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_mm_128 0.0549 ms 23.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.4324 seconds and 1.6906 seconds precompiling for 18 choices
[1,1]<stdout>:AUTOTUNE mm(64x7168, 7168x256)
[1,1]<stdout>:strides: [7168, 1], [1, 7168]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  mm 0.0130 ms 100.0% 
[1,1]<stdout>:  triton_mm_131 0.0188 ms 69.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_135 0.0224 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_139 0.0261 ms 49.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,1]<stdout>:  triton_mm_143 0.0318 ms 40.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_130 0.0446 ms 29.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_129 0.0466 ms 27.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_134 0.0495 ms 26.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,1]<stdout>:  triton_mm_138 0.0500 ms 26.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_mm_128 0.0542 ms 24.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.3980 seconds and 1.5943 seconds precompiling for 18 choices
[1,1]<stdout>:AUTOTUNE mm(64x7168, 7168x256)
[1,1]<stdout>:strides: [7168, 1], [1, 7168]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  mm 0.0133 ms 100.0% 
[1,1]<stdout>:  triton_mm_131 0.0192 ms 69.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_135 0.0227 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_139 0.0260 ms 51.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,1]<stdout>:  triton_mm_143 0.0317 ms 41.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_130 0.0450 ms 29.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_129 0.0471 ms 28.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_134 0.0496 ms 26.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,1]<stdout>:  triton_mm_138 0.0496 ms 26.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_mm_128 0.0542 ms 24.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.3981 seconds and 1.5988 seconds precompiling for 18 choices
[1,1]<stdout>:AUTOTUNE mm(64x7168, 7168x256)
[1,1]<stdout>:strides: [7168, 1], [1, 7168]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  mm 0.0136 ms 100.0% 
[1,1]<stdout>:  triton_mm_131 0.0193 ms 70.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_135 0.0228 ms 59.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_139 0.0266 ms 51.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,1]<stdout>:  triton_mm_143 0.0322 ms 42.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_130 0.0450 ms 30.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_129 0.0470 ms 28.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_134 0.0498 ms 27.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,1]<stdout>:  triton_mm_138 0.0499 ms 27.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_mm_128 0.0544 ms 24.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.3977 seconds and 1.7004 seconds precompiling for 18 choices
[1,1]<stdout>:AUTOTUNE mm(64x7168, 7168x256)
[1,1]<stdout>:strides: [7168, 1], [1, 7168]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  mm 0.0133 ms 100.0% 
[1,1]<stdout>:  triton_mm_131 0.0190 ms 69.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_135 0.0226 ms 58.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_139 0.0260 ms 51.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,1]<stdout>:  triton_mm_143 0.0319 ms 41.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_130 0.0451 ms 29.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_129 0.0469 ms 28.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_134 0.0500 ms 26.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,1]<stdout>:  triton_mm_138 0.0501 ms 26.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_mm_128 0.0543 ms 24.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.4626 seconds and 1.6594 seconds precompiling for 18 choices
[1,1]<stdout>:AUTOTUNE mm(64x7168, 7168x256)
[1,1]<stdout>:strides: [7168, 1], [1, 7168]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  mm 0.0131 ms 100.0% 
[1,1]<stdout>:  triton_mm_131 0.0189 ms 69.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_135 0.0224 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_139 0.0263 ms 49.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,1]<stdout>:  triton_mm_143 0.0318 ms 41.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_130 0.0451 ms 29.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_129 0.0467 ms 28.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_134 0.0498 ms 26.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,1]<stdout>:  triton_mm_138 0.0500 ms 26.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_mm_128 0.0542 ms 24.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.3996 seconds and 1.6760 seconds precompiling for 18 choices
[1,1]<stdout>:AUTOTUNE mm(64x7168, 7168x256)
[1,1]<stdout>:strides: [7168, 1], [1, 7168]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  mm 0.0129 ms 100.0% 
[1,1]<stdout>:  triton_mm_131 0.0193 ms 67.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_135 0.0226 ms 57.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_139 0.0262 ms 49.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,1]<stdout>:  triton_mm_143 0.0319 ms 40.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_130 0.0451 ms 28.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_129 0.0471 ms 27.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_134 0.0495 ms 26.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,1]<stdout>:  triton_mm_138 0.0497 ms 26.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_mm_128 0.0539 ms 24.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.3983 seconds and 1.7167 seconds precompiling for 18 choices
[1,1]<stdout>:AUTOTUNE mm(64x7168, 7168x256)
[1,1]<stdout>:strides: [7168, 1], [1, 7168]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  mm 0.0128 ms 100.0% 
[1,1]<stdout>:  triton_mm_131 0.0190 ms 67.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_135 0.0224 ms 57.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_139 0.0261 ms 49.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,1]<stdout>:  triton_mm_143 0.0319 ms 40.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_130 0.0451 ms 28.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_129 0.0471 ms 27.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_134 0.0497 ms 25.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,1]<stdout>:  triton_mm_138 0.0497 ms 25.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_mm_128 0.0543 ms 23.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.3996 seconds and 1.7068 seconds precompiling for 18 choices
[1,0]<stdout>:AUTOTUNE mm(64x7168, 7168x256)
[1,0]<stdout>:strides: [7168, 1], [1, 7168]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  mm 0.0134 ms 100.0% 
[1,0]<stdout>:  triton_mm_131 0.0195 ms 68.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_135 0.0227 ms 59.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_139 0.0264 ms 50.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,0]<stdout>:  triton_mm_143 0.0320 ms 41.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_130 0.0444 ms 30.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_129 0.0470 ms 28.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_138 0.0490 ms 27.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_mm_134 0.0491 ms 27.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,0]<stdout>:  triton_mm_128 0.0540 ms 24.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.3767 seconds and 1.8534 seconds precompiling for 18 choices
[1,0]<stdout>:AUTOTUNE mm(64x7168, 7168x256)
[1,0]<stdout>:strides: [7168, 1], [1, 7168]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  mm 0.0129 ms 100.0% 
[1,0]<stdout>:  triton_mm_131 0.0192 ms 67.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_135 0.0226 ms 57.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_139 0.0262 ms 49.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,0]<stdout>:  triton_mm_143 0.0319 ms 40.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_130 0.0446 ms 28.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_129 0.0468 ms 27.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_134 0.0492 ms 26.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,0]<stdout>:  triton_mm_138 0.0495 ms 26.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_mm_128 0.0536 ms 24.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.3700 seconds and 1.6958 seconds precompiling for 18 choices
[1,0]<stdout>:AUTOTUNE mm(64x7168, 7168x256)
[1,0]<stdout>:strides: [7168, 1], [1, 7168]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  mm 0.0131 ms 100.0% 
[1,0]<stdout>:  triton_mm_131 0.0189 ms 69.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_135 0.0224 ms 58.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_139 0.0261 ms 50.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,0]<stdout>:  triton_mm_143 0.0316 ms 41.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_130 0.0447 ms 29.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_129 0.0465 ms 28.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_134 0.0494 ms 26.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,0]<stdout>:  triton_mm_138 0.0498 ms 26.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_mm_128 0.0544 ms 24.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.3710 seconds and 1.8393 seconds precompiling for 18 choices
[1,0]<stdout>:AUTOTUNE mm(64x7168, 7168x256)
[1,0]<stdout>:strides: [7168, 1], [1, 7168]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  mm 0.0135 ms 100.0% 
[1,0]<stdout>:  triton_mm_131 0.0196 ms 69.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_135 0.0228 ms 59.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_139 0.0266 ms 50.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,0]<stdout>:  triton_mm_143 0.0325 ms 41.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_130 0.0448 ms 30.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_129 0.0471 ms 28.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_134 0.0491 ms 27.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,0]<stdout>:  triton_mm_138 0.0496 ms 27.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_mm_128 0.0542 ms 25.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.3772 seconds and 1.8530 seconds precompiling for 18 choices
[1,0]<stdout>:AUTOTUNE mm(64x7168, 7168x256)
[1,0]<stdout>:strides: [7168, 1], [1, 7168]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  mm 0.0135 ms 100.0% 
[1,0]<stdout>:  triton_mm_131 0.0191 ms 70.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_135 0.0225 ms 60.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_139 0.0263 ms 51.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,0]<stdout>:  triton_mm_143 0.0317 ms 42.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_130 0.0454 ms 29.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_129 0.0471 ms 28.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_134 0.0498 ms 27.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,0]<stdout>:  triton_mm_138 0.0501 ms 27.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_mm_128 0.0547 ms 24.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.3758 seconds and 1.8299 seconds precompiling for 18 choices
[1,0]<stdout>:AUTOTUNE mm(64x7168, 7168x256)
[1,0]<stdout>:strides: [7168, 1], [1, 7168]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  mm 0.0130 ms 100.0% 
[1,0]<stdout>:  triton_mm_131 0.0197 ms 66.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_135 0.0229 ms 56.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_139 0.0263 ms 49.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,0]<stdout>:  triton_mm_143 0.0319 ms 40.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_130 0.0447 ms 29.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_129 0.0475 ms 27.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_138 0.0491 ms 26.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_mm_134 0.0493 ms 26.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,0]<stdout>:  triton_mm_128 0.0532 ms 24.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.3753 seconds and 1.7391 seconds precompiling for 18 choices
[1,0]<stdout>:AUTOTUNE mm(64x7168, 7168x256)
[1,0]<stdout>:strides: [7168, 1], [1, 7168]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  mm 0.0132 ms 100.0% 
[1,0]<stdout>:  triton_mm_131 0.0195 ms 67.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_135 0.0226 ms 58.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_139 0.0263 ms 50.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,0]<stdout>:  triton_mm_143 0.0319 ms 41.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_130 0.0454 ms 29.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_129 0.0474 ms 27.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_134 0.0498 ms 26.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,0]<stdout>:  triton_mm_138 0.0498 ms 26.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_mm_128 0.0545 ms 24.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.3778 seconds and 1.8278 seconds precompiling for 18 choices
[1,1]<stdout>:AUTOTUNE mm(64x7168, 7168x8080)
[1,1]<stdout>:strides: [7168, 1], [1, 7168]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  triton_mm_152 0.0525 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_156 0.0536 ms 98.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,1]<stdout>:  mm 0.0550 ms 95.6% 
[1,1]<stdout>:  triton_mm_148 0.0562 ms 93.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_160 0.0568 ms 92.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_151 0.0725 ms 72.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,1]<stdout>:  triton_mm_155 0.0734 ms 71.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_mm_145 0.0748 ms 70.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,1]<stdout>:  triton_mm_147 0.0766 ms 68.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_146 0.0802 ms 65.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.5598 seconds and 0.9548 seconds precompiling for 18 choices
[1,1]<stdout>:AUTOTUNE mm(64x7168, 7168x8080)
[1,1]<stdout>:strides: [7168, 1], [1, 7168]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  triton_mm_152 0.0526 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  mm 0.0538 ms 97.7% 
[1,1]<stdout>:  triton_mm_156 0.0540 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,1]<stdout>:  triton_mm_148 0.0564 ms 93.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_160 0.0568 ms 92.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_151 0.0727 ms 72.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,1]<stdout>:  triton_mm_155 0.0741 ms 70.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_mm_145 0.0755 ms 69.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,1]<stdout>:  triton_mm_147 0.0769 ms 68.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_146 0.0812 ms 64.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.5600 seconds and 0.9723 seconds precompiling for 18 choices
[1,1]<stdout>:AUTOTUNE mm(64x7168, 7168x8080)
[1,1]<stdout>:strides: [7168, 1], [1, 7168]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  triton_mm_152 0.0526 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  mm 0.0533 ms 98.7% 
[1,1]<stdout>:  triton_mm_156 0.0539 ms 97.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,1]<stdout>:  triton_mm_148 0.0567 ms 92.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_160 0.0568 ms 92.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_151 0.0728 ms 72.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,1]<stdout>:  triton_mm_155 0.0740 ms 71.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_mm_145 0.0752 ms 70.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,1]<stdout>:  triton_mm_147 0.0770 ms 68.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_146 0.0807 ms 65.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.5487 seconds and 0.8490 seconds precompiling for 18 choices
[1,1]<stdout>:AUTOTUNE mm(64x7168, 7168x8080)
[1,1]<stdout>:strides: [7168, 1], [1, 7168]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  triton_mm_152 0.0527 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  mm 0.0531 ms 99.3% 
[1,1]<stdout>:  triton_mm_156 0.0541 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,1]<stdout>:  triton_mm_148 0.0565 ms 93.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_160 0.0573 ms 92.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_151 0.0730 ms 72.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,1]<stdout>:  triton_mm_155 0.0741 ms 71.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_mm_145 0.0754 ms 69.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,1]<stdout>:  triton_mm_147 0.0793 ms 66.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_146 0.0812 ms 64.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.5611 seconds and 0.9469 seconds precompiling for 18 choices
[1,1]<stdout>:AUTOTUNE mm(64x7168, 7168x8080)
[1,1]<stdout>:strides: [7168, 1], [1, 7168]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  triton_mm_152 0.0525 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  mm 0.0532 ms 98.8% 
[1,1]<stdout>:  triton_mm_156 0.0540 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,1]<stdout>:  triton_mm_148 0.0565 ms 93.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_160 0.0571 ms 91.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_151 0.0732 ms 71.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,1]<stdout>:  triton_mm_155 0.0744 ms 70.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_mm_145 0.0751 ms 69.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,1]<stdout>:  triton_mm_147 0.0779 ms 67.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_146 0.0809 ms 64.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.5982 seconds and 0.9366 seconds precompiling for 18 choices
[1,1]<stdout>:AUTOTUNE mm(64x7168, 7168x8080)
[1,1]<stdout>:strides: [7168, 1], [1, 7168]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  triton_mm_152 0.0527 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  mm 0.0533 ms 98.9% 
[1,1]<stdout>:  triton_mm_156 0.0541 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,1]<stdout>:  triton_mm_148 0.0566 ms 93.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_160 0.0570 ms 92.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_151 0.0729 ms 72.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,1]<stdout>:  triton_mm_155 0.0741 ms 71.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_mm_145 0.0754 ms 69.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,1]<stdout>:  triton_mm_147 0.0780 ms 67.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_146 0.0806 ms 65.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.5616 seconds and 0.9438 seconds precompiling for 18 choices
[1,0]<stdout>:AUTOTUNE mm(64x7168, 7168x8080)
[1,0]<stdout>:strides: [7168, 1], [1, 7168]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  triton_mm_152 0.0526 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  mm 0.0537 ms 98.1% 
[1,0]<stdout>:  triton_mm_156 0.0539 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,0]<stdout>:  triton_mm_160 0.0565 ms 93.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_148 0.0570 ms 92.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_151 0.0728 ms 72.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,0]<stdout>:  triton_mm_155 0.0738 ms 71.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_mm_145 0.0750 ms 70.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,0]<stdout>:  triton_mm_147 0.0777 ms 67.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_146 0.0807 ms 65.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.4258 seconds and 1.2030 seconds precompiling for 18 choices
[1,0]<stdout>:AUTOTUNE mm(64x7168, 7168x8080)
[1,0]<stdout>:strides: [7168, 1], [1, 7168]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  triton_mm_152 0.0524 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  mm 0.0534 ms 98.1% 
[1,0]<stdout>:  triton_mm_156 0.0540 ms 97.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,0]<stdout>:  triton_mm_148 0.0567 ms 92.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_160 0.0569 ms 92.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_151 0.0729 ms 71.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,0]<stdout>:  triton_mm_155 0.0741 ms 70.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_mm_145 0.0757 ms 69.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,0]<stdout>:  triton_mm_147 0.0789 ms 66.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_146 0.0806 ms 65.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.4462 seconds and 1.1891 seconds precompiling for 18 choices
[1,1]<stdout>:AUTOTUNE mm(64x7168, 7168x8080)
[1,1]<stdout>:strides: [7168, 1], [1, 7168]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  triton_mm_152 0.0528 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  mm 0.0530 ms 99.6% 
[1,1]<stdout>:  triton_mm_156 0.0539 ms 97.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,1]<stdout>:  triton_mm_148 0.0565 ms 93.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_160 0.0568 ms 92.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_151 0.0729 ms 72.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,1]<stdout>:  triton_mm_155 0.0742 ms 71.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_mm_145 0.0757 ms 69.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,1]<stdout>:  triton_mm_147 0.0800 ms 66.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_146 0.0804 ms 65.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.7967 seconds and 0.9242 seconds precompiling for 18 choices
[1,0]<stdout>:AUTOTUNE mm(64x7168, 7168x8080)
[1,0]<stdout>:strides: [7168, 1], [1, 7168]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  triton_mm_152 0.0527 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  mm 0.0532 ms 99.0% 
[1,0]<stdout>:  triton_mm_156 0.0537 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,0]<stdout>:  triton_mm_148 0.0563 ms 93.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_160 0.0567 ms 92.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_151 0.0725 ms 72.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,0]<stdout>:  triton_mm_155 0.0732 ms 71.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_mm_145 0.0753 ms 70.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,0]<stdout>:  triton_mm_147 0.0780 ms 67.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_146 0.0805 ms 65.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.4296 seconds and 1.2771 seconds precompiling for 18 choices
[1,0]<stdout>:AUTOTUNE mm(64x7168, 7168x8080)
[1,0]<stdout>:strides: [7168, 1], [1, 7168]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  triton_mm_152 0.0526 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  mm 0.0530 ms 99.3% 
[1,0]<stdout>:  triton_mm_156 0.0539 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,0]<stdout>:  triton_mm_148 0.0563 ms 93.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_160 0.0569 ms 92.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_151 0.0723 ms 72.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,0]<stdout>:  triton_mm_155 0.0733 ms 71.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_mm_145 0.0752 ms 70.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,0]<stdout>:  triton_mm_147 0.0769 ms 68.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_146 0.0811 ms 64.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.4314 seconds and 1.2844 seconds precompiling for 18 choices
[1,0]<stdout>:AUTOTUNE mm(64x7168, 7168x8080)
[1,0]<stdout>:strides: [7168, 1], [1, 7168]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  mm 0.0526 ms 100.0% 
[1,0]<stdout>:  triton_mm_152 0.0527 ms 99.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_156 0.0540 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,0]<stdout>:  triton_mm_160 0.0567 ms 92.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_148 0.0568 ms 92.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_151 0.0719 ms 73.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,0]<stdout>:  triton_mm_155 0.0732 ms 71.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_mm_145 0.0750 ms 70.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,0]<stdout>:  triton_mm_147 0.0770 ms 68.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_146 0.0806 ms 65.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.6603 seconds and 1.3038 seconds precompiling for 18 choices
[1,0]<stdout>:AUTOTUNE mm(64x7168, 7168x8080)
[1,0]<stdout>:strides: [7168, 1], [1, 7168]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  triton_mm_152 0.0527 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  mm 0.0529 ms 99.7% 
[1,0]<stdout>:  triton_mm_156 0.0542 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,0]<stdout>:  triton_mm_148 0.0566 ms 93.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_160 0.0569 ms 92.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_151 0.0719 ms 73.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,0]<stdout>:  triton_mm_155 0.0732 ms 72.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_mm_145 0.0757 ms 69.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,0]<stdout>:  triton_mm_147 0.0786 ms 67.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_146 0.0813 ms 64.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.6779 seconds and 1.3047 seconds precompiling for 18 choices
[1,0]<stdout>:AUTOTUNE mm(64x7168, 7168x8080)
[1,0]<stdout>:strides: [7168, 1], [1, 7168]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  triton_mm_152 0.0525 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  mm 0.0530 ms 99.1% 
[1,0]<stdout>:  triton_mm_156 0.0540 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,0]<stdout>:  triton_mm_148 0.0563 ms 93.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_160 0.0570 ms 92.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_151 0.0729 ms 72.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,0]<stdout>:  triton_mm_155 0.0743 ms 70.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_mm_145 0.0756 ms 69.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,0]<stdout>:  triton_mm_147 0.0772 ms 68.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_146 0.0807 ms 65.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.7030 seconds and 1.3243 seconds precompiling for 18 choices
[1,0]<stdout>:AUTOTUNE mm(64x7168, 7168x8080)
[1,0]<stdout>:strides: [7168, 1], [1, 7168]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  triton_mm_152 0.0527 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  mm 0.0532 ms 99.1% 
[1,0]<stdout>:  triton_mm_156 0.0539 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,0]<stdout>:  triton_mm_148 0.0564 ms 93.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_160 0.0567 ms 92.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_151 0.0728 ms 72.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,0]<stdout>:  triton_mm_155 0.0735 ms 71.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_mm_145 0.0750 ms 70.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,0]<stdout>:  triton_mm_147 0.0789 ms 66.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_146 0.0807 ms 65.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 1.6917 seconds and 0.7681 seconds precompiling for 18 choices
[1,1]<stdout>:AUTOTUNE mm(64x7168, 7168x8080)
[1,1]<stdout>:strides: [7168, 1], [1, 7168]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  triton_mm_152 0.0523 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_156 0.0536 ms 97.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,1]<stdout>:  mm 0.0540 ms 96.9% 
[1,1]<stdout>:  triton_mm_148 0.0560 ms 93.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_160 0.0566 ms 92.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_151 0.0726 ms 72.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,1]<stdout>:  triton_mm_155 0.0738 ms 70.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_mm_145 0.0758 ms 69.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,1]<stdout>:  triton_mm_147 0.0767 ms 68.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_146 0.0808 ms 64.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 2.1710 seconds and 0.6988 seconds precompiling for 18 choices
[1,0]<stdout>:Capturing batches (bs=16 avail_mem=20.00 GB):  88% 7/8 [02:30<00:32, 32.44s/it][1,0]<stdout>:Capturing batches (bs=8 avail_mem=19.95 GB):  88% 7/8 [02:30<00:32, 32.44s/it] [1,1]<stdout>:AUTOTUNE bmm(16x16x128, 16x128x512)
[1,1]<stdout>:strides: [192, 3072, 1], [65536, 1, 128]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  triton_bmm_164 0.0069 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
[1,1]<stdout>:  triton_bmm_168 0.0071 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_163 0.0071 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_bmm_172 0.0072 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_174 0.0072 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_175 0.0074 ms 93.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,1]<stdout>:  bmm 0.0076 ms 91.9% 
[1,1]<stdout>:  triton_bmm_170 0.0076 ms 91.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_177 0.0076 ms 91.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_bmm_171 0.0076 ms 90.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.3085 seconds and 0.4602 seconds precompiling for 18 choices
[1,1]<stdout>:AUTOTUNE bmm(16x16x128, 16x128x512)
[1,1]<stdout>:strides: [192, 3072, 1], [65536, 1, 128]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  triton_bmm_164 0.0070 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
[1,1]<stdout>:  triton_bmm_168 0.0072 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_174 0.0073 ms 95.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_163 0.0073 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_bmm_172 0.0074 ms 94.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_175 0.0076 ms 92.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,1]<stdout>:  triton_bmm_170 0.0077 ms 90.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_171 0.0078 ms 89.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,1]<stdout>:  triton_bmm_169 0.0079 ms 88.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_bmm_177 0.0080 ms 87.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.3162 seconds and 0.4441 seconds precompiling for 18 choices
[1,1]<stdout>:AUTOTUNE bmm(16x16x128, 16x128x512)
[1,1]<stdout>:strides: [192, 3072, 1], [65536, 1, 128]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  triton_bmm_164 0.0070 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
[1,1]<stdout>:  triton_bmm_168 0.0071 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_163 0.0073 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_bmm_174 0.0074 ms 95.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_172 0.0074 ms 94.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_170 0.0076 ms 92.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  bmm 0.0076 ms 92.1% 
[1,1]<stdout>:  triton_bmm_175 0.0078 ms 90.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,1]<stdout>:  triton_bmm_171 0.0079 ms 89.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,1]<stdout>:  triton_bmm_169 0.0080 ms 88.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.2937 seconds and 0.3927 seconds precompiling for 18 choices
[1,1]<stdout>:AUTOTUNE bmm(16x16x128, 16x128x512)
[1,1]<stdout>:strides: [192, 3072, 1], [65536, 1, 128]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  triton_bmm_168 0.0073 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_172 0.0074 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_175 0.0074 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,1]<stdout>:  triton_bmm_174 0.0075 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_163 0.0076 ms 96.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_bmm_171 0.0076 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,1]<stdout>:  triton_bmm_170 0.0077 ms 94.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_177 0.0078 ms 93.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_bmm_169 0.0080 ms 91.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_bmm_173 0.0085 ms 86.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.3449 seconds and 0.4138 seconds precompiling for 18 choices
[1,1]<stdout>:AUTOTUNE bmm(16x16x128, 16x128x512)
[1,1]<stdout>:strides: [192, 3072, 1], [65536, 1, 128]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  triton_bmm_164 0.0068 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
[1,1]<stdout>:  triton_bmm_168 0.0070 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_172 0.0073 ms 93.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_174 0.0073 ms 93.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_163 0.0074 ms 92.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  bmm 0.0076 ms 90.7% 
[1,1]<stdout>:  triton_bmm_170 0.0076 ms 90.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_175 0.0076 ms 90.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,1]<stdout>:  triton_bmm_171 0.0076 ms 89.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,1]<stdout>:  triton_bmm_177 0.0077 ms 88.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.3259 seconds and 0.3757 seconds precompiling for 18 choices
[1,1]<stdout>:AUTOTUNE bmm(16x16x128, 16x128x512)
[1,1]<stdout>:strides: [192, 3072, 1], [65536, 1, 128]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  triton_bmm_163 0.0068 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_bmm_164 0.0072 ms 95.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
[1,1]<stdout>:  triton_bmm_168 0.0072 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_175 0.0072 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,1]<stdout>:  triton_bmm_171 0.0072 ms 94.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,1]<stdout>:  triton_bmm_177 0.0072 ms 94.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_bmm_169 0.0073 ms 93.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_bmm_172 0.0073 ms 93.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_174 0.0074 ms 91.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  bmm 0.0076 ms 89.1% 
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.2958 seconds and 0.3305 seconds precompiling for 18 choices
[1,0]<stdout>:AUTOTUNE bmm(16x16x128, 16x128x512)
[1,0]<stdout>:strides: [192, 3072, 1], [65536, 1, 128]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  triton_bmm_164 0.0069 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
[1,0]<stdout>:  triton_bmm_163 0.0071 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_bmm_168 0.0071 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_bmm_172 0.0072 ms 95.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_bmm_174 0.0073 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_bmm_175 0.0075 ms 91.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,0]<stdout>:  triton_bmm_170 0.0075 ms 91.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_bmm_177 0.0076 ms 90.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  bmm 0.0077 ms 89.6% 
[1,0]<stdout>:  triton_bmm_169 0.0078 ms 88.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.4111 seconds and 0.5795 seconds precompiling for 18 choices
[1,0]<stdout>:AUTOTUNE bmm(16x16x128, 16x128x512)
[1,0]<stdout>:strides: [192, 3072, 1], [65536, 1, 128]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  triton_bmm_164 0.0069 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
[1,0]<stdout>:  triton_bmm_168 0.0070 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_bmm_163 0.0071 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_bmm_174 0.0072 ms 96.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_bmm_175 0.0076 ms 90.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,0]<stdout>:  triton_bmm_177 0.0078 ms 88.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_bmm_165 0.0084 ms 82.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
[1,0]<stdout>:  triton_bmm_167 0.0087 ms 80.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,0]<stdout>:  triton_bmm_162 0.0088 ms 78.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=2
[1,0]<stdout>:  triton_bmm_176 0.0088 ms 78.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=8
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.3708 seconds and 0.6126 seconds precompiling for 18 choices
[1,1]<stdout>:AUTOTUNE bmm(16x16x128, 16x128x512)
[1,1]<stdout>:strides: [192, 3072, 1], [65536, 1, 128]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  triton_bmm_164 0.0069 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
[1,1]<stdout>:  triton_bmm_168 0.0070 ms 97.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_172 0.0072 ms 95.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_174 0.0073 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_163 0.0073 ms 94.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  bmm 0.0074 ms 92.7% 
[1,1]<stdout>:  triton_bmm_171 0.0075 ms 92.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,1]<stdout>:  triton_bmm_175 0.0075 ms 91.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,1]<stdout>:  triton_bmm_170 0.0076 ms 91.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_177 0.0076 ms 90.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.3195 seconds and 0.0019 seconds precompiling for 18 choices
[1,1]<stdout>:AUTOTUNE bmm(16x16x128, 16x128x512)
[1,1]<stdout>:strides: [192, 3072, 1], [65536, 1, 128]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  triton_bmm_164 0.0068 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
[1,1]<stdout>:  triton_bmm_168 0.0070 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_163 0.0072 ms 95.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_bmm_172 0.0072 ms 95.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_174 0.0072 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_175 0.0074 ms 93.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,1]<stdout>:  triton_bmm_170 0.0074 ms 92.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  bmm 0.0074 ms 92.2% 
[1,1]<stdout>:  triton_bmm_177 0.0077 ms 89.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_bmm_171 0.0078 ms 88.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.3466 seconds and 0.0021 seconds precompiling for 18 choices
[1,0]<stdout>:AUTOTUNE bmm(16x16x128, 16x128x512)
[1,0]<stdout>:strides: [192, 3072, 1], [65536, 1, 128]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  triton_bmm_163 0.0070 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_bmm_175 0.0072 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,0]<stdout>:  triton_bmm_164 0.0073 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
[1,0]<stdout>:  triton_bmm_168 0.0075 ms 94.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_bmm_177 0.0075 ms 93.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_bmm_169 0.0076 ms 93.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_bmm_171 0.0076 ms 93.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,0]<stdout>:  triton_bmm_174 0.0077 ms 90.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_bmm_170 0.0078 ms 89.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_bmm_172 0.0078 ms 89.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.2663 seconds and 0.7636 seconds precompiling for 18 choices
[1,0]<stdout>:AUTOTUNE bmm(16x16x128, 16x128x512)
[1,0]<stdout>:strides: [192, 3072, 1], [65536, 1, 128]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  triton_bmm_164 0.0068 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
[1,0]<stdout>:  triton_bmm_168 0.0070 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_bmm_163 0.0072 ms 95.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_bmm_174 0.0072 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_bmm_172 0.0073 ms 94.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_bmm_175 0.0074 ms 93.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,0]<stdout>:  triton_bmm_170 0.0075 ms 91.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  bmm 0.0076 ms 90.7% 
[1,0]<stdout>:  triton_bmm_177 0.0076 ms 90.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_bmm_171 0.0076 ms 89.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.2488 seconds and 0.8428 seconds precompiling for 18 choices
[1,0]<stdout>:AUTOTUNE bmm(16x16x128, 16x128x512)
[1,0]<stdout>:strides: [192, 3072, 1], [65536, 1, 128]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  triton_bmm_163 0.0070 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_bmm_175 0.0073 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,0]<stdout>:  triton_bmm_164 0.0073 ms 96.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
[1,0]<stdout>:  triton_bmm_168 0.0074 ms 94.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_bmm_177 0.0075 ms 94.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_bmm_171 0.0075 ms 93.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,0]<stdout>:  triton_bmm_172 0.0075 ms 93.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_bmm_174 0.0075 ms 93.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_bmm_169 0.0076 ms 93.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_bmm_170 0.0079 ms 89.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.2511 seconds and 0.8166 seconds precompiling for 18 choices
[1,0]<stdout>:AUTOTUNE bmm(16x16x128, 16x128x512)
[1,0]<stdout>:strides: [192, 3072, 1], [65536, 1, 128]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  triton_bmm_163 0.0070 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_bmm_164 0.0071 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
[1,0]<stdout>:  triton_bmm_175 0.0073 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,0]<stdout>:  triton_bmm_171 0.0073 ms 96.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,0]<stdout>:  triton_bmm_168 0.0074 ms 95.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_bmm_177 0.0074 ms 95.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_bmm_174 0.0074 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_bmm_169 0.0075 ms 94.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_bmm_172 0.0076 ms 93.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_bmm_170 0.0077 ms 91.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.2518 seconds and 0.8382 seconds precompiling for 18 choices
[1,0]<stdout>:AUTOTUNE bmm(16x16x128, 16x128x512)
[1,0]<stdout>:strides: [192, 3072, 1], [65536, 1, 128]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  triton_bmm_163 0.0073 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_bmm_164 0.0073 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
[1,0]<stdout>:  triton_bmm_175 0.0074 ms 98.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,0]<stdout>:  triton_bmm_177 0.0075 ms 96.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_bmm_171 0.0077 ms 94.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,0]<stdout>:  triton_bmm_172 0.0077 ms 94.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_bmm_169 0.0077 ms 94.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_bmm_168 0.0077 ms 93.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_bmm_174 0.0078 ms 93.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_bmm_170 0.0079 ms 92.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.2499 seconds and 0.7085 seconds precompiling for 18 choices
[1,0]<stdout>:AUTOTUNE bmm(16x16x128, 16x128x512)
[1,0]<stdout>:strides: [192, 3072, 1], [65536, 1, 128]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  triton_bmm_164 0.0069 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
[1,0]<stdout>:  triton_bmm_168 0.0069 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_bmm_163 0.0072 ms 96.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  bmm 0.0072 ms 95.1% 
[1,0]<stdout>:  triton_bmm_170 0.0074 ms 93.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_bmm_169 0.0077 ms 89.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_bmm_177 0.0078 ms 87.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_bmm_171 0.0080 ms 85.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,0]<stdout>:  triton_bmm_162 0.0082 ms 84.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=2
[1,0]<stdout>:  triton_bmm_165 0.0084 ms 82.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.2945 seconds and 0.8123 seconds precompiling for 18 choices
[1,0]<stdout>:[rank4]:W1011 22:08:21.383000 3698711 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/_inductor/codecache.py:1026] [30/3_1] fx graph cache unable to load compiled graph
[1,0]<stdout>:[rank4]:W1011 22:08:21.383000 3698711 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/_inductor/codecache.py:1026] [30/3_1] Traceback (most recent call last):
[1,0]<stdout>:[rank4]:W1011 22:08:21.383000 3698711 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/_inductor/codecache.py:1026] [30/3_1]   File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_inductor/codecache.py", line 1022, in iterate_over_candidates
[1,0]<stdout>:[rank4]:W1011 22:08:21.383000 3698711 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/_inductor/codecache.py:1026] [30/3_1]     with open(os.path.join(subdir, path), "rb") as f:
[1,0]<stdout>:[rank4]:W1011 22:08:21.383000 3698711 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/_inductor/codecache.py:1026] [30/3_1]          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:[rank4]:W1011 22:08:21.383000 3698711 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/_inductor/codecache.py:1026] [30/3_1] FileNotFoundError: [Errno 2] No such file or directory: '/home/users/industry/ai-hpc/apacsc34/scratch/.torchinductor_cache/fxgraph/du/fduoriqi47vkwesbl7jonlgrs7vhkiavvfuxzor5nhbqgxajui55/.3125269.140737350502208.tmp'
[1,0]<stdout>:AUTOTUNE mm(32x7168, 7168x256)
[1,0]<stdout>:strides: [7168, 1], [1, 7168]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  triton_mm_182 0.0253 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_186 0.0357 ms 70.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_190 0.0372 ms 68.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,0]<stdout>:  triton_mm_181 0.0418 ms 60.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_180 0.0475 ms 53.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_194 0.0501 ms 50.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_185 0.0501 ms 50.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,0]<stdout>:  triton_mm_179 0.0531 ms 47.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,0]<stdout>:  triton_mm_189 0.0548 ms 46.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_mm_192 0.0595 ms 42.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.4091 seconds and 1.1156 seconds precompiling for 18 choices
[1,0]<stdout>:AUTOTUNE mm(32x7168, 7168x256)
[1,0]<stdout>:strides: [7168, 1], [1, 7168]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  mm 0.0133 ms 100.0% 
[1,0]<stdout>:  triton_mm_182 0.0253 ms 52.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_186 0.0357 ms 37.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_190 0.0366 ms 36.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,0]<stdout>:  triton_mm_181 0.0416 ms 31.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_180 0.0473 ms 28.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_194 0.0498 ms 26.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_185 0.0500 ms 26.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,0]<stdout>:  triton_mm_179 0.0526 ms 25.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,0]<stdout>:  triton_mm_189 0.0540 ms 24.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.4154 seconds and 1.1105 seconds precompiling for 18 choices
[1,0]<stdout>:AUTOTUNE mm(32x7168, 7168x256)
[1,0]<stdout>:strides: [7168, 1], [1, 7168]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  mm 0.0138 ms 100.0% 
[1,0]<stdout>:  triton_mm_182 0.0250 ms 55.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_186 0.0356 ms 38.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_190 0.0371 ms 37.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,0]<stdout>:  triton_mm_181 0.0419 ms 32.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_180 0.0475 ms 29.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_194 0.0497 ms 27.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_185 0.0500 ms 27.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,0]<stdout>:  triton_mm_179 0.0531 ms 25.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,0]<stdout>:  triton_mm_189 0.0547 ms 25.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.3980 seconds and 1.1306 seconds precompiling for 18 choices
[1,1]<stdout>:AUTOTUNE mm(32x7168, 7168x256)
[1,1]<stdout>:strides: [7168, 1], [1, 7168]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  mm 0.0143 ms 100.0% 
[1,1]<stdout>:  triton_mm_182 0.0250 ms 57.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_186 0.0356 ms 40.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_190 0.0368 ms 38.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,1]<stdout>:  triton_mm_181 0.0420 ms 34.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_180 0.0473 ms 30.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_194 0.0499 ms 28.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_185 0.0500 ms 28.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,1]<stdout>:  triton_mm_179 0.0529 ms 27.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,1]<stdout>:  triton_mm_189 0.0546 ms 26.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.5122 seconds and 1.0402 seconds precompiling for 18 choices
[1,0]<stdout>:AUTOTUNE mm(32x7168, 7168x256)
[1,0]<stdout>:strides: [7168, 1], [1, 7168]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  mm 0.0131 ms 100.0% 
[1,0]<stdout>:  triton_mm_182 0.0246 ms 53.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_186 0.0352 ms 37.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_190 0.0366 ms 35.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,0]<stdout>:  triton_mm_181 0.0420 ms 31.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_180 0.0468 ms 28.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_194 0.0495 ms 26.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_185 0.0500 ms 26.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,0]<stdout>:  triton_mm_179 0.0528 ms 24.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,0]<stdout>:  triton_mm_189 0.0548 ms 23.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.3925 seconds and 1.1363 seconds precompiling for 18 choices
[1,1]<stdout>:AUTOTUNE mm(32x7168, 7168x256)
[1,1]<stdout>:strides: [7168, 1], [1, 7168]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  mm 0.0135 ms 100.0% 
[1,1]<stdout>:  triton_mm_182 0.0251 ms 53.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_186 0.0357 ms 37.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_190 0.0372 ms 36.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,1]<stdout>:  triton_mm_181 0.0421 ms 32.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_180 0.0476 ms 28.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_194 0.0497 ms 27.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_185 0.0503 ms 26.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,1]<stdout>:  triton_mm_179 0.0532 ms 25.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,1]<stdout>:  triton_mm_189 0.0546 ms 24.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.4882 seconds and 1.0527 seconds precompiling for 18 choices
[1,1]<stdout>:AUTOTUNE mm(32x7168, 7168x256)
[1,1]<stdout>:strides: [7168, 1], [1, 7168]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  mm 0.0133 ms 100.0% 
[1,1]<stdout>:  triton_mm_182 0.0250 ms 53.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_186 0.0354 ms 37.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_190 0.0368 ms 36.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,1]<stdout>:  triton_mm_181 0.0419 ms 31.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_180 0.0474 ms 28.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_194 0.0497 ms 26.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_185 0.0500 ms 26.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,1]<stdout>:  triton_mm_179 0.0530 ms 25.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,1]<stdout>:  triton_mm_189 0.0547 ms 24.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.4882 seconds and 1.0290 seconds precompiling for 18 choices
[1,1]<stdout>:AUTOTUNE mm(32x7168, 7168x256)
[1,1]<stdout>:strides: [7168, 1], [1, 7168]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  mm 0.0127 ms 100.0% 
[1,1]<stdout>:  triton_mm_182 0.0249 ms 51.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_186 0.0353 ms 36.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_190 0.0366 ms 34.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,1]<stdout>:  triton_mm_181 0.0421 ms 30.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_180 0.0471 ms 27.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_194 0.0495 ms 25.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_185 0.0504 ms 25.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,1]<stdout>:  triton_mm_179 0.0531 ms 23.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,1]<stdout>:  triton_mm_189 0.0548 ms 23.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.4849 seconds and 1.0675 seconds precompiling for 18 choices
[1,1]<stdout>:AUTOTUNE mm(32x7168, 7168x256)
[1,1]<stdout>:strides: [7168, 1], [1, 7168]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  mm 0.0134 ms 100.0% 
[1,1]<stdout>:  triton_mm_182 0.0253 ms 53.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_186 0.0349 ms 38.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_190 0.0369 ms 36.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,1]<stdout>:  triton_mm_181 0.0419 ms 32.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_180 0.0475 ms 28.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_185 0.0499 ms 26.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,1]<stdout>:  triton_mm_194 0.0500 ms 26.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_179 0.0535 ms 25.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,1]<stdout>:  triton_mm_189 0.0544 ms 24.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.4890 seconds and 1.0451 seconds precompiling for 18 choices
[1,1]<stdout>:AUTOTUNE mm(32x7168, 7168x256)
[1,1]<stdout>:strides: [7168, 1], [1, 7168]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  mm 0.0136 ms 100.0% 
[1,1]<stdout>:  triton_mm_182 0.0249 ms 54.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_186 0.0353 ms 38.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_190 0.0369 ms 36.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,1]<stdout>:  triton_mm_181 0.0419 ms 32.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_180 0.0474 ms 28.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_194 0.0496 ms 27.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_185 0.0502 ms 27.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,1]<stdout>:  triton_mm_179 0.0533 ms 25.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,1]<stdout>:  triton_mm_189 0.0547 ms 24.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.4946 seconds and 1.0466 seconds precompiling for 18 choices
[1,1]<stdout>:AUTOTUNE mm(32x7168, 7168x256)
[1,1]<stdout>:strides: [7168, 1], [1, 7168]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  mm 0.0130 ms 100.0% 
[1,1]<stdout>:  triton_mm_182 0.0251 ms 51.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_186 0.0356 ms 36.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_190 0.0370 ms 35.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,1]<stdout>:  triton_mm_181 0.0416 ms 31.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_180 0.0474 ms 27.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_194 0.0500 ms 25.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_185 0.0502 ms 25.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,1]<stdout>:  triton_mm_179 0.0528 ms 24.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,1]<stdout>:  triton_mm_189 0.0548 ms 23.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.4890 seconds and 1.0506 seconds precompiling for 18 choices
[1,1]<stdout>:AUTOTUNE mm(32x7168, 7168x256)
[1,1]<stdout>:strides: [7168, 1], [1, 7168]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  mm 0.0136 ms 100.0% 
[1,1]<stdout>:  triton_mm_182 0.0249 ms 54.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_186 0.0356 ms 38.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_190 0.0372 ms 36.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,1]<stdout>:  triton_mm_181 0.0419 ms 32.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_180 0.0477 ms 28.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_194 0.0502 ms 27.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_185 0.0503 ms 27.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,1]<stdout>:  triton_mm_179 0.0534 ms 25.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,1]<stdout>:  triton_mm_189 0.0549 ms 24.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.5030 seconds and 1.0565 seconds precompiling for 18 choices
[1,0]<stdout>:AUTOTUNE mm(32x7168, 7168x256)
[1,0]<stdout>:strides: [7168, 1], [1, 7168]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  mm 0.0132 ms 100.0% 
[1,0]<stdout>:  triton_mm_182 0.0248 ms 53.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_186 0.0353 ms 37.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_190 0.0367 ms 36.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,0]<stdout>:  triton_mm_181 0.0421 ms 31.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_180 0.0474 ms 27.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_194 0.0495 ms 26.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_185 0.0504 ms 26.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,0]<stdout>:  triton_mm_179 0.0536 ms 24.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,0]<stdout>:  triton_mm_189 0.0550 ms 24.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.4073 seconds and 1.1412 seconds precompiling for 18 choices
[1,0]<stdout>:AUTOTUNE mm(32x7168, 7168x256)
[1,0]<stdout>:strides: [7168, 1], [1, 7168]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  mm 0.0131 ms 100.0% 
[1,0]<stdout>:  triton_mm_182 0.0251 ms 52.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_186 0.0353 ms 37.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_190 0.0359 ms 36.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,0]<stdout>:  triton_mm_181 0.0422 ms 30.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_180 0.0472 ms 27.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_194 0.0490 ms 26.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_185 0.0504 ms 25.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,0]<stdout>:  triton_mm_179 0.0540 ms 24.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,0]<stdout>:  triton_mm_189 0.0545 ms 23.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.3882 seconds and 1.2485 seconds precompiling for 18 choices
[1,0]<stdout>:AUTOTUNE mm(32x7168, 7168x256)
[1,0]<stdout>:strides: [7168, 1], [1, 7168]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  mm 0.0131 ms 100.0% 
[1,0]<stdout>:  triton_mm_182 0.0250 ms 52.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_186 0.0355 ms 36.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_190 0.0369 ms 35.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,0]<stdout>:  triton_mm_181 0.0416 ms 31.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_180 0.0470 ms 27.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_194 0.0494 ms 26.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_185 0.0498 ms 26.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,0]<stdout>:  triton_mm_179 0.0524 ms 24.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,0]<stdout>:  triton_mm_189 0.0543 ms 24.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.3820 seconds and 1.1376 seconds precompiling for 18 choices
[1,0]<stdout>:AUTOTUNE mm(32x7168, 7168x256)
[1,0]<stdout>:strides: [7168, 1], [1, 7168]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  triton_mm_182 0.0265 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_186 0.0352 ms 75.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_190 0.0366 ms 72.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,0]<stdout>:  triton_mm_181 0.0419 ms 63.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_180 0.0473 ms 56.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_194 0.0496 ms 53.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_185 0.0503 ms 52.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,0]<stdout>:  triton_mm_179 0.0536 ms 49.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,0]<stdout>:  triton_mm_189 0.0548 ms 48.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_mm_192 0.0591 ms 44.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.4141 seconds and 1.1474 seconds precompiling for 18 choices
[1,1]<stdout>:AUTOTUNE mm(32x7168, 7168x8080)
[1,1]<stdout>:strides: [7168, 1], [1, 7168]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  triton_mm_199 0.0533 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_203 0.0534 ms 99.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  mm 0.0543 ms 98.2% 
[1,1]<stdout>:  triton_mm_207 0.0543 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,1]<stdout>:  triton_mm_211 0.0601 ms 88.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_196 0.0717 ms 74.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,1]<stdout>:  triton_mm_198 0.0729 ms 73.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_202 0.0733 ms 72.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,1]<stdout>:  triton_mm_197 0.0769 ms 69.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_206 0.0773 ms 69.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.5052 seconds and 0.7876 seconds precompiling for 18 choices
[1,1]<stdout>:AUTOTUNE mm(32x7168, 7168x8080)
[1,1]<stdout>:strides: [7168, 1], [1, 7168]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  mm 0.0533 ms 100.0% 
[1,1]<stdout>:  triton_mm_203 0.0534 ms 99.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_199 0.0536 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_207 0.0544 ms 97.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,1]<stdout>:  triton_mm_211 0.0600 ms 88.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_196 0.0716 ms 74.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,1]<stdout>:  triton_mm_198 0.0728 ms 73.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_202 0.0733 ms 72.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,1]<stdout>:  triton_mm_197 0.0769 ms 69.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_206 0.0771 ms 69.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.5128 seconds and 0.7692 seconds precompiling for 18 choices
[1,1]<stdout>:AUTOTUNE mm(32x7168, 7168x8080)
[1,1]<stdout>:strides: [7168, 1], [1, 7168]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  mm 0.0528 ms 100.0% 
[1,1]<stdout>:  triton_mm_199 0.0530 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_203 0.0532 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_207 0.0540 ms 97.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,1]<stdout>:  triton_mm_211 0.0597 ms 88.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_196 0.0714 ms 74.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,1]<stdout>:  triton_mm_198 0.0726 ms 72.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_202 0.0731 ms 72.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,1]<stdout>:  triton_mm_197 0.0767 ms 68.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_206 0.0768 ms 68.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.5299 seconds and 0.9206 seconds precompiling for 18 choices
[1,1]<stdout>:AUTOTUNE mm(32x7168, 7168x8080)
[1,1]<stdout>:strides: [7168, 1], [1, 7168]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  triton_mm_203 0.0532 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_199 0.0533 ms 99.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_207 0.0541 ms 98.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,1]<stdout>:  mm 0.0543 ms 97.9% 
[1,1]<stdout>:  triton_mm_211 0.0600 ms 88.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_196 0.0711 ms 74.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,1]<stdout>:  triton_mm_198 0.0728 ms 73.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_202 0.0730 ms 72.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,1]<stdout>:  triton_mm_197 0.0767 ms 69.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_206 0.0770 ms 69.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.5312 seconds and 0.8751 seconds precompiling for 18 choices
[1,1]<stdout>:AUTOTUNE mm(32x7168, 7168x8080)
[1,1]<stdout>:strides: [7168, 1], [1, 7168]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  triton_mm_199 0.0531 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_203 0.0531 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_207 0.0540 ms 98.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,1]<stdout>:  triton_mm_211 0.0600 ms 88.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  mm 0.0630 ms 84.3% 
[1,1]<stdout>:  triton_mm_196 0.0716 ms 74.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,1]<stdout>:  triton_mm_198 0.0727 ms 73.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_202 0.0731 ms 72.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,1]<stdout>:  triton_mm_197 0.0765 ms 69.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_206 0.0767 ms 69.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.5359 seconds and 0.8955 seconds precompiling for 18 choices
[1,1]<stdout>:AUTOTUNE mm(32x7168, 7168x8080)
[1,1]<stdout>:strides: [7168, 1], [1, 7168]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  triton_mm_203 0.0531 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_199 0.0532 ms 99.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  mm 0.0538 ms 98.8% 
[1,1]<stdout>:  triton_mm_207 0.0541 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,1]<stdout>:  triton_mm_211 0.0599 ms 88.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_196 0.0709 ms 74.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,1]<stdout>:  triton_mm_198 0.0730 ms 72.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_202 0.0733 ms 72.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,1]<stdout>:  triton_mm_197 0.0768 ms 69.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_206 0.0771 ms 68.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.5316 seconds and 0.8756 seconds precompiling for 18 choices
[1,1]<stdout>:AUTOTUNE mm(32x7168, 7168x8080)
[1,1]<stdout>:strides: [7168, 1], [1, 7168]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  triton_mm_199 0.0533 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_203 0.0533 ms 99.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_207 0.0542 ms 98.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,1]<stdout>:  mm 0.0543 ms 98.2% 
[1,1]<stdout>:  triton_mm_211 0.0601 ms 88.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_196 0.0714 ms 74.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,1]<stdout>:  triton_mm_198 0.0729 ms 73.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_202 0.0737 ms 72.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,1]<stdout>:  triton_mm_197 0.0771 ms 69.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_206 0.0773 ms 68.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.5355 seconds and 0.8974 seconds precompiling for 18 choices
[1,0]<stdout>:AUTOTUNE mm(32x7168, 7168x8080)
[1,0]<stdout>:strides: [7168, 1], [1, 7168]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  triton_mm_199 0.0530 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_203 0.0532 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_207 0.0539 ms 98.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,0]<stdout>:  triton_mm_211 0.0595 ms 89.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_196 0.0718 ms 73.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,0]<stdout>:  triton_mm_198 0.0728 ms 72.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_202 0.0735 ms 72.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,0]<stdout>:  triton_mm_197 0.0763 ms 69.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_206 0.0772 ms 68.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  mm 0.0839 ms 63.1% 
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.4462 seconds and 1.0826 seconds precompiling for 18 choices
[1,0]<stdout>:AUTOTUNE mm(32x7168, 7168x8080)
[1,0]<stdout>:strides: [7168, 1], [1, 7168]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  triton_mm_199 0.0530 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_203 0.0532 ms 99.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_207 0.0539 ms 98.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,0]<stdout>:  mm 0.0545 ms 97.3% 
[1,0]<stdout>:  triton_mm_211 0.0597 ms 88.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_196 0.0716 ms 74.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,0]<stdout>:  triton_mm_198 0.0727 ms 72.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_202 0.0735 ms 72.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,0]<stdout>:  triton_mm_197 0.0760 ms 69.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_206 0.0772 ms 68.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.4264 seconds and 1.2294 seconds precompiling for 18 choices
[1,0]<stdout>:AUTOTUNE mm(32x7168, 7168x8080)
[1,0]<stdout>:strides: [7168, 1], [1, 7168]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  triton_mm_199 0.0528 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_203 0.0530 ms 99.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  mm 0.0534 ms 98.9% 
[1,0]<stdout>:  triton_mm_207 0.0539 ms 98.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,0]<stdout>:  triton_mm_211 0.0596 ms 88.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_196 0.0713 ms 74.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,0]<stdout>:  triton_mm_198 0.0726 ms 72.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_202 0.0733 ms 72.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,0]<stdout>:  triton_mm_206 0.0764 ms 69.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_mm_197 0.0764 ms 69.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.4294 seconds and 1.3472 seconds precompiling for 18 choices
[1,0]<stdout>:AUTOTUNE mm(32x7168, 7168x8080)
[1,0]<stdout>:strides: [7168, 1], [1, 7168]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  triton_mm_203 0.0528 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_199 0.0529 ms 99.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_207 0.0540 ms 97.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,0]<stdout>:  mm 0.0541 ms 97.6% 
[1,0]<stdout>:  triton_mm_211 0.0593 ms 89.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_196 0.0714 ms 74.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,0]<stdout>:  triton_mm_198 0.0727 ms 72.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_202 0.0733 ms 72.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,0]<stdout>:  triton_mm_197 0.0768 ms 68.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_206 0.0771 ms 68.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.4409 seconds and 1.3325 seconds precompiling for 18 choices
[1,0]<stdout>:AUTOTUNE mm(32x7168, 7168x8080)
[1,0]<stdout>:strides: [7168, 1], [1, 7168]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  triton_mm_203 0.0529 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  mm 0.0530 ms 99.8% 
[1,0]<stdout>:  triton_mm_199 0.0531 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_207 0.0539 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,0]<stdout>:  triton_mm_211 0.0596 ms 88.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_196 0.0714 ms 74.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,0]<stdout>:  triton_mm_198 0.0729 ms 72.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_202 0.0732 ms 72.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,0]<stdout>:  triton_mm_197 0.0766 ms 69.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_206 0.0769 ms 68.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.4307 seconds and 1.3394 seconds precompiling for 18 choices
[1,0]<stdout>:AUTOTUNE mm(32x7168, 7168x8080)
[1,0]<stdout>:strides: [7168, 1], [1, 7168]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  triton_mm_203 0.0532 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_199 0.0533 ms 99.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_207 0.0537 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,0]<stdout>:  triton_mm_211 0.0594 ms 89.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_196 0.0716 ms 74.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,0]<stdout>:  triton_mm_198 0.0723 ms 73.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_202 0.0728 ms 73.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,0]<stdout>:  triton_mm_206 0.0763 ms 69.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_mm_197 0.0764 ms 69.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_209 0.0880 ms 60.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.4567 seconds and 1.3211 seconds precompiling for 18 choices
[1,1]<stdout>:AUTOTUNE mm(32x7168, 7168x8080)
[1,1]<stdout>:strides: [7168, 1], [1, 7168]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  triton_mm_199 0.0527 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_203 0.0528 ms 99.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_207 0.0535 ms 98.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,1]<stdout>:  mm 0.0540 ms 97.5% 
[1,1]<stdout>:  triton_mm_211 0.0592 ms 89.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_196 0.0717 ms 73.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,1]<stdout>:  triton_mm_198 0.0731 ms 72.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,1]<stdout>:  triton_mm_202 0.0735 ms 71.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,1]<stdout>:  triton_mm_197 0.0764 ms 69.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_mm_206 0.0772 ms 68.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 1.9279 seconds and 0.6507 seconds precompiling for 18 choices
[1,0]<stdout>:AUTOTUNE mm(32x7168, 7168x8080)
[1,0]<stdout>:strides: [7168, 1], [1, 7168]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  triton_mm_203 0.0532 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_199 0.0532 ms 99.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  mm 0.0536 ms 99.2% 
[1,0]<stdout>:  triton_mm_207 0.0541 ms 98.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,0]<stdout>:  triton_mm_211 0.0599 ms 88.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_196 0.0714 ms 74.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,0]<stdout>:  triton_mm_198 0.0725 ms 73.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
[1,0]<stdout>:  triton_mm_202 0.0731 ms 72.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,0]<stdout>:  triton_mm_197 0.0765 ms 69.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_mm_206 0.0769 ms 69.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 1.5825 seconds and 0.8542 seconds precompiling for 18 choices
[1,1]<stdout>:[rank14]:W1011 22:08:52.099000 3125271 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/_inductor/codecache.py:1026] [70/3] fx graph cache unable to load compiled graph
[1,1]<stdout>:[rank14]:W1011 22:08:52.099000 3125271 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/_inductor/codecache.py:1026] [70/3] Traceback (most recent call last):
[1,1]<stdout>:[rank14]:W1011 22:08:52.099000 3125271 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/_inductor/codecache.py:1026] [70/3]   File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_inductor/codecache.py", line 1022, in iterate_over_candidates
[1,1]<stdout>:[rank14]:W1011 22:08:52.099000 3125271 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/_inductor/codecache.py:1026] [70/3]     with open(os.path.join(subdir, path), "rb") as f:
[1,1]<stdout>:[rank14]:W1011 22:08:52.099000 3125271 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/_inductor/codecache.py:1026] [70/3]          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:[rank14]:W1011 22:08:52.099000 3125271 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/_inductor/codecache.py:1026] [70/3] FileNotFoundError: [Errno 2] No such file or directory: '/home/users/industry/ai-hpc/apacsc34/scratch/.torchinductor_cache/fxgraph/yn/fyncndfjzfubz7ygzaff3icfyx4csdco7rtnd4uddmzag7u6vkb3/.3698713.140737350502208.tmp'
[1,0]<stdout>:Capturing batches (bs=8 avail_mem=19.95 GB): 100% 8/8 [03:13<00:00, 35.72s/it]Capturing batches (bs=8 avail_mem=19.95 GB): 100% 8/8 [03:13<00:00, 24.19s/it]
[1,0]<stdout>:[2025-10-11 22:08:54 DP0 TP0] Capture cuda graph end. Time elapsed: 195.05 s. mem usage=2.16 GB. avail mem=19.88 GB.
[1,0]<stdout>:[2025-10-11 22:08:54 DP0 TP0] MLA optimization is turned on. Use flashinfer backend.
[1,0]<stdout>:[2025-10-11 22:08:54 DP0 TP0] Chunked prefix cache is turned on.
[1,0]<stdout>:[2025-10-11 22:08:54 DP0 TP0] Init torch distributed begin.
[1,0]<stdout>:[2025-10-11 22:08:54 DP0 TP0] Init torch distributed ends. mem usage=0.00 GB
[1,0]<stdout>:[2025-10-11 22:08:54 DP0 TP0] Load weight begin. avail mem=19.88 GB
[1,0]<stdout>:[2025-10-11 22:08:54 DP0 TP0] Detected fp8 checkpoint.
[1,0]<stdout>:[2025-10-11 22:08:55 DP0 TP0] Load weight end. type=DeepseekV3ForCausalLMNextN, dtype=torch.bfloat16, avail mem=17.14 GB, mem usage=2.73 GB.
[1,1]<stdout>:[2025-10-11 22:08:55 DP1 TP10] KV Cache is allocated. #tokens: 170034, KV size: 0.18 GB
[1,1]<stdout>:[2025-10-11 22:08:55 DP1 TP8] KV Cache is allocated. #tokens: 170034, KV size: 0.18 GB
[1,1]<stdout>:[2025-10-11 22:08:55 DP1 TP9] KV Cache is allocated. #tokens: 170034, KV size: 0.18 GB
[1,1]<stdout>:[2025-10-11 22:08:55 DP1 TP13] KV Cache is allocated. #tokens: 170034, KV size: 0.18 GB
[1,1]<stdout>:[2025-10-11 22:08:55 DP1 TP14] KV Cache is allocated. #tokens: 170034, KV size: 0.18 GB
[1,1]<stdout>:[2025-10-11 22:08:55 DP1 TP15] KV Cache is allocated. #tokens: 170034, KV size: 0.18 GB
[1,1]<stdout>:[2025-10-11 22:08:55 DP1 TP11] KV Cache is allocated. #tokens: 170034, KV size: 0.18 GB
[1,1]<stdout>:[2025-10-11 22:08:55 DP1 TP12] KV Cache is allocated. #tokens: 170034, KV size: 0.18 GB
[1,0]<stdout>:[2025-10-11 22:08:55 DP0 TP4] KV Cache is allocated. #tokens: 170034, KV size: 0.18 GB
[1,0]<stdout>:[2025-10-11 22:08:55 DP0 TP1] KV Cache is allocated. #tokens: 170034, KV size: 0.18 GB
[1,0]<stdout>:[2025-10-11 22:08:55 DP0 TP7] KV Cache is allocated. #tokens: 170034, KV size: 0.18 GB
[1,0]<stdout>:[2025-10-11 22:08:55 DP0 TP5] KV Cache is allocated. #tokens: 170034, KV size: 0.18 GB
[1,0]<stdout>:[2025-10-11 22:08:55 DP0 TP3] KV Cache is allocated. #tokens: 170034, KV size: 0.18 GB
[1,0]<stdout>:[2025-10-11 22:08:55 DP0 TP2] KV Cache is allocated. #tokens: 170034, KV size: 0.18 GB
[1,0]<stdout>:[2025-10-11 22:08:55 DP0 TP0] KV Cache is allocated. #tokens: 170034, KV size: 0.18 GB
[1,0]<stdout>:[2025-10-11 22:08:55 DP0 TP0] Memory pool end. avail mem=16.97 GB
[1,0]<stdout>:[2025-10-11 22:08:55 DP0 TP6] KV Cache is allocated. #tokens: 170034, KV size: 0.18 GB
[1,1]<stdout>:[2025-10-11 22:08:56 DP1 TP15] Capture draft cuda graph begin. This can take up to several minutes. avail mem=19.88 GB
[1,1]<stdout>:[2025-10-11 22:08:56 DP1 TP9] Capture draft cuda graph begin. This can take up to several minutes. avail mem=18.64 GB
[1,1]<stdout>:[2025-10-11 22:08:56 DP1 TP11] Capture draft cuda graph begin. This can take up to several minutes. avail mem=18.62 GB
[1,1]<stdout>:[2025-10-11 22:08:56 DP1 TP8] Capture draft cuda graph begin. This can take up to several minutes. avail mem=18.71 GB
[1,1]<stdout>:[2025-10-11 22:08:56 DP1 TP12] Capture draft cuda graph begin. This can take up to several minutes. avail mem=18.64 GB
[1,0]<stdout>:[2025-10-11 22:08:56 DP0 TP4] Capture draft cuda graph begin. This can take up to several minutes. avail mem=18.64 GB
[1,1]<stdout>:[2025-10-11 22:08:56 DP1 TP10] Capture draft cuda graph begin. This can take up to several minutes. avail mem=19.67 GB
[1,0]<stdout>:[2025-10-11 22:08:56 DP0 TP5] Capture draft cuda graph begin. This can take up to several minutes. avail mem=18.72 GB
[1,0]<stdout>:[2025-10-11 22:08:56 DP0 TP1] Capture draft cuda graph begin. This can take up to several minutes. avail mem=18.64 GB
[1,0]<stdout>:[2025-10-11 22:08:56 DP0 TP3] Capture draft cuda graph begin. This can take up to several minutes. avail mem=18.62 GB
[1,0]<stdout>:[2025-10-11 22:08:56 DP0 TP2] Capture draft cuda graph begin. This can take up to several minutes. avail mem=19.67 GB
[1,0]<stdout>:[2025-10-11 22:08:56 DP0 TP0] Capture draft cuda graph begin. This can take up to several minutes. avail mem=18.71 GB
[1,1]<stdout>:[2025-10-11 22:08:56 DP1 TP14] Capture draft cuda graph begin. This can take up to several minutes. avail mem=18.75 GB
[1,1]<stdout>:[2025-10-11 22:08:56 DP1 TP13] Capture draft cuda graph begin. This can take up to several minutes. avail mem=18.72 GB
[1,0]<stdout>:[2025-10-11 22:08:56 DP0 TP7] Capture draft cuda graph begin. This can take up to several minutes. avail mem=19.88 GB
[1,0]<stdout>:[2025-10-11 22:08:56 DP0 TP6] Capture draft cuda graph begin. This can take up to several minutes. avail mem=18.75 GB
[1,0]<stdout>:  0% 0/8 [00:00<?, ?it/s][1,0]<stdout>:Capturing batches (bs=64 avail_mem=18.67 GB):   0% 0/8 [00:00<?, ?it/s][1,0]<stdout>:Capturing batches (bs=64 avail_mem=18.67 GB):  12% 1/8 [00:02<00:15,  2.28s/it][1,0]<stdout>:Capturing batches (bs=56 avail_mem=18.67 GB):  12% 1/8 [00:02<00:15,  2.28s/it][1,0]<stdout>:Capturing batches (bs=56 avail_mem=18.67 GB):  25% 2/8 [00:03<00:09,  1.54s/it]Capturing batches (bs=48 avail_mem=18.67 GB):  25% 2/8 [00:03<00:09,  1.54s/it][1,0]<stdout>:Capturing batches (bs=40 avail_mem=18.67 GB):  25% 2/8 [00:03<00:09,  1.54s/it][1,0]<stdout>:Capturing batches (bs=40 avail_mem=18.67 GB):  50% 4/8 [00:03<00:02,  1.64it/s]Capturing batches (bs=32 avail_mem=18.67 GB):  50% 4/8 [00:03<00:02,  1.64it/s][1,0]<stdout>:Capturing batches (bs=32 avail_mem=18.67 GB):  62% 5/8 [00:03<00:01,  1.99it/s][1,0]<stdout>:Capturing batches (bs=24 avail_mem=18.67 GB):  62% 5/8 [00:03<00:01,  1.99it/s][1,0]<stdout>:Capturing batches (bs=24 avail_mem=18.67 GB):  75% 6/8 [00:03<00:00,  2.58it/s][1,0]<stdout>:Capturing batches (bs=16 avail_mem=18.67 GB):  75% 6/8 [00:03<00:00,  2.58it/s][1,0]<stdout>:Capturing batches (bs=16 avail_mem=18.67 GB):  88% 7/8 [00:04<00:00,  2.86it/s][1,0]<stdout>:Capturing batches (bs=8 avail_mem=18.67 GB):  88% 7/8 [00:04<00:00,  2.86it/s] [1,0]<stdout>:[2025-10-11 22:09:02 DP0 TP5] Capture draft cuda graph end. Time elapsed: 6.56 s. mem usage=0.04 GB. avail mem=18.68 GB.
[1,0]<stdout>:[2025-10-11 22:09:02 DP0 TP5] Capture draft extend cuda graph begin. This can take up to several minutes. avail mem=18.68 GB
[1,0]<stdout>:[2025-10-11 22:09:02 DP0 TP4] Capture draft cuda graph end. Time elapsed: 6.57 s. mem usage=0.04 GB. avail mem=18.60 GB.
[1,1]<stdout>:[2025-10-11 22:09:02 DP1 TP10] Capture draft cuda graph end. Time elapsed: 6.57 s. mem usage=0.04 GB. avail mem=19.62 GB.
[1,0]<stdout>:[2025-10-11 22:09:02 DP0 TP4] Capture draft extend cuda graph begin. This can take up to several minutes. avail mem=18.60 GB
[1,0]<stdout>:[2025-10-11 22:09:02 DP0 TP7] Capture draft cuda graph end. Time elapsed: 6.45 s. mem usage=0.04 GB. avail mem=19.84 GB.
[1,1]<stdout>:[2025-10-11 22:09:02 DP1 TP10] Capture draft extend cuda graph begin. This can take up to several minutes. avail mem=19.62 GB
[1,0]<stdout>:[2025-10-11 22:09:02 DP0 TP7] Capture draft extend cuda graph begin. This can take up to several minutes. avail mem=19.84 GB
[1,0]<stdout>:[2025-10-11 22:09:02 DP0 TP6] Capture draft cuda graph end. Time elapsed: 6.45 s. mem usage=0.04 GB. avail mem=18.71 GB.
[1,0]<stdout>:[2025-10-11 22:09:02 DP0 TP6] Capture draft extend cuda graph begin. This can take up to several minutes. avail mem=18.71 GB
[1,1]<stdout>:[2025-10-11 22:09:02 DP1 TP11] Capture draft cuda graph end. Time elapsed: 6.58 s. mem usage=0.04 GB. avail mem=18.58 GB.
[1,1]<stdout>:[2025-10-11 22:09:02 DP1 TP11] Capture draft extend cuda graph begin. This can take up to several minutes. avail mem=18.58 GB
[1,0]<stdout>:[2025-10-11 22:09:02 DP0 TP1] Capture draft cuda graph end. Time elapsed: 6.56 s. mem usage=0.04 GB. avail mem=18.60 GB.
[1,0]<stdout>:[2025-10-11 22:09:02 DP0 TP1] Capture draft extend cuda graph begin. This can take up to several minutes. avail mem=18.60 GB
[1,1]<stdout>:[2025-10-11 22:09:02 DP1 TP9] Capture draft cuda graph end. Time elapsed: 6.59 s. mem usage=0.04 GB. avail mem=18.60 GB.
[1,0]<stdout>:[2025-10-11 22:09:02 DP0 TP2] Capture draft cuda graph end. Time elapsed: 6.55 s. mem usage=0.04 GB. avail mem=19.62 GB.
[1,1]<stdout>:[2025-10-11 22:09:02 DP1 TP9] Capture draft extend cuda graph begin. This can take up to several minutes. avail mem=18.60 GB
[1,0]<stdout>:[2025-10-11 22:09:02 DP0 TP2] Capture draft extend cuda graph begin. This can take up to several minutes. avail mem=19.62 GB
[1,0]<stdout>:[2025-10-11 22:09:02 DP0 TP3] Capture draft cuda graph end. Time elapsed: 6.55 s. mem usage=0.04 GB. avail mem=18.58 GB.
[1,0]<stdout>:[2025-10-11 22:09:02 DP0 TP3] Capture draft extend cuda graph begin. This can take up to several minutes. avail mem=18.58 GB
[1,1]<stdout>:[2025-10-11 22:09:02 DP1 TP15] Capture draft cuda graph end. Time elapsed: 6.60 s. mem usage=0.04 GB. avail mem=19.84 GB.
[1,1]<stdout>:[2025-10-11 22:09:02 DP1 TP13] Capture draft cuda graph end. Time elapsed: 6.46 s. mem usage=0.04 GB. avail mem=18.68 GB.
[1,1]<stdout>:[2025-10-11 22:09:02 DP1 TP15] Capture draft extend cuda graph begin. This can take up to several minutes. avail mem=19.84 GB
[1,1]<stdout>:[2025-10-11 22:09:02 DP1 TP13] Capture draft extend cuda graph begin. This can take up to several minutes. avail mem=18.68 GB
[1,1]<stdout>:[2025-10-11 22:09:02 DP1 TP8] Capture draft cuda graph end. Time elapsed: 6.58 s. mem usage=0.04 GB. avail mem=18.67 GB.
[1,1]<stdout>:[2025-10-11 22:09:02 DP1 TP8] Capture draft extend cuda graph begin. This can take up to several minutes. avail mem=18.67 GB
[1,1]<stdout>:[2025-10-11 22:09:02 DP1 TP14] Capture draft cuda graph end. Time elapsed: 6.53 s. mem usage=0.04 GB. avail mem=18.71 GB.
[1,1]<stdout>:[2025-10-11 22:09:02 DP1 TP14] Capture draft extend cuda graph begin. This can take up to several minutes. avail mem=18.71 GB
[1,1]<stdout>:[2025-10-11 22:09:02 DP1 TP12] Capture draft cuda graph end. Time elapsed: 6.58 s. mem usage=0.04 GB. avail mem=18.59 GB.
[1,1]<stdout>:[2025-10-11 22:09:02 DP1 TP12] Capture draft extend cuda graph begin. This can take up to several minutes. avail mem=18.59 GB
[1,0]<stdout>:Capturing batches (bs=8 avail_mem=18.67 GB): 100% 8/8 [00:04<00:00,  2.85it/s][1,0]<stdout>:Capturing batches (bs=8 avail_mem=18.67 GB): 100% 8/8 [00:04<00:00,  1.81it/s]
[1,0]<stdout>:[2025-10-11 22:09:02 DP0 TP0] Capture draft cuda graph end. Time elapsed: 6.53 s. mem usage=0.04 GB. avail mem=18.66 GB.
[1,0]<stdout>:[2025-10-11 22:09:02 DP0 TP0] Capture draft extend cuda graph begin. This can take up to several minutes. avail mem=18.66 GB
[1,0]<stdout>:  0% 0/8 [00:00<?, ?it/s]Capturing batches (bs=64 avail_mem=18.59 GB):   0% 0/8 [00:00<?, ?it/s][1,0]<stdout>:Capturing batches (bs=64 avail_mem=18.59 GB):  12% 1/8 [00:00<00:03,  2.05it/s][1,0]<stdout>:Capturing batches (bs=56 avail_mem=18.38 GB):  12% 1/8 [00:00<00:03,  2.05it/s][1,0]<stdout>:Capturing batches (bs=48 avail_mem=18.38 GB):  12% 1/8 [00:00<00:03,  2.05it/s][1,0]<stdout>:Capturing batches (bs=48 avail_mem=18.38 GB):  38% 3/8 [00:00<00:00,  5.96it/s][1,0]<stdout>:Capturing batches (bs=40 avail_mem=18.38 GB):  38% 3/8 [00:00<00:00,  5.96it/s][1,0]<stdout>:Capturing batches (bs=32 avail_mem=18.36 GB):  38% 3/8 [00:00<00:00,  5.96it/s][1,0]<stdout>:Capturing batches (bs=32 avail_mem=18.36 GB):  62% 5/8 [00:01<00:00,  5.08it/s][1,0]<stdout>:Capturing batches (bs=24 avail_mem=18.35 GB):  62% 5/8 [00:01<00:00,  5.08it/s][1,0]<stdout>:Capturing batches (bs=24 avail_mem=18.35 GB):  75% 6/8 [00:01<00:00,  5.60it/s][1,0]<stdout>:Capturing batches (bs=16 avail_mem=18.33 GB):  75% 6/8 [00:01<00:00,  5.60it/s][1,0]<stdout>:Capturing batches (bs=8 avail_mem=18.33 GB):  75% 6/8 [00:01<00:00,  5.60it/s] [1,0]<stdout>:[2025-10-11 22:09:06 DP0 TP4] Capture draft extend cuda graph end. Time elapsed: 3.32 s. mem usage=0.34 GB. avail mem=18.26 GB.
[1,0]<stdout>:[2025-10-11 22:09:06 DP0 TP6] Capture draft extend cuda graph end. Time elapsed: 3.32 s. mem usage=0.34 GB. avail mem=18.37 GB.
[1,0]<stdout>:[2025-10-11 22:09:06 DP0 TP1] Capture draft extend cuda graph end. Time elapsed: 3.32 s. mem usage=0.34 GB. avail mem=18.27 GB.
[1,1]<stdout>:[2025-10-11 22:09:06 DP1 TP8] Capture draft extend cuda graph end. Time elapsed: 3.32 s. mem usage=0.34 GB. avail mem=18.33 GB.
[1,1]<stdout>:[2025-10-11 22:09:06 DP1 TP9] Capture draft extend cuda graph end. Time elapsed: 3.32 s. mem usage=0.34 GB. avail mem=18.26 GB.
[1,1]<stdout>:[2025-10-11 22:09:06 DP1 TP11] Capture draft extend cuda graph end. Time elapsed: 3.32 s. mem usage=0.34 GB. avail mem=18.25 GB.
[1,1]<stdout>:[2025-10-11 22:09:06 DP1 TP13] Capture draft extend cuda graph end. Time elapsed: 3.32 s. mem usage=0.34 GB. avail mem=18.34 GB.
[1,0]<stdout>:Capturing batches (bs=8 avail_mem=18.33 GB): 100% 8/8 [00:01<00:00,  6.84it/s]Capturing batches (bs=8 avail_mem=18.33 GB): 100% 8/8 [00:01<00:00,  5.78it/s]
[1,0]<stdout>:[2025-10-11 22:09:06 DP0 TP3] Capture draft extend cuda graph end. Time elapsed: 3.32 s. mem usage=0.34 GB. avail mem=18.25 GB.
[1,0]<stdout>:[2025-10-11 22:09:06 DP0 TP0] Capture draft extend cuda graph end. Time elapsed: 3.32 s. mem usage=0.34 GB. avail mem=18.33 GB.
[1,0]<stdout>:[2025-10-11 22:09:06 DP0 TP5] Capture draft extend cuda graph end. Time elapsed: 3.32 s. mem usage=0.34 GB. avail mem=18.34 GB.
[1,0]<stdout>:[2025-10-11 22:09:06 DP0 TP0] max_total_num_tokens=170034, chunked_prefill_size=2048, max_prefill_tokens=16384, max_running_requests=32, context_len=163840, available_gpu_mem=18.33 GB
[1,0]<stdout>:[2025-10-11 22:09:06 DP0 TP2] Capture draft extend cuda graph end. Time elapsed: 3.32 s. mem usage=0.32 GB. avail mem=19.30 GB.
[1,0]<stdout>:[2025-10-11 22:09:06 DP0 TP7] Capture draft extend cuda graph end. Time elapsed: 3.33 s. mem usage=0.32 GB. avail mem=19.52 GB.
[1,1]<stdout>:[2025-10-11 22:09:06 DP1 TP10] Capture draft extend cuda graph end. Time elapsed: 3.33 s. mem usage=0.32 GB. avail mem=19.30 GB.
[1,1]<stdout>:[2025-10-11 22:09:06 DP1 TP14] Capture draft extend cuda graph end. Time elapsed: 3.33 s. mem usage=0.34 GB. avail mem=18.37 GB.
[1,1]<stdout>:[2025-10-11 22:09:06 DP1 TP12] Capture draft extend cuda graph end. Time elapsed: 3.33 s. mem usage=0.34 GB. avail mem=18.26 GB.
[1,1]<stdout>:[2025-10-11 22:09:06 DP1 TP15] Capture draft extend cuda graph end. Time elapsed: 3.35 s. mem usage=0.32 GB. avail mem=19.52 GB.
[1,1]<stdout>:[2025-10-11 22:09:06] Starting dummy health check server at 127.0.0.1:30000
[1,0]<stdout>:#Input tokens: 626729
[1,0]<stdout>:#Output tokens: 388685
[1,0]<stdout>:#Input tokens: 4096
[1,0]<stdout>:#Output tokens: 256
[1,0]<stdout>:[2025-10-11 22:09:20] 
[1,0]<stdout>:Warmup...
[1,0]<stdout>:[2025-10-11 22:09:20 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 257, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[1,0]<stdout>:[2025-10-11 22:09:20 DP0 TP4] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=4096, K=512, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-11 22:09:20 DP0 TP5] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=4096, K=512, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-11 22:09:20 DP0 TP7] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=4096, K=512, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-11 22:09:20 DP0 TP6] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=4096, K=512, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-11 22:09:20 DP0 TP3] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=4096, K=512, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-11 22:09:20 DP0 TP2] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=4096, K=512, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-11 22:09:20 DP0 TP1] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=4096, K=512, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-11 22:09:20 DP0 TP0] Entering DeepGEMM JIT Pre-Compile session. It may takes a long time (typically 10-20 mins) if you have not run `sglang.compile_deep_gemm`. It is recommended to run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to reduce the overhead if you have not run it before. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[1,0]<stdout>:[2025-10-11 22:09:20 DP0 TP0] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=4096, K=512, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:  0% 0/24 [00:00<?, ?it/s][1,0]<stdout>:100% 24/24 [00:00<00:00, 7314.05it/s]
[1,0]<stdout>:  0% 0/24 [00:00<?, ?it/s][1,0]<stdout>:100% 24/24 [00:00<00:00, 16408.04it/s][1,0]<stdout>:
[1,0]<stdout>:  0% 0/24 [00:00<?, ?it/s][1,0]<stdout>:100% 24/24 [00:00<00:00, 10253.98it/s][1,0]<stdout>:
[1,0]<stdout>:  0% 0/24 [00:00<?, ?it/s][1,0]<stdout>:100% 24/24 [00:00<00:00, 12681.19it/s]
[1,0]<stdout>:  0% 0/24 [00:00<?, ?it/s][1,0]<stdout>:100% 24/24 [00:00<00:00, 10366.97it/s]
[1,0]<stdout>:  0% 0/24 [00:00<?, ?it/s][1,0]<stdout>:100% 24/24 [00:00<00:00, 14183.92it/s]
[1,0]<stdout>:  0% 0/24 [00:00<?, ?it/s][1,0]<stdout>:100% 24/24 [00:00<00:00, 11657.59it/s]
[1,0]<stdout>:  0% 0/24 [00:00<?, ?it/s][1,0]<stdout>:100% 24/24 [00:00<00:00, 13107.20it/s]
[1,0]<stdout>:[2025-10-11 22:09:21 DP0 TP0] Prefill batch. #new-seq: 7, #new-token: 1785, #cached-token: 14, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[1,1]<stdout>:[2025-10-11 22:09:21 DP1 TP8] Prefill batch. #new-seq: 8, #new-token: 2048, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[1,1]<stdout>:[2025-10-11 22:09:21 DP1 TP10] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=4096, K=512, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-11 22:09:21 DP1 TP13] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=4096, K=512, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-11 22:09:21 DP1 TP11] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=4096, K=512, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-11 22:09:21 DP1 TP14] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=4096, K=512, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-11 22:09:21 DP1 TP12] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=4096, K=512, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-11 22:09:22 DP1 TP9] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=4096, K=512, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-11 22:09:22 DP1 TP8] Entering DeepGEMM JIT Pre-Compile session. It may takes a long time (typically 10-20 mins) if you have not run `sglang.compile_deep_gemm`. It is recommended to run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to reduce the overhead if you have not run it before. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[1,1]<stdout>:[2025-10-11 22:09:22 DP1 TP8] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=4096, K=512, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-11 22:09:22 DP1 TP15] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=4096, K=512, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:  0% 0/24 [00:00<?, ?it/s][1,1]<stdout>:100% 24/24 [00:00<00:00, 8716.95it/s]
[1,1]<stdout>:  0% 0/24 [00:00<?, ?it/s][1,1]<stdout>:100% 24/24 [00:00<00:00, 15548.86it/s]
[1,1]<stdout>:  0% 0/24 [00:00<?, ?it/s][1,1]<stdout>:100% 24/24 [00:00<00:00, 16163.02it/s]
[1,1]<stdout>:  0% 0/24 [00:00<?, ?it/s][1,1]<stdout>:100% 24/24 [00:00<00:00, 13846.40it/s]
[1,1]<stdout>:  0% 0/24 [00:00<?, ?it/s][1,1]<stdout>:100% 24/24 [00:00<00:00, 14781.69it/s]
[1,1]<stdout>:  0% 0/24 [00:00<?, ?it/s][1,1]<stdout>:  0% 0/24 [00:00<?, ?it/s][1,1]<stdout>:100% 24/24 [00:00<00:00, 14939.64it/s]
[1,1]<stdout>:100% 24/24 [00:00<00:00, 14403.10it/s]
[1,1]<stdout>:  0% 0/24 [00:00<?, ?it/s][1,1]<stdout>:100% 24/24 [00:00<00:00, 12706.80it/s]
[1,1]<stdout>:[2025-10-11 22:09:23 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 8, #cached-token: 0, token usage: 0.01, #running-req: 7, #queue-req: 0, 
[1,0]<stdout>:[2025-10-11 22:09:28] 
[1,0]<stdout>:Benchmark...
[1,1]<stdout>:[2025-10-11 22:09:28 DP1 TP8] Prefill batch. #new-seq: 2, #new-token: 233, #cached-token: 2, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[1,0]<stdout>:[2025-10-11 22:09:28 DP0 TP0] Prefill batch. #new-seq: 2, #new-token: 515, #cached-token: 2, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[1,1]<stdout>:[2025-10-11 22:09:29 DP1 TP8] Prefill batch. #new-seq: 11, #new-token: 2048, #cached-token: 11, token usage: 0.00, #running-req: 2, #queue-req: 483, 
[1,0]<stdout>:[2025-10-11 22:09:29 DP0 TP0] Prefill batch. #new-seq: 13, #new-token: 2048, #cached-token: 18, token usage: 0.00, #running-req: 2, #queue-req: 481, 
[1,1]<stdout>:[2025-10-11 22:09:30 DP1 TP8] Prefill batch. #new-seq: 3, #new-token: 2048, #cached-token: 2, token usage: 0.01, #running-req: 12, #queue-req: 985, 
[1,0]<stdout>:[2025-10-11 22:09:30 DP0 TP0] Prefill batch. #new-seq: 2, #new-token: 2048, #cached-token: 1, token usage: 0.02, #running-req: 14, #queue-req: 984, 
[1,0]<stdout>:[2025-10-11 22:09:30 DP0 TP0] Prefill batch. #new-seq: 9, #new-token: 2048, #cached-token: 10, token usage: 0.03, #running-req: 15, #queue-req: 976, 
[1,1]<stdout>:[2025-10-11 22:09:30 DP1 TP8] Prefill batch. #new-seq: 7, #new-token: 2048, #cached-token: 7, token usage: 0.03, #running-req: 14, #queue-req: 979, 
[1,0]<stdout>:[2025-10-11 22:09:30 DP0 TP0] Prefill batch. #new-seq: 2, #new-token: 2048, #cached-token: 1, token usage: 0.04, #running-req: 23, #queue-req: 975, 
[1,1]<stdout>:[2025-10-11 22:09:30 DP1 TP8] Prefill batch. #new-seq: 7, #new-token: 2048, #cached-token: 7, token usage: 0.04, #running-req: 20, #queue-req: 973, 
[1,1]<stdout>:[2025-10-11 22:09:31 DP1 TP8] Prefill batch. #new-seq: 6, #new-token: 659, #cached-token: 7, token usage: 0.05, #running-req: 26, #queue-req: 968, 
[1,0]<stdout>:[2025-10-11 22:09:31 DP0 TP0] Prefill batch. #new-seq: 8, #new-token: 804, #cached-token: 11, token usage: 0.05, #running-req: 24, #queue-req: 968, 
[1,1]<stdout>:[2025-10-11 22:09:31 DP1 TP8] Prefill batch. #new-seq: 2, #new-token: 82, #cached-token: 4, token usage: 0.05, #running-req: 30, #queue-req: 966, 
[1,1]<stdout>:[2025-10-11 22:09:31 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 492, #cached-token: 2, token usage: 0.04, #running-req: 31, #queue-req: 965, 
[1,1]<stdout>:[2025-10-11 22:09:32 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 242, #cached-token: 1, token usage: 0.04, #running-req: 31, #queue-req: 964, 
[1,0]<stdout>:[2025-10-11 22:09:32 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 10, #cached-token: 2, token usage: 0.05, #running-req: 31, #queue-req: 967, 
[1,1]<stdout>:[2025-10-11 22:09:32 DP1 TP8] Prefill batch. #new-seq: 2, #new-token: 64, #cached-token: 2, token usage: 0.04, #running-req: 30, #queue-req: 962, 
[1,0]<stdout>:[2025-10-11 22:09:32 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 228, #cached-token: 1, token usage: 0.05, #running-req: 31, #queue-req: 966, 
[1,1]<stdout>:[2025-10-11 22:09:32 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 511, #cached-token: 2, token usage: 0.04, #running-req: 31, #queue-req: 961, 
[1,1]<stdout>:[2025-10-11 22:09:33 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 411, #cached-token: 2, token usage: 0.04, #running-req: 31, #queue-req: 960, 
[1,0]<stdout>:[2025-10-11 22:09:33 DP0 TP0] Prefill batch. #new-seq: 2, #new-token: 469, #cached-token: 4, token usage: 0.05, #running-req: 30, #queue-req: 964, 
[1,0]<stdout>:[2025-10-11 22:09:34 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 774, #cached-token: 4, token usage: 0.05, #running-req: 31, #queue-req: 963, 
[1,0]<stdout>:[2025-10-11 22:09:34 DP0 TP0] Decode batch. #running-req: 31, #token: 9598, token usage: 0.06, accept len: 1.00, cuda graph: True, gen throughput (token/s): 32.31, #queue-req: 963, 
[1,1]<stdout>:[2025-10-11 22:09:34 DP1 TP8] Decode batch. #running-req: 32, #token: 7049, token usage: 0.04, accept len: 1.00, cuda graph: True, gen throughput (token/s): 32.23, #queue-req: 960, 
[1,0]<stdout>:[2025-10-11 22:09:34 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 232, #cached-token: 1, token usage: 0.06, #running-req: 31, #queue-req: 962, 
[1,1]<stdout>:[2025-10-11 22:09:34 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 2048, #cached-token: 1, token usage: 0.04, #running-req: 31, #queue-req: 959, 
[1,1]<stdout>:[2025-10-11 22:09:34 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 1818, #cached-token: 0, token usage: 0.05, #running-req: 31, #queue-req: 959, 
[1,1]<stdout>:[2025-10-11 22:09:34 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 626, #cached-token: 4, token usage: 0.04, #running-req: 31, #queue-req: 958, 
[1,0]<stdout>:[2025-10-11 22:09:35 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 394, #cached-token: 6, token usage: 0.06, #running-req: 31, #queue-req: 961, 
[1,1]<stdout>:[2025-10-11 22:09:35 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 28, #cached-token: 1, token usage: 0.05, #running-req: 31, #queue-req: 957, 
[1,0]<stdout>:[2025-10-11 22:09:35 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 473, #cached-token: 1, token usage: 0.06, #running-req: 31, #queue-req: 960, 
[1,0]<stdout>:[2025-10-11 22:09:36 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 214, #cached-token: 3, token usage: 0.06, #running-req: 31, #queue-req: 959, 
[1,0]<stdout>:[2025-10-11 22:09:36 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 1381, #cached-token: 1, token usage: 0.06, #running-req: 31, #queue-req: 958, 
[1,0]<stdout>:[2025-10-11 22:09:36 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 136, #cached-token: 1, token usage: 0.06, #running-req: 31, #queue-req: 957, 
[1,1]<stdout>:[2025-10-11 22:09:36 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 10, #cached-token: 1, token usage: 0.05, #running-req: 31, #queue-req: 956, 
[1,0]<stdout>:[2025-10-11 22:09:36 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 460, #cached-token: 1, token usage: 0.06, #running-req: 31, #queue-req: 956, 
[1,1]<stdout>:[2025-10-11 22:09:37 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 92, #cached-token: 2, token usage: 0.05, #running-req: 31, #queue-req: 955, 
[1,0]<stdout>:[2025-10-11 22:09:37 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 227, #cached-token: 5, token usage: 0.06, #running-req: 31, #queue-req: 955, 
[1,0]<stdout>:[2025-10-11 22:09:38 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 773, #cached-token: 1, token usage: 0.06, #running-req: 31, #queue-req: 954, 
[1,0]<stdout>:[2025-10-11 22:09:38 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 70, #cached-token: 1, token usage: 0.06, #running-req: 31, #queue-req: 953, 
[1,0]<stdout>:[2025-10-11 22:09:38 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 140, #cached-token: 1, token usage: 0.06, #running-req: 31, #queue-req: 952, 
[1,0]<stdout>:[2025-10-11 22:09:38 DP0 TP0] Decode batch. #running-req: 32, #token: 10557, token usage: 0.06, accept len: 1.00, cuda graph: True, gen throughput (token/s): 288.16, #queue-req: 952, 
[1,1]<stdout>:[2025-10-11 22:09:38 DP1 TP8] Decode batch. #running-req: 32, #token: 8356, token usage: 0.05, accept len: 1.00, cuda graph: True, gen throughput (token/s): 289.32, #queue-req: 955, 
[1,1]<stdout>:[2025-10-11 22:09:38 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 2, #cached-token: 1, token usage: 0.05, #running-req: 31, #queue-req: 954, 
[1,0]<stdout>:[2025-10-11 22:09:39 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 66, #cached-token: 1, token usage: 0.06, #running-req: 31, #queue-req: 951, 
[1,1]<stdout>:[2025-10-11 22:09:39 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 7, #cached-token: 1, token usage: 0.05, #running-req: 31, #queue-req: 953, 
[1,0]<stdout>:[2025-10-11 22:09:39 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 354, #cached-token: 1, token usage: 0.06, #running-req: 31, #queue-req: 950, 
[1,0]<stdout>:[2025-10-11 22:09:40 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 18, #cached-token: 2, token usage: 0.06, #running-req: 31, #queue-req: 949, 
[1,1]<stdout>:[2025-10-11 22:09:40 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 127, #cached-token: 3, token usage: 0.05, #running-req: 31, #queue-req: 952, 
[1,1]<stdout>:[2025-10-11 22:09:40 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 155, #cached-token: 1, token usage: 0.05, #running-req: 31, #queue-req: 951, 
[1,0]<stdout>:[2025-10-11 22:09:40 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 5, #cached-token: 2, token usage: 0.06, #running-req: 31, #queue-req: 948, 
[1,1]<stdout>:[2025-10-11 22:09:41 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 6, #cached-token: 1, token usage: 0.05, #running-req: 31, #queue-req: 950, 
[1,0]<stdout>:[2025-10-11 22:09:41 DP0 TP0] Prefill batch. #new-seq: 2, #new-token: 98, #cached-token: 4, token usage: 0.06, #running-req: 30, #queue-req: 946, 
[1,1]<stdout>:[2025-10-11 22:09:41 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 13, #cached-token: 1, token usage: 0.05, #running-req: 31, #queue-req: 949, 
[1,0]<stdout>:[2025-10-11 22:09:42 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 55, #cached-token: 1, token usage: 0.06, #running-req: 31, #queue-req: 945, 
[1,0]<stdout>:[2025-10-11 22:09:42 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 307, #cached-token: 1, token usage: 0.05, #running-req: 31, #queue-req: 944, 
[1,0]<stdout>:[2025-10-11 22:09:42 DP0 TP0] Decode batch. #running-req: 32, #token: 8226, token usage: 0.05, accept len: 1.00, cuda graph: True, gen throughput (token/s): 327.72, #queue-req: 944, 
[1,1]<stdout>:[2025-10-11 22:09:42 DP1 TP8] Decode batch. #running-req: 32, #token: 9001, token usage: 0.05, accept len: 1.00, cuda graph: True, gen throughput (token/s): 328.26, #queue-req: 949, 
[1,1]<stdout>:[2025-10-11 22:09:42 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 575, #cached-token: 5, token usage: 0.05, #running-req: 31, #queue-req: 948, 
[1,1]<stdout>:[2025-10-11 22:09:43 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 363, #cached-token: 3, token usage: 0.06, #running-req: 31, #queue-req: 947, 
[1,0]<stdout>:[2025-10-11 22:09:43 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 4, #cached-token: 1, token usage: 0.05, #running-req: 31, #queue-req: 943, 
[1,1]<stdout>:[2025-10-11 22:09:43 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 11, #cached-token: 3, token usage: 0.06, #running-req: 31, #queue-req: 946, 
[1,0]<stdout>:[2025-10-11 22:09:44 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 652, #cached-token: 11, token usage: 0.05, #running-req: 31, #queue-req: 942, 
[1,0]<stdout>:[2025-10-11 22:09:44 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 37, #cached-token: 1, token usage: 0.05, #running-req: 31, #queue-req: 941, 
[1,0]<stdout>:[2025-10-11 22:09:44 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 15, #cached-token: 1, token usage: 0.05, #running-req: 31, #queue-req: 940, 
[1,1]<stdout>:[2025-10-11 22:09:44 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 31, #cached-token: 2, token usage: 0.06, #running-req: 31, #queue-req: 945, 
[1,1]<stdout>:[2025-10-11 22:09:45 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 2048, #cached-token: 1, token usage: 0.06, #running-req: 31, #queue-req: 944, 
[1,1]<stdout>:[2025-10-11 22:09:45 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 367, #cached-token: 0, token usage: 0.07, #running-req: 31, #queue-req: 944, 
[1,0]<stdout>:[2025-10-11 22:09:45 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 187, #cached-token: 5, token usage: 0.05, #running-req: 31, #queue-req: 939, 
[1,1]<stdout>:[2025-10-11 22:09:46 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 119, #cached-token: 3, token usage: 0.07, #running-req: 31, #queue-req: 943, 
[1,0]<stdout>:[2025-10-11 22:09:46 DP0 TP0] Decode batch. #running-req: 32, #token: 9027, token usage: 0.05, accept len: 1.00, cuda graph: True, gen throughput (token/s): 338.11, #queue-req: 939, 
[1,1]<stdout>:[2025-10-11 22:09:46 DP1 TP8] Decode batch. #running-req: 32, #token: 12055, token usage: 0.07, accept len: 1.00, cuda graph: True, gen throughput (token/s): 337.85, #queue-req: 943, 
[1,0]<stdout>:[2025-10-11 22:09:46 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 15, #cached-token: 1, token usage: 0.05, #running-req: 31, #queue-req: 938, 
[1,0]<stdout>:[2025-10-11 22:09:46 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 177, #cached-token: 1, token usage: 0.05, #running-req: 31, #queue-req: 937, 
[1,1]<stdout>:[2025-10-11 22:09:46 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 16, #cached-token: 1, token usage: 0.06, #running-req: 31, #queue-req: 942, 
[1,1]<stdout>:[2025-10-11 22:09:47 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 44, #cached-token: 2, token usage: 0.06, #running-req: 31, #queue-req: 941, 
[1,0]<stdout>:[2025-10-11 22:09:47 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 364, #cached-token: 1, token usage: 0.06, #running-req: 31, #queue-req: 936, 
[1,0]<stdout>:[2025-10-11 22:09:48 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 59, #cached-token: 4, token usage: 0.06, #running-req: 31, #queue-req: 935, 
[1,0]<stdout>:[2025-10-11 22:09:49 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 74, #cached-token: 1, token usage: 0.06, #running-req: 31, #queue-req: 934, 
[1,1]<stdout>:[2025-10-11 22:09:49 DP1 TP8] Decode batch. #running-req: 32, #token: 10610, token usage: 0.06, accept len: 1.00, cuda graph: True, gen throughput (token/s): 441.87, #queue-req: 941, 
[1,0]<stdout>:[2025-10-11 22:09:49 DP0 TP0] Decode batch. #running-req: 32, #token: 9234, token usage: 0.05, accept len: 1.00, cuda graph: True, gen throughput (token/s): 440.79, #queue-req: 934, 
[1,0]<stdout>:[2025-10-11 22:09:49 DP0 TP0] Prefill batch. #new-seq: 2, #new-token: 15, #cached-token: 5, token usage: 0.05, #running-req: 30, #queue-req: 932, 
[1,0]<stdout>:[2025-10-11 22:09:49 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 806, #cached-token: 1, token usage: 0.04, #running-req: 31, #queue-req: 931, 
[1,0]<stdout>:[2025-10-11 22:09:50 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 53, #cached-token: 1, token usage: 0.04, #running-req: 31, #queue-req: 930, 
[1,1]<stdout>:[2025-10-11 22:09:50 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 5, #cached-token: 1, token usage: 0.06, #running-req: 31, #queue-req: 940, 
[1,1]<stdout>:[2025-10-11 22:09:50 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 359, #cached-token: 2, token usage: 0.06, #running-req: 31, #queue-req: 939, 
[1,0]<stdout>:[2025-10-11 22:09:51 DP0 TP0] Prefill batch. #new-seq: 2, #new-token: 967, #cached-token: 3, token usage: 0.04, #running-req: 30, #queue-req: 928, 
[1,0]<stdout>:[2025-10-11 22:09:51 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 102, #cached-token: 1, token usage: 0.04, #running-req: 31, #queue-req: 927, 
[1,0]<stdout>:[2025-10-11 22:09:51 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 182, #cached-token: 1, token usage: 0.04, #running-req: 31, #queue-req: 926, 
[1,0]<stdout>:[2025-10-11 22:09:51 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 1389, #cached-token: 1, token usage: 0.04, #running-req: 31, #queue-req: 925, 
[1,0]<stdout>:[2025-10-11 22:09:52 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 10, #cached-token: 3, token usage: 0.05, #running-req: 31, #queue-req: 924, 
[1,0]<stdout>:[2025-10-11 22:09:52 DP0 TP0] Decode batch. #running-req: 32, #token: 8420, token usage: 0.05, accept len: 1.00, cuda graph: True, gen throughput (token/s): 383.84, #queue-req: 924, 
[1,1]<stdout>:[2025-10-11 22:09:52 DP1 TP8] Decode batch. #running-req: 32, #token: 11308, token usage: 0.07, accept len: 1.00, cuda graph: True, gen throughput (token/s): 386.18, #queue-req: 939, 
[1,1]<stdout>:[2025-10-11 22:09:52 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 1017, #cached-token: 1, token usage: 0.07, #running-req: 31, #queue-req: 938, 
[1,0]<stdout>:[2025-10-11 22:09:53 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 699, #cached-token: 1, token usage: 0.05, #running-req: 31, #queue-req: 923, 
[1,1]<stdout>:[2025-10-11 22:09:54 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 429, #cached-token: 1, token usage: 0.07, #running-req: 31, #queue-req: 937, 
[1,1]<stdout>:[2025-10-11 22:09:54 DP1 TP8] Prefill batch. #new-seq: 2, #new-token: 759, #cached-token: 4, token usage: 0.07, #running-req: 30, #queue-req: 935, 
[1,0]<stdout>:[2025-10-11 22:09:55 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 102, #cached-token: 6, token usage: 0.05, #running-req: 31, #queue-req: 922, 
[1,1]<stdout>:[2025-10-11 22:09:55 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 13, #cached-token: 2, token usage: 0.08, #running-req: 31, #queue-req: 934, 
[1,0]<stdout>:[2025-10-11 22:09:55 DP0 TP0] Prefill batch. #new-seq: 2, #new-token: 613, #cached-token: 2, token usage: 0.05, #running-req: 30, #queue-req: 920, 
[1,1]<stdout>:[2025-10-11 22:09:55 DP1 TP8] Decode batch. #running-req: 32, #token: 13177, token usage: 0.08, accept len: 1.00, cuda graph: True, gen throughput (token/s): 453.02, #queue-req: 934, 
[1,0]<stdout>:[2025-10-11 22:09:55 DP0 TP0] Decode batch. #running-req: 32, #token: 8986, token usage: 0.05, accept len: 1.00, cuda graph: True, gen throughput (token/s): 453.27, #queue-req: 920, 
[1,0]<stdout>:[2025-10-11 22:09:55 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 11, #cached-token: 2, token usage: 0.05, #running-req: 31, #queue-req: 919, 
[1,1]<stdout>:[2025-10-11 22:09:55 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 109, #cached-token: 2, token usage: 0.07, #running-req: 31, #queue-req: 933, 
[1,0]<stdout>:[2025-10-11 22:09:56 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 1, token usage: 0.05, #running-req: 31, #queue-req: 918, 
[1,1]<stdout>:[2025-10-11 22:09:56 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 1978, #cached-token: 1, token usage: 0.07, #running-req: 31, #queue-req: 932, 
[1,0]<stdout>:[2025-10-11 22:09:56 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 489, #cached-token: 1, token usage: 0.05, #running-req: 31, #queue-req: 917, 
[1,1]<stdout>:[2025-10-11 22:09:56 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 72, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 931, 
[1,1]<stdout>:[2025-10-11 22:09:57 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 96, #cached-token: 3, token usage: 0.08, #running-req: 31, #queue-req: 930, 
[1,0]<stdout>:[2025-10-11 22:09:57 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 65, #cached-token: 1, token usage: 0.06, #running-req: 31, #queue-req: 916, 
[1,0]<stdout>:[2025-10-11 22:09:58 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 389, #cached-token: 2, token usage: 0.06, #running-req: 31, #queue-req: 915, 
[1,1]<stdout>:[2025-10-11 22:09:58 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 30, #cached-token: 2, token usage: 0.08, #running-req: 31, #queue-req: 929, 
[1,0]<stdout>:[2025-10-11 22:09:58 DP0 TP0] Decode batch. #running-req: 32, #token: 9712, token usage: 0.06, accept len: 1.00, cuda graph: True, gen throughput (token/s): 399.07, #queue-req: 915, 
[1,1]<stdout>:[2025-10-11 22:09:58 DP1 TP8] Decode batch. #running-req: 32, #token: 14006, token usage: 0.08, accept len: 1.00, cuda graph: True, gen throughput (token/s): 399.05, #queue-req: 929, 
[1,0]<stdout>:[2025-10-11 22:09:58 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 93, #cached-token: 1, token usage: 0.06, #running-req: 31, #queue-req: 914, 
[1,1]<stdout>:[2025-10-11 22:09:59 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 346, #cached-token: 2, token usage: 0.08, #running-req: 31, #queue-req: 928, 
[1,1]<stdout>:[2025-10-11 22:09:59 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 11, #cached-token: 2, token usage: 0.08, #running-req: 31, #queue-req: 927, 
[1,0]<stdout>:[2025-10-11 22:09:59 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 121, #cached-token: 4, token usage: 0.06, #running-req: 31, #queue-req: 913, 
[1,1]<stdout>:[2025-10-11 22:09:59 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 101, #cached-token: 2, token usage: 0.08, #running-req: 31, #queue-req: 926, 
[1,0]<stdout>:[2025-10-11 22:10:00 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 385, #cached-token: 3, token usage: 0.06, #running-req: 31, #queue-req: 912, 
[1,1]<stdout>:[2025-10-11 22:10:00 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 703, #cached-token: 2, token usage: 0.07, #running-req: 31, #queue-req: 925, 
[1,0]<stdout>:[2025-10-11 22:10:00 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 1066, #cached-token: 1, token usage: 0.06, #running-req: 31, #queue-req: 911, 
[1,1]<stdout>:[2025-10-11 22:10:00 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 3, #cached-token: 3, token usage: 0.07, #running-req: 31, #queue-req: 924, 
[1,0]<stdout>:[2025-10-11 22:10:00 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 467, #cached-token: 4, token usage: 0.07, #running-req: 31, #queue-req: 910, 
[1,1]<stdout>:[2025-10-11 22:10:00 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 213, #cached-token: 2, token usage: 0.07, #running-req: 31, #queue-req: 923, 
[1,1]<stdout>:[2025-10-11 22:10:01 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 1, token usage: 0.07, #running-req: 31, #queue-req: 922, 
[1,1]<stdout>:[2025-10-11 22:10:01 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 810, #cached-token: 3, token usage: 0.07, #running-req: 31, #queue-req: 921, 
[1,0]<stdout>:[2025-10-11 22:10:01 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 509, #cached-token: 1, token usage: 0.07, #running-req: 31, #queue-req: 909, 
[1,0]<stdout>:[2025-10-11 22:10:02 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 26, #cached-token: 1, token usage: 0.07, #running-req: 31, #queue-req: 908, 
[1,1]<stdout>:[2025-10-11 22:10:02 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 516, #cached-token: 2, token usage: 0.07, #running-req: 31, #queue-req: 920, 
[1,1]<stdout>:[2025-10-11 22:10:02 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 350, #cached-token: 2, token usage: 0.07, #running-req: 31, #queue-req: 919, 
[1,1]<stdout>:[2025-10-11 22:10:02 DP1 TP8] Decode batch. #running-req: 32, #token: 12302, token usage: 0.07, accept len: 1.00, cuda graph: True, gen throughput (token/s): 316.10, #queue-req: 919, 
[1,0]<stdout>:[2025-10-11 22:10:02 DP0 TP0] Decode batch. #running-req: 32, #token: 11779, token usage: 0.07, accept len: 1.00, cuda graph: True, gen throughput (token/s): 316.86, #queue-req: 908, 
[1,1]<stdout>:[2025-10-11 22:10:02 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 12, #cached-token: 4, token usage: 0.06, #running-req: 31, #queue-req: 918, 
[1,1]<stdout>:[2025-10-11 22:10:03 DP1 TP8] Prefill batch. #new-seq: 2, #new-token: 616, #cached-token: 5, token usage: 0.06, #running-req: 30, #queue-req: 916, 
[1,0]<stdout>:[2025-10-11 22:10:03 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 292, #cached-token: 2, token usage: 0.07, #running-req: 31, #queue-req: 907, 
[1,1]<stdout>:[2025-10-11 22:10:03 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 494, #cached-token: 1, token usage: 0.07, #running-req: 31, #queue-req: 915, 
[1,0]<stdout>:[2025-10-11 22:10:04 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 141, #cached-token: 2, token usage: 0.07, #running-req: 31, #queue-req: 906, 
[1,0]<stdout>:[2025-10-11 22:10:04 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 588, #cached-token: 2, token usage: 0.07, #running-req: 31, #queue-req: 905, 
[1,0]<stdout>:[2025-10-11 22:10:05 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 8, #cached-token: 1, token usage: 0.07, #running-req: 31, #queue-req: 904, 
[1,0]<stdout>:[2025-10-11 22:10:05 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 467, #cached-token: 1, token usage: 0.07, #running-req: 31, #queue-req: 903, 
[1,1]<stdout>:[2025-10-11 22:10:05 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 23, #cached-token: 3, token usage: 0.07, #running-req: 31, #queue-req: 914, 
[1,1]<stdout>:[2025-10-11 22:10:05 DP1 TP8] Decode batch. #running-req: 32, #token: 11941, token usage: 0.07, accept len: 1.00, cuda graph: True, gen throughput (token/s): 392.62, #queue-req: 914, 
[1,0]<stdout>:[2025-10-11 22:10:05 DP0 TP0] Decode batch. #running-req: 32, #token: 12560, token usage: 0.07, accept len: 1.00, cuda graph: True, gen throughput (token/s): 392.57, #queue-req: 903, 
[1,0]<stdout>:[2025-10-11 22:10:05 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 249, #cached-token: 2, token usage: 0.07, #running-req: 31, #queue-req: 902, 
[1,0]<stdout>:[2025-10-11 22:10:06 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 20, #cached-token: 1, token usage: 0.07, #running-req: 31, #queue-req: 901, 
[1,0]<stdout>:[2025-10-11 22:10:06 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 49, #cached-token: 1, token usage: 0.07, #running-req: 31, #queue-req: 900, 
[1,1]<stdout>:[2025-10-11 22:10:06 DP1 TP8] Prefill batch. #new-seq: 2, #new-token: 766, #cached-token: 7, token usage: 0.07, #running-req: 30, #queue-req: 912, 
[1,0]<stdout>:[2025-10-11 22:10:06 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 21, #cached-token: 1, token usage: 0.07, #running-req: 31, #queue-req: 899, 
[1,1]<stdout>:[2025-10-11 22:10:07 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 51, #cached-token: 1, token usage: 0.07, #running-req: 31, #queue-req: 911, 
[1,0]<stdout>:[2025-10-11 22:10:07 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 663, #cached-token: 5, token usage: 0.07, #running-req: 31, #queue-req: 898, 
[1,0]<stdout>:[2025-10-11 22:10:07 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 7, #cached-token: 1, token usage: 0.06, #running-req: 31, #queue-req: 897, 
[1,0]<stdout>:[2025-10-11 22:10:07 DP0 TP0] Prefill batch. #new-seq: 2, #new-token: 539, #cached-token: 8, token usage: 0.06, #running-req: 30, #queue-req: 895, 
[1,1]<stdout>:[2025-10-11 22:10:08 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 473, #cached-token: 1, token usage: 0.06, #running-req: 31, #queue-req: 910, 
[1,1]<stdout>:[2025-10-11 22:10:08 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 6, #cached-token: 1, token usage: 0.07, #running-req: 31, #queue-req: 909, 
[1,1]<stdout>:[2025-10-11 22:10:08 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 765, #cached-token: 1, token usage: 0.07, #running-req: 31, #queue-req: 908, 
[1,0]<stdout>:[2025-10-11 22:10:09 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 643, #cached-token: 6, token usage: 0.07, #running-req: 31, #queue-req: 894, 
[1,1]<stdout>:[2025-10-11 22:10:09 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 1228, #cached-token: 2, token usage: 0.07, #running-req: 31, #queue-req: 907, 
[1,0]<stdout>:[2025-10-11 22:10:09 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 635, #cached-token: 2, token usage: 0.07, #running-req: 31, #queue-req: 893, 
[1,0]<stdout>:[2025-10-11 22:10:10 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 375, #cached-token: 2, token usage: 0.07, #running-req: 31, #queue-req: 892, 
[1,0]<stdout>:[2025-10-11 22:10:10 DP0 TP0] Prefill batch. #new-seq: 2, #new-token: 596, #cached-token: 4, token usage: 0.06, #running-req: 30, #queue-req: 890, 
[1,0]<stdout>:[2025-10-11 22:10:10 DP0 TP0] Decode batch. #running-req: 30, #token: 11363, token usage: 0.07, accept len: 1.00, cuda graph: True, gen throughput (token/s): 282.97, #queue-req: 890, 
[1,1]<stdout>:[2025-10-11 22:10:10 DP1 TP8] Decode batch. #running-req: 32, #token: 13366, token usage: 0.08, accept len: 1.00, cuda graph: True, gen throughput (token/s): 284.73, #queue-req: 907, 
[1,0]<stdout>:[2025-10-11 22:10:10 DP0 TP0] Prefill batch. #new-seq: 2, #new-token: 713, #cached-token: 9, token usage: 0.07, #running-req: 30, #queue-req: 888, 
[1,1]<stdout>:[2025-10-11 22:10:10 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 372, #cached-token: 1, token usage: 0.07, #running-req: 31, #queue-req: 906, 
[1,0]<stdout>:[2025-10-11 22:10:10 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 222, #cached-token: 8, token usage: 0.07, #running-req: 31, #queue-req: 887, 
[1,1]<stdout>:[2025-10-11 22:10:11 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 44, #cached-token: 5, token usage: 0.08, #running-req: 31, #queue-req: 905, 
[1,1]<stdout>:[2025-10-11 22:10:12 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 4, #cached-token: 1, token usage: 0.07, #running-req: 31, #queue-req: 904, 
[1,0]<stdout>:[2025-10-11 22:10:12 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 4, #cached-token: 1, token usage: 0.07, #running-req: 31, #queue-req: 886, 
[1,0]<stdout>:[2025-10-11 22:10:12 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 529, #cached-token: 1, token usage: 0.07, #running-req: 31, #queue-req: 885, 
[1,0]<stdout>:[2025-10-11 22:10:13 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 10, #cached-token: 1, token usage: 0.07, #running-req: 31, #queue-req: 884, 
[1,1]<stdout>:[2025-10-11 22:10:13 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 13, #cached-token: 3, token usage: 0.07, #running-req: 31, #queue-req: 903, 
[1,1]<stdout>:[2025-10-11 22:10:13 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 71, #cached-token: 4, token usage: 0.07, #running-req: 31, #queue-req: 902, 
[1,0]<stdout>:[2025-10-11 22:10:13 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 45, #cached-token: 3, token usage: 0.07, #running-req: 31, #queue-req: 883, 
[1,0]<stdout>:[2025-10-11 22:10:13 DP0 TP0] Decode batch. #running-req: 32, #token: 12013, token usage: 0.07, accept len: 1.00, cuda graph: True, gen throughput (token/s): 397.03, #queue-req: 883, 
[1,1]<stdout>:[2025-10-11 22:10:13 DP1 TP8] Decode batch. #running-req: 30, #token: 10771, token usage: 0.06, accept len: 1.00, cuda graph: True, gen throughput (token/s): 396.41, #queue-req: 902, 
[1,1]<stdout>:[2025-10-11 22:10:13 DP1 TP8] Prefill batch. #new-seq: 2, #new-token: 460, #cached-token: 5, token usage: 0.06, #running-req: 30, #queue-req: 900, 
[1,1]<stdout>:[2025-10-11 22:10:14 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 54, #cached-token: 4, token usage: 0.06, #running-req: 31, #queue-req: 899, 
[1,1]<stdout>:[2025-10-11 22:10:14 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 84, #cached-token: 1, token usage: 0.06, #running-req: 31, #queue-req: 898, 
[1,1]<stdout>:[2025-10-11 22:10:14 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 15, #cached-token: 1, token usage: 0.06, #running-req: 31, #queue-req: 897, 
[1,1]<stdout>:[2025-10-11 22:10:15 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 9, #cached-token: 2, token usage: 0.06, #running-req: 31, #queue-req: 896, 
[1,1]<stdout>:[2025-10-11 22:10:15 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 202, #cached-token: 1, token usage: 0.06, #running-req: 31, #queue-req: 895, 
[1,0]<stdout>:[2025-10-11 22:10:15 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 4, #cached-token: 3, token usage: 0.07, #running-req: 31, #queue-req: 882, 
[1,0]<stdout>:[2025-10-11 22:10:15 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 991, #cached-token: 3, token usage: 0.07, #running-req: 31, #queue-req: 881, 
[1,1]<stdout>:[2025-10-11 22:10:16 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 17, #cached-token: 2, token usage: 0.06, #running-req: 31, #queue-req: 894, 
[1,1]<stdout>:[2025-10-11 22:10:16 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 408, #cached-token: 1, token usage: 0.06, #running-req: 31, #queue-req: 893, 
[1,0]<stdout>:[2025-10-11 22:10:16 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 14, #cached-token: 3, token usage: 0.07, #running-req: 31, #queue-req: 880, 
[1,0]<stdout>:[2025-10-11 22:10:17 DP0 TP0] Decode batch. #running-req: 31, #token: 12229, token usage: 0.07, accept len: 1.00, cuda graph: True, gen throughput (token/s): 368.46, #queue-req: 880, 
[1,1]<stdout>:[2025-10-11 22:10:17 DP1 TP8] Decode batch. #running-req: 32, #token: 10517, token usage: 0.06, accept len: 1.00, cuda graph: True, gen throughput (token/s): 367.57, #queue-req: 893, 
[1,0]<stdout>:[2025-10-11 22:10:17 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 368, #cached-token: 1, token usage: 0.07, #running-req: 31, #queue-req: 879, 
[1,1]<stdout>:[2025-10-11 22:10:17 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 9, #cached-token: 1, token usage: 0.06, #running-req: 31, #queue-req: 892, 
[1,0]<stdout>:[2025-10-11 22:10:17 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 84, #cached-token: 2, token usage: 0.07, #running-req: 31, #queue-req: 878, 
[1,1]<stdout>:[2025-10-11 22:10:18 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 120, #cached-token: 2, token usage: 0.06, #running-req: 31, #queue-req: 891, 
[1,0]<stdout>:[2025-10-11 22:10:18 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 438, #cached-token: 4, token usage: 0.07, #running-req: 31, #queue-req: 877, 
[1,0]<stdout>:[2025-10-11 22:10:18 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 9, #cached-token: 2, token usage: 0.08, #running-req: 31, #queue-req: 876, 
[1,1]<stdout>:[2025-10-11 22:10:19 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 98, #cached-token: 2, token usage: 0.06, #running-req: 31, #queue-req: 890, 
[1,0]<stdout>:[2025-10-11 22:10:19 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 430, #cached-token: 2, token usage: 0.08, #running-req: 31, #queue-req: 875, 
[1,0]<stdout>:[2025-10-11 22:10:19 DP0 TP0] Prefill batch. #new-seq: 3, #new-token: 752, #cached-token: 5, token usage: 0.07, #running-req: 29, #queue-req: 872, 
[1,0]<stdout>:[2025-10-11 22:10:20 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 372, #cached-token: 2, token usage: 0.08, #running-req: 31, #queue-req: 871, 
[1,0]<stdout>:[2025-10-11 22:10:20 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 546, #cached-token: 1, token usage: 0.07, #running-req: 31, #queue-req: 870, 
[1,0]<stdout>:[2025-10-11 22:10:20 DP0 TP0] Decode batch. #running-req: 31, #token: 12982, token usage: 0.08, accept len: 1.00, cuda graph: True, gen throughput (token/s): 363.79, #queue-req: 870, 
[1,1]<stdout>:[2025-10-11 22:10:20 DP1 TP8] Decode batch. #running-req: 32, #token: 11295, token usage: 0.07, accept len: 1.00, cuda graph: True, gen throughput (token/s): 365.79, #queue-req: 890, 
[1,0]<stdout>:[2025-10-11 22:10:20 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 558, #cached-token: 6, token usage: 0.08, #running-req: 31, #queue-req: 869, 
[1,0]<stdout>:[2025-10-11 22:10:20 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 8, #cached-token: 3, token usage: 0.08, #running-req: 31, #queue-req: 868, 
[1,0]<stdout>:[2025-10-11 22:10:21 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 138, #cached-token: 2, token usage: 0.08, #running-req: 31, #queue-req: 867, 
[1,0]<stdout>:[2025-10-11 22:10:21 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 11, #cached-token: 3, token usage: 0.07, #running-req: 31, #queue-req: 866, 
[1,0]<stdout>:[2025-10-11 22:10:21 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 868, #cached-token: 3, token usage: 0.07, #running-req: 31, #queue-req: 865, 
[1,1]<stdout>:[2025-10-11 22:10:21 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 36, #cached-token: 2, token usage: 0.07, #running-req: 31, #queue-req: 889, 
[1,1]<stdout>:[2025-10-11 22:10:22 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 518, #cached-token: 3, token usage: 0.07, #running-req: 31, #queue-req: 888, 
[1,0]<stdout>:[2025-10-11 22:10:22 DP0 TP0] Prefill batch. #new-seq: 2, #new-token: 114, #cached-token: 4, token usage: 0.07, #running-req: 30, #queue-req: 863, 
[1,0]<stdout>:[2025-10-11 22:10:22 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 20, #cached-token: 1, token usage: 0.07, #running-req: 31, #queue-req: 862, 
[1,1]<stdout>:[2025-10-11 22:10:23 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 510, #cached-token: 4, token usage: 0.07, #running-req: 31, #queue-req: 887, 
[1,1]<stdout>:[2025-10-11 22:10:23 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 185, #cached-token: 2, token usage: 0.07, #running-req: 31, #queue-req: 886, 
[1,0]<stdout>:[2025-10-11 22:10:24 DP0 TP0] Decode batch. #running-req: 32, #token: 13140, token usage: 0.08, accept len: 1.00, cuda graph: True, gen throughput (token/s): 367.42, #queue-req: 862, 
[1,1]<stdout>:[2025-10-11 22:10:24 DP1 TP8] Decode batch. #running-req: 31, #token: 12161, token usage: 0.07, accept len: 1.00, cuda graph: True, gen throughput (token/s): 368.04, #queue-req: 886, 
[1,1]<stdout>:[2025-10-11 22:10:24 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 124, #cached-token: 1, token usage: 0.07, #running-req: 31, #queue-req: 885, 
[1,0]<stdout>:[2025-10-11 22:10:24 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 493, #cached-token: 2, token usage: 0.07, #running-req: 31, #queue-req: 861, 
[1,0]<stdout>:[2025-10-11 22:10:24 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 62, #cached-token: 4, token usage: 0.07, #running-req: 31, #queue-req: 860, 
[1,0]<stdout>:[2025-10-11 22:10:25 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 317, #cached-token: 4, token usage: 0.07, #running-req: 31, #queue-req: 859, 
[1,0]<stdout>:[2025-10-11 22:10:25 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 24, #cached-token: 3, token usage: 0.07, #running-req: 31, #queue-req: 858, 
[1,1]<stdout>:[2025-10-11 22:10:25 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 420, #cached-token: 3, token usage: 0.07, #running-req: 31, #queue-req: 884, 
[1,1]<stdout>:[2025-10-11 22:10:25 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 7, #cached-token: 1, token usage: 0.07, #running-req: 31, #queue-req: 883, 
[1,1]<stdout>:[2025-10-11 22:10:26 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 307, #cached-token: 2, token usage: 0.07, #running-req: 31, #queue-req: 882, 
[1,0]<stdout>:[2025-10-11 22:10:26 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 12, #cached-token: 2, token usage: 0.07, #running-req: 31, #queue-req: 857, 
[1,1]<stdout>:[2025-10-11 22:10:26 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 6, #cached-token: 2, token usage: 0.07, #running-req: 31, #queue-req: 881, 
[1,0]<stdout>:[2025-10-11 22:10:26 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 612, #cached-token: 2, token usage: 0.07, #running-req: 31, #queue-req: 856, 
[1,1]<stdout>:[2025-10-11 22:10:26 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 5, #cached-token: 2, token usage: 0.07, #running-req: 31, #queue-req: 880, 
[1,0]<stdout>:[2025-10-11 22:10:27 DP0 TP0] Decode batch. #running-req: 32, #token: 12777, token usage: 0.08, accept len: 1.00, cuda graph: True, gen throughput (token/s): 365.52, #queue-req: 856, 
[1,1]<stdout>:[2025-10-11 22:10:27 DP1 TP8] Decode batch. #running-req: 32, #token: 11458, token usage: 0.07, accept len: 1.00, cuda graph: True, gen throughput (token/s): 365.76, #queue-req: 880, 
[1,1]<stdout>:[2025-10-11 22:10:27 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 19, #cached-token: 2, token usage: 0.07, #running-req: 31, #queue-req: 879, 
[1,1]<stdout>:[2025-10-11 22:10:27 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 167, #cached-token: 2, token usage: 0.06, #running-req: 31, #queue-req: 878, 
[1,0]<stdout>:[2025-10-11 22:10:28 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 747, #cached-token: 1, token usage: 0.07, #running-req: 31, #queue-req: 855, 
[1,0]<stdout>:[2025-10-11 22:10:28 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 246, #cached-token: 2, token usage: 0.08, #running-req: 31, #queue-req: 854, 
[1,0]<stdout>:[2025-10-11 22:10:28 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 347, #cached-token: 5, token usage: 0.08, #running-req: 31, #queue-req: 853, 
[1,1]<stdout>:[2025-10-11 22:10:28 DP1 TP8] Prefill batch. #new-seq: 2, #new-token: 477, #cached-token: 3, token usage: 0.06, #running-req: 30, #queue-req: 876, 
[1,1]<stdout>:[2025-10-11 22:10:29 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 780, #cached-token: 3, token usage: 0.06, #running-req: 31, #queue-req: 875, 
[1,1]<stdout>:[2025-10-11 22:10:29 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 734, #cached-token: 3, token usage: 0.06, #running-req: 31, #queue-req: 874, 
[1,1]<stdout>:[2025-10-11 22:10:29 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 203, #cached-token: 1, token usage: 0.06, #running-req: 31, #queue-req: 873, 
[1,1]<stdout>:[2025-10-11 22:10:29 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 87, #cached-token: 1, token usage: 0.06, #running-req: 31, #queue-req: 872, 
[1,1]<stdout>:[2025-10-11 22:10:30 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 276, #cached-token: 1, token usage: 0.06, #running-req: 31, #queue-req: 871, 
[1,0]<stdout>:[2025-10-11 22:10:30 DP0 TP0] Prefill batch. #new-seq: 2, #new-token: 321, #cached-token: 3, token usage: 0.08, #running-req: 30, #queue-req: 851, 
[1,0]<stdout>:[2025-10-11 22:10:31 DP0 TP0] Decode batch. #running-req: 32, #token: 13337, token usage: 0.08, accept len: 1.00, cuda graph: True, gen throughput (token/s): 364.34, #queue-req: 851, 
[1,1]<stdout>:[2025-10-11 22:10:31 DP1 TP8] Decode batch. #running-req: 31, #token: 10106, token usage: 0.06, accept len: 1.00, cuda graph: True, gen throughput (token/s): 362.97, #queue-req: 871, 
[1,1]<stdout>:[2025-10-11 22:10:31 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 923, #cached-token: 1, token usage: 0.06, #running-req: 31, #queue-req: 870, 
[1,0]<stdout>:[2025-10-11 22:10:31 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 12, #cached-token: 3, token usage: 0.08, #running-req: 31, #queue-req: 850, 
[1,1]<stdout>:[2025-10-11 22:10:31 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 393, #cached-token: 1, token usage: 0.06, #running-req: 31, #queue-req: 869, 
[1,0]<stdout>:[2025-10-11 22:10:31 DP0 TP0] Prefill batch. #new-seq: 2, #new-token: 560, #cached-token: 7, token usage: 0.08, #running-req: 30, #queue-req: 848, 
[1,1]<stdout>:[2025-10-11 22:10:31 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 189, #cached-token: 4, token usage: 0.06, #running-req: 31, #queue-req: 868, 
[1,1]<stdout>:[2025-10-11 22:10:32 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 11, #cached-token: 1, token usage: 0.06, #running-req: 31, #queue-req: 867, 
[1,0]<stdout>:[2025-10-11 22:10:32 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 763, #cached-token: 2, token usage: 0.08, #running-req: 31, #queue-req: 847, 
[1,1]<stdout>:[2025-10-11 22:10:32 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 179, #cached-token: 2, token usage: 0.06, #running-req: 31, #queue-req: 866, 
[1,1]<stdout>:[2025-10-11 22:10:32 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 673, #cached-token: 3, token usage: 0.06, #running-req: 31, #queue-req: 865, 
[1,1]<stdout>:[2025-10-11 22:10:32 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 99, #cached-token: 1, token usage: 0.06, #running-req: 31, #queue-req: 864, 
[1,1]<stdout>:[2025-10-11 22:10:33 DP1 TP8] Prefill batch. #new-seq: 2, #new-token: 671, #cached-token: 5, token usage: 0.06, #running-req: 30, #queue-req: 862, 
[1,1]<stdout>:[2025-10-11 22:10:33 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 52, #cached-token: 2, token usage: 0.06, #running-req: 31, #queue-req: 861, 
[1,0]<stdout>:[2025-10-11 22:10:34 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 452, #cached-token: 3, token usage: 0.09, #running-req: 31, #queue-req: 846, 
[1,0]<stdout>:[2025-10-11 22:10:34 DP0 TP0] Decode batch. #running-req: 32, #token: 14975, token usage: 0.09, accept len: 1.00, cuda graph: True, gen throughput (token/s): 347.11, #queue-req: 846, 
[1,1]<stdout>:[2025-10-11 22:10:34 DP1 TP8] Decode batch. #running-req: 32, #token: 10738, token usage: 0.06, accept len: 1.00, cuda graph: True, gen throughput (token/s): 346.03, #queue-req: 861, 
[1,1]<stdout>:[2025-10-11 22:10:34 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 7, #cached-token: 2, token usage: 0.06, #running-req: 31, #queue-req: 860, 
[1,1]<stdout>:[2025-10-11 22:10:35 DP1 TP8] Prefill batch. #new-seq: 2, #new-token: 307, #cached-token: 3, token usage: 0.05, #running-req: 30, #queue-req: 858, 
[1,1]<stdout>:[2025-10-11 22:10:35 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 669, #cached-token: 4, token usage: 0.05, #running-req: 31, #queue-req: 857, 
[1,0]<stdout>:[2025-10-11 22:10:35 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 112, #cached-token: 3, token usage: 0.09, #running-req: 31, #queue-req: 845, 
[1,1]<stdout>:[2025-10-11 22:10:35 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 212, #cached-token: 2, token usage: 0.06, #running-req: 31, #queue-req: 856, 
[1,1]<stdout>:[2025-10-11 22:10:36 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 186, #cached-token: 1, token usage: 0.05, #running-req: 31, #queue-req: 855, 
[1,0]<stdout>:[2025-10-11 22:10:36 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 151, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 844, 
[1,1]<stdout>:[2025-10-11 22:10:36 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 467, #cached-token: 4, token usage: 0.05, #running-req: 31, #queue-req: 854, 
[1,1]<stdout>:[2025-10-11 22:10:37 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 657, #cached-token: 2, token usage: 0.06, #running-req: 31, #queue-req: 853, 
[1,1]<stdout>:[2025-10-11 22:10:37 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 31, #cached-token: 6, token usage: 0.06, #running-req: 31, #queue-req: 852, 
[1,1]<stdout>:[2025-10-11 22:10:37 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 25, #cached-token: 4, token usage: 0.06, #running-req: 31, #queue-req: 851, 
[1,1]<stdout>:[2025-10-11 22:10:37 DP1 TP8] Decode batch. #running-req: 31, #token: 10019, token usage: 0.06, accept len: 1.00, cuda graph: True, gen throughput (token/s): 384.26, #queue-req: 851, 
[1,0]<stdout>:[2025-10-11 22:10:37 DP0 TP0] Decode batch. #running-req: 32, #token: 15549, token usage: 0.09, accept len: 1.00, cuda graph: True, gen throughput (token/s): 387.02, #queue-req: 844, 
[1,1]<stdout>:[2025-10-11 22:10:37 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 501, #cached-token: 1, token usage: 0.06, #running-req: 31, #queue-req: 850, 
[1,1]<stdout>:[2025-10-11 22:10:38 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 120, #cached-token: 1, token usage: 0.06, #running-req: 31, #queue-req: 849, 
[1,0]<stdout>:[2025-10-11 22:10:38 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 14, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 843, 
[1,0]<stdout>:[2025-10-11 22:10:38 DP0 TP0] Prefill batch. #new-seq: 2, #new-token: 116, #cached-token: 3, token usage: 0.09, #running-req: 30, #queue-req: 841, 
[1,1]<stdout>:[2025-10-11 22:10:38 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 224, #cached-token: 4, token usage: 0.06, #running-req: 31, #queue-req: 848, 
[1,0]<stdout>:[2025-10-11 22:10:39 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 516, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 840, 
[1,1]<stdout>:[2025-10-11 22:10:39 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 5, #cached-token: 3, token usage: 0.06, #running-req: 31, #queue-req: 847, 
[1,1]<stdout>:[2025-10-11 22:10:39 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 501, #cached-token: 5, token usage: 0.06, #running-req: 31, #queue-req: 846, 
[1,0]<stdout>:[2025-10-11 22:10:40 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 58, #cached-token: 3, token usage: 0.09, #running-req: 31, #queue-req: 839, 
[1,0]<stdout>:[2025-10-11 22:10:41 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 903, #cached-token: 3, token usage: 0.09, #running-req: 31, #queue-req: 838, 
[1,1]<stdout>:[2025-10-11 22:10:41 DP1 TP8] Prefill batch. #new-seq: 2, #new-token: 801, #cached-token: 3, token usage: 0.05, #running-req: 30, #queue-req: 844, 
[1,1]<stdout>:[2025-10-11 22:10:41 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 10, #cached-token: 3, token usage: 0.06, #running-req: 31, #queue-req: 843, 
[1,0]<stdout>:[2025-10-11 22:10:41 DP0 TP0] Decode batch. #running-req: 31, #token: 15112, token usage: 0.09, accept len: 1.00, cuda graph: True, gen throughput (token/s): 368.23, #queue-req: 838, 
[1,1]<stdout>:[2025-10-11 22:10:41 DP1 TP8] Decode batch. #running-req: 32, #token: 9887, token usage: 0.06, accept len: 1.00, cuda graph: True, gen throughput (token/s): 368.18, #queue-req: 843, 
[1,0]<stdout>:[2025-10-11 22:10:41 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 35, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 837, 
[1,1]<stdout>:[2025-10-11 22:10:41 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 364, #cached-token: 6, token usage: 0.06, #running-req: 31, #queue-req: 842, 
[1,1]<stdout>:[2025-10-11 22:10:41 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 455, #cached-token: 1, token usage: 0.06, #running-req: 31, #queue-req: 841, 
[1,0]<stdout>:[2025-10-11 22:10:42 DP0 TP0] Prefill batch. #new-seq: 2, #new-token: 287, #cached-token: 2, token usage: 0.09, #running-req: 30, #queue-req: 835, 
[1,1]<stdout>:[2025-10-11 22:10:42 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 473, #cached-token: 6, token usage: 0.06, #running-req: 31, #queue-req: 840, 
[1,1]<stdout>:[2025-10-11 22:10:42 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 8, #cached-token: 3, token usage: 0.06, #running-req: 31, #queue-req: 839, 
[1,0]<stdout>:[2025-10-11 22:10:42 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 11, #cached-token: 2, token usage: 0.09, #running-req: 31, #queue-req: 834, 
[1,0]<stdout>:[2025-10-11 22:10:42 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 110, #cached-token: 2, token usage: 0.08, #running-req: 31, #queue-req: 833, 
[1,1]<stdout>:[2025-10-11 22:10:43 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 5, #cached-token: 1, token usage: 0.06, #running-req: 31, #queue-req: 838, 
[1,1]<stdout>:[2025-10-11 22:10:43 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 242, #cached-token: 1, token usage: 0.06, #running-req: 31, #queue-req: 837, 
[1,1]<stdout>:[2025-10-11 22:10:43 DP1 TP8] Prefill batch. #new-seq: 2, #new-token: 54, #cached-token: 5, token usage: 0.05, #running-req: 30, #queue-req: 835, 
[1,0]<stdout>:[2025-10-11 22:10:44 DP0 TP0] Decode batch. #running-req: 32, #token: 14969, token usage: 0.09, accept len: 1.00, cuda graph: True, gen throughput (token/s): 388.59, #queue-req: 833, 
[1,1]<stdout>:[2025-10-11 22:10:44 DP1 TP8] Decode batch. #running-req: 32, #token: 9446, token usage: 0.06, accept len: 1.00, cuda graph: True, gen throughput (token/s): 387.43, #queue-req: 835, 
[1,1]<stdout>:[2025-10-11 22:10:44 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 33, #cached-token: 1, token usage: 0.05, #running-req: 31, #queue-req: 834, 
[1,1]<stdout>:[2025-10-11 22:10:45 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 25, #cached-token: 1, token usage: 0.05, #running-req: 31, #queue-req: 833, 
[1,0]<stdout>:[2025-10-11 22:10:45 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 318, #cached-token: 3, token usage: 0.09, #running-req: 31, #queue-req: 832, 
[1,1]<stdout>:[2025-10-11 22:10:46 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 492, #cached-token: 1, token usage: 0.05, #running-req: 31, #queue-req: 832, 
[1,0]<stdout>:[2025-10-11 22:10:46 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 11, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 831, 
[1,1]<stdout>:[2025-10-11 22:10:46 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 18, #cached-token: 3, token usage: 0.05, #running-req: 31, #queue-req: 831, 
[1,0]<stdout>:[2025-10-11 22:10:47 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 5, #cached-token: 3, token usage: 0.08, #running-req: 31, #queue-req: 830, 
[1,0]<stdout>:[2025-10-11 22:10:47 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 13, #cached-token: 3, token usage: 0.08, #running-req: 31, #queue-req: 829, 
[1,0]<stdout>:[2025-10-11 22:10:47 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 463, #cached-token: 4, token usage: 0.08, #running-req: 31, #queue-req: 828, 
[1,0]<stdout>:[2025-10-11 22:10:47 DP0 TP0] Decode batch. #running-req: 31, #token: 14324, token usage: 0.08, accept len: 1.00, cuda graph: True, gen throughput (token/s): 401.85, #queue-req: 828, 
[1,1]<stdout>:[2025-10-11 22:10:47 DP1 TP8] Decode batch. #running-req: 32, #token: 9656, token usage: 0.06, accept len: 1.00, cuda graph: True, gen throughput (token/s): 402.52, #queue-req: 831, 
[1,0]<stdout>:[2025-10-11 22:10:47 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 17, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 827, 
[1,0]<stdout>:[2025-10-11 22:10:48 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 822, #cached-token: 5, token usage: 0.08, #running-req: 31, #queue-req: 826, 
[1,1]<stdout>:[2025-10-11 22:10:48 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 670, #cached-token: 1, token usage: 0.06, #running-req: 31, #queue-req: 830, 
[1,1]<stdout>:[2025-10-11 22:10:49 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 21, #cached-token: 2, token usage: 0.06, #running-req: 31, #queue-req: 829, 
[1,0]<stdout>:[2025-10-11 22:10:49 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 272, #cached-token: 5, token usage: 0.09, #running-req: 31, #queue-req: 825, 
[1,0]<stdout>:[2025-10-11 22:10:49 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 7, #cached-token: 4, token usage: 0.09, #running-req: 31, #queue-req: 824, 
[1,0]<stdout>:[2025-10-11 22:10:49 DP0 TP0] Prefill batch. #new-seq: 2, #new-token: 1867, #cached-token: 10, token usage: 0.08, #running-req: 30, #queue-req: 822, 
[1,0]<stdout>:[2025-10-11 22:10:50 DP0 TP0] Prefill batch. #new-seq: 2, #new-token: 307, #cached-token: 7, token usage: 0.09, #running-req: 30, #queue-req: 820, 
[1,0]<stdout>:[2025-10-11 22:10:50 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 80, #cached-token: 3, token usage: 0.09, #running-req: 31, #queue-req: 819, 
[1,1]<stdout>:[2025-10-11 22:10:50 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 211, #cached-token: 1, token usage: 0.06, #running-req: 31, #queue-req: 828, 
[1,0]<stdout>:[2025-10-11 22:10:50 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 6, #cached-token: 2, token usage: 0.09, #running-req: 31, #queue-req: 818, 
[1,0]<stdout>:[2025-10-11 22:10:50 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 1231, #cached-token: 2, token usage: 0.09, #running-req: 31, #queue-req: 817, 
[1,0]<stdout>:[2025-10-11 22:10:51 DP0 TP0] Decode batch. #running-req: 32, #token: 16665, token usage: 0.10, accept len: 1.00, cuda graph: True, gen throughput (token/s): 364.48, #queue-req: 817, 
[1,1]<stdout>:[2025-10-11 22:10:51 DP1 TP8] Decode batch. #running-req: 32, #token: 10442, token usage: 0.06, accept len: 1.00, cuda graph: True, gen throughput (token/s): 366.44, #queue-req: 828, 
[1,0]<stdout>:[2025-10-11 22:10:51 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 172, #cached-token: 1, token usage: 0.10, #running-req: 31, #queue-req: 816, 
[1,0]<stdout>:[2025-10-11 22:10:51 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 399, #cached-token: 7, token usage: 0.10, #running-req: 31, #queue-req: 815, 
[1,1]<stdout>:[2025-10-11 22:10:51 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 14, #cached-token: 1, token usage: 0.06, #running-req: 31, #queue-req: 827, 
[1,1]<stdout>:[2025-10-11 22:10:52 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 62, #cached-token: 1, token usage: 0.06, #running-req: 31, #queue-req: 826, 
[1,0]<stdout>:[2025-10-11 22:10:52 DP0 TP0] Prefill batch. #new-seq: 2, #new-token: 233, #cached-token: 6, token usage: 0.09, #running-req: 30, #queue-req: 813, 
[1,1]<stdout>:[2025-10-11 22:10:52 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 626, #cached-token: 6, token usage: 0.06, #running-req: 31, #queue-req: 825, 
[1,0]<stdout>:[2025-10-11 22:10:53 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 316, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 812, 
[1,1]<stdout>:[2025-10-11 22:10:53 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 543, #cached-token: 6, token usage: 0.06, #running-req: 31, #queue-req: 824, 
[1,0]<stdout>:[2025-10-11 22:10:53 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 1078, #cached-token: 2, token usage: 0.09, #running-req: 31, #queue-req: 811, 
[1,1]<stdout>:[2025-10-11 22:10:53 DP1 TP8] Prefill batch. #new-seq: 2, #new-token: 57, #cached-token: 2, token usage: 0.06, #running-req: 30, #queue-req: 822, 
[1,1]<stdout>:[2025-10-11 22:10:54 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 7, #cached-token: 3, token usage: 0.05, #running-req: 31, #queue-req: 821, 
[1,1]<stdout>:[2025-10-11 22:10:54 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 2048, #cached-token: 1, token usage: 0.05, #running-req: 31, #queue-req: 820, 
[1,1]<stdout>:[2025-10-11 22:10:54 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 361, #cached-token: 0, token usage: 0.07, #running-req: 31, #queue-req: 820, 
[1,0]<stdout>:[2025-10-11 22:10:54 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 992, #cached-token: 1, token usage: 0.10, #running-req: 31, #queue-req: 810, 
[1,0]<stdout>:[2025-10-11 22:10:55 DP0 TP0] Decode batch. #running-req: 32, #token: 17747, token usage: 0.10, accept len: 1.00, cuda graph: True, gen throughput (token/s): 338.30, #queue-req: 810, 
[1,1]<stdout>:[2025-10-11 22:10:55 DP1 TP8] Decode batch. #running-req: 32, #token: 11842, token usage: 0.07, accept len: 1.00, cuda graph: True, gen throughput (token/s): 338.03, #queue-req: 820, 
[1,0]<stdout>:[2025-10-11 22:10:55 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 30, #cached-token: 1, token usage: 0.10, #running-req: 31, #queue-req: 809, 
[1,0]<stdout>:[2025-10-11 22:10:56 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 1940, #cached-token: 1, token usage: 0.10, #running-req: 31, #queue-req: 808, 
[1,0]<stdout>:[2025-10-11 22:10:56 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 770, #cached-token: 5, token usage: 0.12, #running-req: 31, #queue-req: 807, 
[1,1]<stdout>:[2025-10-11 22:10:57 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 16, #cached-token: 2, token usage: 0.07, #running-req: 31, #queue-req: 819, 
[1,1]<stdout>:[2025-10-11 22:10:57 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 377, #cached-token: 1, token usage: 0.07, #running-req: 31, #queue-req: 818, 
[1,0]<stdout>:[2025-10-11 22:10:57 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 492, #cached-token: 1, token usage: 0.12, #running-req: 31, #queue-req: 806, 
[1,0]<stdout>:[2025-10-11 22:10:57 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 17, #cached-token: 2, token usage: 0.12, #running-req: 31, #queue-req: 805, 
[1,0]<stdout>:[2025-10-11 22:10:58 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 9, #cached-token: 3, token usage: 0.12, #running-req: 31, #queue-req: 804, 
[1,1]<stdout>:[2025-10-11 22:10:58 DP1 TP8] Decode batch. #running-req: 32, #token: 12936, token usage: 0.08, accept len: 1.00, cuda graph: True, gen throughput (token/s): 416.16, #queue-req: 818, 
[1,0]<stdout>:[2025-10-11 22:10:58 DP0 TP0] Decode batch. #running-req: 32, #token: 20534, token usage: 0.12, accept len: 1.00, cuda graph: True, gen throughput (token/s): 414.79, #queue-req: 804, 
[1,0]<stdout>:[2025-10-11 22:10:58 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 16, #cached-token: 3, token usage: 0.12, #running-req: 31, #queue-req: 803, 
[1,1]<stdout>:[2025-10-11 22:10:58 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 30, #cached-token: 3, token usage: 0.07, #running-req: 31, #queue-req: 817, 
[1,0]<stdout>:[2025-10-11 22:10:59 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 183, #cached-token: 2, token usage: 0.11, #running-req: 31, #queue-req: 802, 
[1,0]<stdout>:[2025-10-11 22:10:59 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 262, #cached-token: 4, token usage: 0.11, #running-req: 31, #queue-req: 801, 
[1,1]<stdout>:[2025-10-11 22:10:59 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 375, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 816, 
[1,0]<stdout>:[2025-10-11 22:11:00 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 368, #cached-token: 2, token usage: 0.12, #running-req: 31, #queue-req: 800, 
[1,1]<stdout>:[2025-10-11 22:11:00 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 17, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 815, 
[1,1]<stdout>:[2025-10-11 22:11:00 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 724, #cached-token: 9, token usage: 0.07, #running-req: 31, #queue-req: 814, 
[1,1]<stdout>:[2025-10-11 22:11:00 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 905, #cached-token: 2, token usage: 0.08, #running-req: 31, #queue-req: 813, 
[1,0]<stdout>:[2025-10-11 22:11:01 DP0 TP0] Decode batch. #running-req: 31, #token: 20216, token usage: 0.12, accept len: 1.00, cuda graph: True, gen throughput (token/s): 400.35, #queue-req: 800, 
[1,1]<stdout>:[2025-10-11 22:11:01 DP1 TP8] Decode batch. #running-req: 32, #token: 14041, token usage: 0.08, accept len: 1.00, cuda graph: True, gen throughput (token/s): 400.27, #queue-req: 813, 
[1,0]<stdout>:[2025-10-11 22:11:01 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 447, #cached-token: 1, token usage: 0.12, #running-req: 31, #queue-req: 799, 
[1,1]<stdout>:[2025-10-11 22:11:01 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 19, #cached-token: 3, token usage: 0.08, #running-req: 31, #queue-req: 812, 
[1,0]<stdout>:[2025-10-11 22:11:01 DP0 TP0] Prefill batch. #new-seq: 2, #new-token: 501, #cached-token: 5, token usage: 0.12, #running-req: 30, #queue-req: 797, 
[1,1]<stdout>:[2025-10-11 22:11:02 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 114, #cached-token: 3, token usage: 0.08, #running-req: 31, #queue-req: 811, 
[1,0]<stdout>:[2025-10-11 22:11:02 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 14, #cached-token: 2, token usage: 0.12, #running-req: 31, #queue-req: 796, 
[1,1]<stdout>:[2025-10-11 22:11:02 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 4, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 810, 
[1,1]<stdout>:[2025-10-11 22:11:03 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 174, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 809, 
[1,1]<stdout>:[2025-10-11 22:11:03 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 712, #cached-token: 2, token usage: 0.08, #running-req: 31, #queue-req: 808, 
[1,1]<stdout>:[2025-10-11 22:11:03 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 834, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 807, 
[1,0]<stdout>:[2025-10-11 22:11:04 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 759, #cached-token: 2, token usage: 0.12, #running-req: 31, #queue-req: 795, 
[1,1]<stdout>:[2025-10-11 22:11:04 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 755, #cached-token: 4, token usage: 0.08, #running-req: 31, #queue-req: 806, 
[1,0]<stdout>:[2025-10-11 22:11:04 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 265, #cached-token: 2, token usage: 0.12, #running-req: 31, #queue-req: 794, 
[1,1]<stdout>:[2025-10-11 22:11:04 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 18, #cached-token: 2, token usage: 0.08, #running-req: 31, #queue-req: 805, 
[1,0]<stdout>:[2025-10-11 22:11:04 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 1, token usage: 0.12, #running-req: 31, #queue-req: 793, 
[1,1]<stdout>:[2025-10-11 22:11:04 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 132, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 804, 
[1,0]<stdout>:[2025-10-11 22:11:05 DP0 TP0] Decode batch. #running-req: 32, #token: 19665, token usage: 0.12, accept len: 1.00, cuda graph: True, gen throughput (token/s): 337.10, #queue-req: 793, 
[1,1]<stdout>:[2025-10-11 22:11:05 DP1 TP8] Decode batch. #running-req: 32, #token: 13565, token usage: 0.08, accept len: 1.00, cuda graph: True, gen throughput (token/s): 336.32, #queue-req: 804, 
[1,1]<stdout>:[2025-10-11 22:11:05 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 498, #cached-token: 2, token usage: 0.08, #running-req: 31, #queue-req: 803, 
[1,0]<stdout>:[2025-10-11 22:11:05 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 398, #cached-token: 1, token usage: 0.12, #running-req: 31, #queue-req: 792, 
[1,0]<stdout>:[2025-10-11 22:11:05 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 28, #cached-token: 3, token usage: 0.12, #running-req: 31, #queue-req: 791, 
[1,1]<stdout>:[2025-10-11 22:11:06 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 49, #cached-token: 5, token usage: 0.08, #running-req: 31, #queue-req: 802, 
[1,1]<stdout>:[2025-10-11 22:11:06 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 455, #cached-token: 2, token usage: 0.07, #running-req: 31, #queue-req: 801, 
[1,1]<stdout>:[2025-10-11 22:11:06 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 7, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 800, 
[1,1]<stdout>:[2025-10-11 22:11:07 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 45, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 799, 
[1,0]<stdout>:[2025-10-11 22:11:07 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 48, #cached-token: 3, token usage: 0.11, #running-req: 31, #queue-req: 790, 
[1,1]<stdout>:[2025-10-11 22:11:07 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 1044, #cached-token: 2, token usage: 0.08, #running-req: 31, #queue-req: 798, 
[1,0]<stdout>:[2025-10-11 22:11:07 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 458, #cached-token: 2, token usage: 0.11, #running-req: 31, #queue-req: 789, 
[1,0]<stdout>:[2025-10-11 22:11:08 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 523, #cached-token: 2, token usage: 0.11, #running-req: 31, #queue-req: 788, 
[1,0]<stdout>:[2025-10-11 22:11:08 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 503, #cached-token: 3, token usage: 0.11, #running-req: 31, #queue-req: 787, 
[1,0]<stdout>:[2025-10-11 22:11:08 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 272, #cached-token: 6, token usage: 0.11, #running-req: 31, #queue-req: 786, 
[1,1]<stdout>:[2025-10-11 22:11:08 DP1 TP8] Prefill batch. #new-seq: 2, #new-token: 1248, #cached-token: 6, token usage: 0.08, #running-req: 30, #queue-req: 796, 
[1,0]<stdout>:[2025-10-11 22:11:08 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 35, #cached-token: 2, token usage: 0.11, #running-req: 31, #queue-req: 785, 
[1,0]<stdout>:[2025-10-11 22:11:08 DP0 TP0] Decode batch. #running-req: 32, #token: 18816, token usage: 0.11, accept len: 1.00, cuda graph: True, gen throughput (token/s): 337.03, #queue-req: 785, 
[1,1]<stdout>:[2025-10-11 22:11:08 DP1 TP8] Decode batch. #running-req: 32, #token: 14590, token usage: 0.09, accept len: 1.00, cuda graph: True, gen throughput (token/s): 337.03, #queue-req: 796, 
[1,0]<stdout>:[2025-10-11 22:11:09 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 10, #cached-token: 3, token usage: 0.11, #running-req: 31, #queue-req: 784, 
[1,1]<stdout>:[2025-10-11 22:11:09 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 33, #cached-token: 4, token usage: 0.09, #running-req: 31, #queue-req: 795, 
[1,0]<stdout>:[2025-10-11 22:11:09 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 4, #cached-token: 3, token usage: 0.10, #running-req: 31, #queue-req: 783, 
[1,0]<stdout>:[2025-10-11 22:11:09 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 775, #cached-token: 1, token usage: 0.10, #running-req: 31, #queue-req: 782, 
[1,1]<stdout>:[2025-10-11 22:11:09 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 18, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 794, 
[1,0]<stdout>:[2025-10-11 22:11:10 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 6, #cached-token: 2, token usage: 0.10, #running-req: 31, #queue-req: 781, 
[1,0]<stdout>:[2025-10-11 22:11:10 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 4, #cached-token: 2, token usage: 0.10, #running-req: 31, #queue-req: 780, 
[1,1]<stdout>:[2025-10-11 22:11:10 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 556, #cached-token: 3, token usage: 0.08, #running-req: 31, #queue-req: 793, 
[1,0]<stdout>:[2025-10-11 22:11:11 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 832, #cached-token: 7, token usage: 0.10, #running-req: 31, #queue-req: 779, 
[1,1]<stdout>:[2025-10-11 22:11:11 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 240, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 792, 
[1,0]<stdout>:[2025-10-11 22:11:11 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 93, #cached-token: 8, token usage: 0.10, #running-req: 31, #queue-req: 778, 
[1,0]<stdout>:[2025-10-11 22:11:11 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 499, #cached-token: 5, token usage: 0.09, #running-req: 31, #queue-req: 777, 
[1,0]<stdout>:[2025-10-11 22:11:11 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 279, #cached-token: 7, token usage: 0.09, #running-req: 31, #queue-req: 776, 
[1,0]<stdout>:[2025-10-11 22:11:12 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 4, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 775, 
[1,0]<stdout>:[2025-10-11 22:11:12 DP0 TP0] Decode batch. #running-req: 30, #token: 14741, token usage: 0.09, accept len: 1.00, cuda graph: True, gen throughput (token/s): 353.18, #queue-req: 775, 
[1,1]<stdout>:[2025-10-11 22:11:12 DP1 TP8] Decode batch. #running-req: 32, #token: 15188, token usage: 0.09, accept len: 1.00, cuda graph: True, gen throughput (token/s): 355.43, #queue-req: 792, 
[1,0]<stdout>:[2025-10-11 22:11:12 DP0 TP0] Prefill batch. #new-seq: 2, #new-token: 44, #cached-token: 2, token usage: 0.09, #running-req: 30, #queue-req: 773, 
[1,0]<stdout>:[2025-10-11 22:11:13 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 60, #cached-token: 2, token usage: 0.09, #running-req: 31, #queue-req: 772, 
[1,1]<stdout>:[2025-10-11 22:11:13 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 260, #cached-token: 7, token usage: 0.09, #running-req: 31, #queue-req: 791, 
[1,1]<stdout>:[2025-10-11 22:11:13 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 9, #cached-token: 2, token usage: 0.09, #running-req: 31, #queue-req: 790, 
[1,1]<stdout>:[2025-10-11 22:11:14 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 39, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 789, 
[1,1]<stdout>:[2025-10-11 22:11:14 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 359, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 788, 
[1,1]<stdout>:[2025-10-11 22:11:14 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 25, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 787, 
[1,1]<stdout>:[2025-10-11 22:11:15 DP1 TP8] Prefill batch. #new-seq: 2, #new-token: 1307, #cached-token: 6, token usage: 0.09, #running-req: 30, #queue-req: 785, 
[1,0]<stdout>:[2025-10-11 22:11:15 DP0 TP0] Decode batch. #running-req: 32, #token: 15828, token usage: 0.09, accept len: 1.00, cuda graph: True, gen throughput (token/s): 418.32, #queue-req: 772, 
[1,1]<stdout>:[2025-10-11 22:11:15 DP1 TP8] Decode batch. #running-req: 32, #token: 16523, token usage: 0.10, accept len: 1.00, cuda graph: True, gen throughput (token/s): 416.32, #queue-req: 785, 
[1,1]<stdout>:[2025-10-11 22:11:15 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 1078, #cached-token: 1, token usage: 0.10, #running-req: 31, #queue-req: 784, 
[1,0]<stdout>:[2025-10-11 22:11:15 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 16, #cached-token: 3, token usage: 0.09, #running-req: 31, #queue-req: 771, 
[1,1]<stdout>:[2025-10-11 22:11:15 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 376, #cached-token: 4, token usage: 0.10, #running-req: 31, #queue-req: 783, 
[1,0]<stdout>:[2025-10-11 22:11:16 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 10, #cached-token: 2, token usage: 0.09, #running-req: 31, #queue-req: 770, 
[1,1]<stdout>:[2025-10-11 22:11:16 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 4, #cached-token: 2, token usage: 0.10, #running-req: 31, #queue-req: 782, 
[1,0]<stdout>:[2025-10-11 22:11:16 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 7, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 769, 
[1,0]<stdout>:[2025-10-11 22:11:16 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 105, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 768, 
[1,0]<stdout>:[2025-10-11 22:11:17 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 176, #cached-token: 2, token usage: 0.08, #running-req: 31, #queue-req: 767, 
[1,0]<stdout>:[2025-10-11 22:11:18 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 176, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 766, 
[1,1]<stdout>:[2025-10-11 22:11:18 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 273, #cached-token: 2, token usage: 0.10, #running-req: 31, #queue-req: 781, 
[1,0]<stdout>:[2025-10-11 22:11:18 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 47, #cached-token: 3, token usage: 0.08, #running-req: 31, #queue-req: 765, 
[1,0]<stdout>:[2025-10-11 22:11:18 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 13, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 764, 
[1,0]<stdout>:[2025-10-11 22:11:18 DP0 TP0] Prefill batch. #new-seq: 2, #new-token: 470, #cached-token: 11, token usage: 0.08, #running-req: 30, #queue-req: 762, 
[1,0]<stdout>:[2025-10-11 22:11:18 DP0 TP0] Decode batch. #running-req: 32, #token: 12918, token usage: 0.08, accept len: 1.00, cuda graph: True, gen throughput (token/s): 381.04, #queue-req: 762, 
[1,1]<stdout>:[2025-10-11 22:11:18 DP1 TP8] Decode batch. #running-req: 32, #token: 16768, token usage: 0.10, accept len: 1.00, cuda graph: True, gen throughput (token/s): 382.90, #queue-req: 781, 
[1,0]<stdout>:[2025-10-11 22:11:18 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 27, #cached-token: 2, token usage: 0.08, #running-req: 31, #queue-req: 761, 
[1,1]<stdout>:[2025-10-11 22:11:19 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 22, #cached-token: 1, token usage: 0.10, #running-req: 31, #queue-req: 780, 
[1,1]<stdout>:[2025-10-11 22:11:19 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 4, #cached-token: 3, token usage: 0.10, #running-req: 31, #queue-req: 779, 
[1,0]<stdout>:[2025-10-11 22:11:19 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 278, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 760, 
[1,1]<stdout>:[2025-10-11 22:11:20 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 1414, #cached-token: 2, token usage: 0.09, #running-req: 31, #queue-req: 778, 
[1,0]<stdout>:[2025-10-11 22:11:20 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 461, #cached-token: 3, token usage: 0.08, #running-req: 31, #queue-req: 759, 
[1,0]<stdout>:[2025-10-11 22:11:21 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 203, #cached-token: 3, token usage: 0.08, #running-req: 31, #queue-req: 758, 
[1,0]<stdout>:[2025-10-11 22:11:21 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 1019, #cached-token: 3, token usage: 0.08, #running-req: 31, #queue-req: 757, 
[1,0]<stdout>:[2025-10-11 22:11:21 DP0 TP0] Decode batch. #running-req: 32, #token: 14354, token usage: 0.08, accept len: 1.00, cuda graph: True, gen throughput (token/s): 420.97, #queue-req: 757, 
[1,1]<stdout>:[2025-10-11 22:11:21 DP1 TP8] Decode batch. #running-req: 31, #token: 15427, token usage: 0.09, accept len: 1.00, cuda graph: True, gen throughput (token/s): 421.23, #queue-req: 778, 
[1,1]<stdout>:[2025-10-11 22:11:21 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 6, #cached-token: 2, token usage: 0.09, #running-req: 31, #queue-req: 777, 
[1,1]<stdout>:[2025-10-11 22:11:22 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 706, #cached-token: 2, token usage: 0.09, #running-req: 31, #queue-req: 776, 
[1,1]<stdout>:[2025-10-11 22:11:23 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 173, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 775, 
[1,1]<stdout>:[2025-10-11 22:11:23 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 5, #cached-token: 3, token usage: 0.08, #running-req: 31, #queue-req: 774, 
[1,1]<stdout>:[2025-10-11 22:11:23 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 170, #cached-token: 5, token usage: 0.09, #running-req: 31, #queue-req: 773, 
[1,1]<stdout>:[2025-10-11 22:11:24 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 2048, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 772, 
[1,1]<stdout>:[2025-10-11 22:11:24 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 517, #cached-token: 0, token usage: 0.10, #running-req: 31, #queue-req: 772, 
[1,0]<stdout>:[2025-10-11 22:11:24 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 3, #cached-token: 3, token usage: 0.08, #running-req: 31, #queue-req: 756, 
[1,0]<stdout>:[2025-10-11 22:11:24 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 13, #cached-token: 3, token usage: 0.08, #running-req: 31, #queue-req: 755, 
[1,0]<stdout>:[2025-10-11 22:11:25 DP0 TP0] Decode batch. #running-req: 32, #token: 14152, token usage: 0.08, accept len: 1.00, cuda graph: True, gen throughput (token/s): 408.02, #queue-req: 755, 
[1,1]<stdout>:[2025-10-11 22:11:25 DP1 TP8] Decode batch. #running-req: 32, #token: 17368, token usage: 0.10, accept len: 1.00, cuda graph: True, gen throughput (token/s): 407.07, #queue-req: 772, 
[1,0]<stdout>:[2025-10-11 22:11:25 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 20, #cached-token: 2, token usage: 0.08, #running-req: 31, #queue-req: 754, 
[1,1]<stdout>:[2025-10-11 22:11:25 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 12, #cached-token: 1, token usage: 0.10, #running-req: 31, #queue-req: 771, 
[1,0]<stdout>:[2025-10-11 22:11:26 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 203, #cached-token: 6, token usage: 0.08, #running-req: 31, #queue-req: 753, 
[1,1]<stdout>:[2025-10-11 22:11:26 DP1 TP8] Prefill batch. #new-seq: 2, #new-token: 79, #cached-token: 6, token usage: 0.09, #running-req: 30, #queue-req: 769, 
[1,0]<stdout>:[2025-10-11 22:11:26 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 273, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 752, 
[1,0]<stdout>:[2025-10-11 22:11:27 DP0 TP0] Prefill batch. #new-seq: 2, #new-token: 567, #cached-token: 4, token usage: 0.08, #running-req: 30, #queue-req: 750, 
[1,1]<stdout>:[2025-10-11 22:11:27 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 21, #cached-token: 2, token usage: 0.09, #running-req: 31, #queue-req: 768, 
[1,0]<stdout>:[2025-10-11 22:11:27 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 29, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 749, 
[1,0]<stdout>:[2025-10-11 22:11:27 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 27, #cached-token: 2, token usage: 0.09, #running-req: 31, #queue-req: 748, 
[1,1]<stdout>:[2025-10-11 22:11:27 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 135, #cached-token: 3, token usage: 0.09, #running-req: 31, #queue-req: 767, 
[1,0]<stdout>:[2025-10-11 22:11:28 DP0 TP0] Decode batch. #running-req: 32, #token: 14817, token usage: 0.09, accept len: 1.00, cuda graph: True, gen throughput (token/s): 417.84, #queue-req: 748, 
[1,1]<stdout>:[2025-10-11 22:11:28 DP1 TP8] Decode batch. #running-req: 31, #token: 15058, token usage: 0.09, accept len: 1.00, cuda graph: True, gen throughput (token/s): 418.18, #queue-req: 767, 
[1,1]<stdout>:[2025-10-11 22:11:28 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 38, #cached-token: 2, token usage: 0.09, #running-req: 31, #queue-req: 766, 
[1,0]<stdout>:[2025-10-11 22:11:28 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 156, #cached-token: 1, token usage: 0.07, #running-req: 31, #queue-req: 747, 
[1,0]<stdout>:[2025-10-11 22:11:29 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 33, #cached-token: 3, token usage: 0.07, #running-req: 31, #queue-req: 746, 
[1,1]<stdout>:[2025-10-11 22:11:29 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 16, #cached-token: 3, token usage: 0.09, #running-req: 31, #queue-req: 765, 
[1,0]<stdout>:[2025-10-11 22:11:30 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 819, #cached-token: 8, token usage: 0.08, #running-req: 31, #queue-req: 745, 
[1,1]<stdout>:[2025-10-11 22:11:30 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 19, #cached-token: 2, token usage: 0.08, #running-req: 31, #queue-req: 764, 
[1,1]<stdout>:[2025-10-11 22:11:30 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 1532, #cached-token: 3, token usage: 0.08, #running-req: 31, #queue-req: 763, 
[1,1]<stdout>:[2025-10-11 22:11:30 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 22, #cached-token: 2, token usage: 0.09, #running-req: 31, #queue-req: 762, 
[1,0]<stdout>:[2025-10-11 22:11:30 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 718, #cached-token: 5, token usage: 0.08, #running-req: 31, #queue-req: 744, 
[1,1]<stdout>:[2025-10-11 22:11:30 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 7, #cached-token: 3, token usage: 0.09, #running-req: 31, #queue-req: 761, 
[1,1]<stdout>:[2025-10-11 22:11:31 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 31, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 760, 
[1,0]<stdout>:[2025-10-11 22:11:31 DP0 TP0] Decode batch. #running-req: 31, #token: 13953, token usage: 0.08, accept len: 1.00, cuda graph: True, gen throughput (token/s): 385.08, #queue-req: 744, 
[1,1]<stdout>:[2025-10-11 22:11:31 DP1 TP8] Decode batch. #running-req: 31, #token: 14804, token usage: 0.09, accept len: 1.00, cuda graph: True, gen throughput (token/s): 384.47, #queue-req: 760, 
[1,1]<stdout>:[2025-10-11 22:11:31 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 155, #cached-token: 2, token usage: 0.09, #running-req: 31, #queue-req: 759, 
[1,0]<stdout>:[2025-10-11 22:11:31 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 10, #cached-token: 3, token usage: 0.08, #running-req: 31, #queue-req: 743, 
[1,0]<stdout>:[2025-10-11 22:11:32 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 975, #cached-token: 2, token usage: 0.08, #running-req: 31, #queue-req: 742, 
[1,0]<stdout>:[2025-10-11 22:11:32 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 881, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 741, 
[1,1]<stdout>:[2025-10-11 22:11:33 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 418, #cached-token: 4, token usage: 0.09, #running-req: 31, #queue-req: 758, 
[1,0]<stdout>:[2025-10-11 22:11:33 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 468, #cached-token: 6, token usage: 0.09, #running-req: 31, #queue-req: 740, 
[1,0]<stdout>:[2025-10-11 22:11:33 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 65, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 739, 
[1,0]<stdout>:[2025-10-11 22:11:34 DP0 TP0] Decode batch. #running-req: 31, #token: 14524, token usage: 0.09, accept len: 1.00, cuda graph: True, gen throughput (token/s): 463.15, #queue-req: 739, 
[1,1]<stdout>:[2025-10-11 22:11:34 DP1 TP8] Decode batch. #running-req: 31, #token: 15582, token usage: 0.09, accept len: 1.00, cuda graph: True, gen throughput (token/s): 464.23, #queue-req: 758, 
[1,1]<stdout>:[2025-10-11 22:11:34 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 757, 
[1,0]<stdout>:[2025-10-11 22:11:34 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 8, #cached-token: 2, token usage: 0.09, #running-req: 31, #queue-req: 738, 
[1,1]<stdout>:[2025-10-11 22:11:34 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 849, #cached-token: 6, token usage: 0.09, #running-req: 31, #queue-req: 756, 
[1,0]<stdout>:[2025-10-11 22:11:34 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 135, #cached-token: 3, token usage: 0.08, #running-req: 31, #queue-req: 737, 
[1,1]<stdout>:[2025-10-11 22:11:35 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 4, #cached-token: 1, token usage: 0.10, #running-req: 31, #queue-req: 755, 
[1,0]<stdout>:[2025-10-11 22:11:35 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 18, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 736, 
[1,1]<stdout>:[2025-10-11 22:11:35 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 566, #cached-token: 5, token usage: 0.10, #running-req: 31, #queue-req: 754, 
[1,0]<stdout>:[2025-10-11 22:11:36 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 253, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 735, 
[1,0]<stdout>:[2025-10-11 22:11:36 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 306, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 734, 
[1,0]<stdout>:[2025-10-11 22:11:36 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 12, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 733, 
[1,1]<stdout>:[2025-10-11 22:11:36 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 27, #cached-token: 3, token usage: 0.10, #running-req: 31, #queue-req: 753, 
[1,0]<stdout>:[2025-10-11 22:11:36 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 110, #cached-token: 7, token usage: 0.08, #running-req: 31, #queue-req: 732, 
[1,0]<stdout>:[2025-10-11 22:11:37 DP0 TP0] Prefill batch. #new-seq: 2, #new-token: 517, #cached-token: 7, token usage: 0.08, #running-req: 30, #queue-req: 730, 
[1,1]<stdout>:[2025-10-11 22:11:37 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 12, #cached-token: 2, token usage: 0.09, #running-req: 31, #queue-req: 752, 
[1,0]<stdout>:[2025-10-11 22:11:37 DP0 TP0] Decode batch. #running-req: 32, #token: 13781, token usage: 0.08, accept len: 1.00, cuda graph: True, gen throughput (token/s): 354.02, #queue-req: 730, 
[1,1]<stdout>:[2025-10-11 22:11:37 DP1 TP8] Decode batch. #running-req: 32, #token: 15897, token usage: 0.09, accept len: 1.00, cuda graph: True, gen throughput (token/s): 354.87, #queue-req: 752, 
[1,0]<stdout>:[2025-10-11 22:11:38 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 7, #cached-token: 3, token usage: 0.08, #running-req: 31, #queue-req: 729, 
[1,0]<stdout>:[2025-10-11 22:11:38 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 24, #cached-token: 2, token usage: 0.08, #running-req: 31, #queue-req: 728, 
[1,1]<stdout>:[2025-10-11 22:11:38 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 581, #cached-token: 6, token usage: 0.09, #running-req: 31, #queue-req: 751, 
[1,0]<stdout>:[2025-10-11 22:11:38 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 527, #cached-token: 2, token usage: 0.08, #running-req: 31, #queue-req: 727, 
[1,0]<stdout>:[2025-10-11 22:11:39 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 117, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 726, 
[1,1]<stdout>:[2025-10-11 22:11:39 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 18, #cached-token: 2, token usage: 0.10, #running-req: 31, #queue-req: 750, 
[1,0]<stdout>:[2025-10-11 22:11:39 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 335, #cached-token: 3, token usage: 0.08, #running-req: 31, #queue-req: 725, 
[1,0]<stdout>:[2025-10-11 22:11:40 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 37, #cached-token: 2, token usage: 0.08, #running-req: 31, #queue-req: 724, 
[1,0]<stdout>:[2025-10-11 22:11:40 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 795, #cached-token: 2, token usage: 0.08, #running-req: 31, #queue-req: 723, 
[1,1]<stdout>:[2025-10-11 22:11:40 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 19, #cached-token: 2, token usage: 0.09, #running-req: 31, #queue-req: 749, 
[1,0]<stdout>:[2025-10-11 22:11:40 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 8, #cached-token: 3, token usage: 0.08, #running-req: 31, #queue-req: 722, 
[1,1]<stdout>:[2025-10-11 22:11:40 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 571, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 748, 
[1,0]<stdout>:[2025-10-11 22:11:40 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 61, #cached-token: 2, token usage: 0.08, #running-req: 31, #queue-req: 721, 
[1,1]<stdout>:[2025-10-11 22:11:41 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 486, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 747, 
[1,0]<stdout>:[2025-10-11 22:11:41 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 6, #cached-token: 2, token usage: 0.08, #running-req: 31, #queue-req: 720, 
[1,1]<stdout>:[2025-10-11 22:11:41 DP1 TP8] Decode batch. #running-req: 32, #token: 16334, token usage: 0.10, accept len: 1.00, cuda graph: True, gen throughput (token/s): 338.77, #queue-req: 747, 
[1,0]<stdout>:[2025-10-11 22:11:41 DP0 TP0] Decode batch. #running-req: 32, #token: 13498, token usage: 0.08, accept len: 1.00, cuda graph: True, gen throughput (token/s): 337.42, #queue-req: 720, 
[1,1]<stdout>:[2025-10-11 22:11:42 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 61, #cached-token: 2, token usage: 0.09, #running-req: 31, #queue-req: 746, 
[1,1]<stdout>:[2025-10-11 22:11:42 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 45, #cached-token: 2, token usage: 0.09, #running-req: 31, #queue-req: 745, 
[1,0]<stdout>:[2025-10-11 22:11:42 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 479, #cached-token: 7, token usage: 0.08, #running-req: 31, #queue-req: 719, 
[1,1]<stdout>:[2025-10-11 22:11:43 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 17, #cached-token: 2, token usage: 0.09, #running-req: 31, #queue-req: 744, 
[1,0]<stdout>:[2025-10-11 22:11:43 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 15, #cached-token: 3, token usage: 0.09, #running-req: 31, #queue-req: 718, 
[1,1]<stdout>:[2025-10-11 22:11:43 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 15, #cached-token: 2, token usage: 0.09, #running-req: 31, #queue-req: 743, 
[1,0]<stdout>:[2025-10-11 22:11:43 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 1595, #cached-token: 2, token usage: 0.08, #running-req: 31, #queue-req: 717, 
[1,1]<stdout>:[2025-10-11 22:11:44 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 69, #cached-token: 2, token usage: 0.09, #running-req: 31, #queue-req: 742, 
[1,0]<stdout>:[2025-10-11 22:11:44 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 589, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 716, 
[1,1]<stdout>:[2025-10-11 22:11:44 DP1 TP8] Decode batch. #running-req: 31, #token: 14671, token usage: 0.09, accept len: 1.00, cuda graph: True, gen throughput (token/s): 415.93, #queue-req: 742, 
[1,0]<stdout>:[2025-10-11 22:11:44 DP0 TP0] Decode batch. #running-req: 32, #token: 16602, token usage: 0.10, accept len: 1.00, cuda graph: True, gen throughput (token/s): 416.58, #queue-req: 716, 
[1,1]<stdout>:[2025-10-11 22:11:44 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 402, #cached-token: 3, token usage: 0.09, #running-req: 31, #queue-req: 741, 
[1,1]<stdout>:[2025-10-11 22:11:45 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 176, #cached-token: 4, token usage: 0.09, #running-req: 31, #queue-req: 740, 
[1,1]<stdout>:[2025-10-11 22:11:45 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 3, #cached-token: 3, token usage: 0.09, #running-req: 31, #queue-req: 739, 
[1,0]<stdout>:[2025-10-11 22:11:45 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 315, #cached-token: 5, token usage: 0.09, #running-req: 31, #queue-req: 715, 
[1,1]<stdout>:[2025-10-11 22:11:45 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 6, #cached-token: 4, token usage: 0.09, #running-req: 31, #queue-req: 738, 
[1,0]<stdout>:[2025-10-11 22:11:45 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 2048, #cached-token: 2, token usage: 0.09, #running-req: 31, #queue-req: 714, 
[1,0]<stdout>:[2025-10-11 22:11:46 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 1146, #cached-token: 0, token usage: 0.10, #running-req: 31, #queue-req: 714, 
[1,0]<stdout>:[2025-10-11 22:11:46 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 144, #cached-token: 1, token usage: 0.10, #running-req: 31, #queue-req: 713, 
[1,1]<stdout>:[2025-10-11 22:11:46 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 6, #cached-token: 2, token usage: 0.09, #running-req: 31, #queue-req: 737, 
[1,0]<stdout>:[2025-10-11 22:11:46 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 134, #cached-token: 8, token usage: 0.10, #running-req: 31, #queue-req: 712, 
[1,0]<stdout>:[2025-10-11 22:11:47 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 15, #cached-token: 1, token usage: 0.10, #running-req: 31, #queue-req: 711, 
[1,0]<stdout>:[2025-10-11 22:11:47 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 62, #cached-token: 4, token usage: 0.10, #running-req: 31, #queue-req: 710, 
[1,0]<stdout>:[2025-10-11 22:11:47 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 257, #cached-token: 7, token usage: 0.10, #running-req: 31, #queue-req: 709, 
[1,0]<stdout>:[2025-10-11 22:11:48 DP0 TP0] Decode batch. #running-req: 32, #token: 16901, token usage: 0.10, accept len: 1.00, cuda graph: True, gen throughput (token/s): 340.73, #queue-req: 709, 
[1,1]<stdout>:[2025-10-11 22:11:48 DP1 TP8] Decode batch. #running-req: 31, #token: 15629, token usage: 0.09, accept len: 1.00, cuda graph: True, gen throughput (token/s): 341.23, #queue-req: 737, 
[1,1]<stdout>:[2025-10-11 22:11:48 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 766, #cached-token: 2, token usage: 0.09, #running-req: 31, #queue-req: 736, 
[1,1]<stdout>:[2025-10-11 22:11:48 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 12, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 735, 
[1,0]<stdout>:[2025-10-11 22:11:48 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 374, #cached-token: 6, token usage: 0.10, #running-req: 31, #queue-req: 708, 
[1,1]<stdout>:[2025-10-11 22:11:48 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 6, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 734, 
[1,1]<stdout>:[2025-10-11 22:11:49 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 577, #cached-token: 4, token usage: 0.09, #running-req: 31, #queue-req: 733, 
[1,1]<stdout>:[2025-10-11 22:11:49 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 6, #cached-token: 3, token usage: 0.09, #running-req: 31, #queue-req: 732, 
[1,1]<stdout>:[2025-10-11 22:11:49 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 40, #cached-token: 3, token usage: 0.09, #running-req: 31, #queue-req: 731, 
[1,0]<stdout>:[2025-10-11 22:11:50 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 246, #cached-token: 2, token usage: 0.10, #running-req: 31, #queue-req: 707, 
[1,0]<stdout>:[2025-10-11 22:11:50 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 219, #cached-token: 1, token usage: 0.10, #running-req: 31, #queue-req: 706, 
[1,0]<stdout>:[2025-10-11 22:11:50 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 9, #cached-token: 1, token usage: 0.10, #running-req: 31, #queue-req: 705, 
[1,1]<stdout>:[2025-10-11 22:11:51 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 678, #cached-token: 4, token usage: 0.08, #running-req: 31, #queue-req: 730, 
[1,0]<stdout>:[2025-10-11 22:11:51 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 786, #cached-token: 8, token usage: 0.09, #running-req: 31, #queue-req: 704, 
[1,0]<stdout>:[2025-10-11 22:11:51 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 357, #cached-token: 5, token usage: 0.10, #running-req: 31, #queue-req: 703, 
[1,0]<stdout>:[2025-10-11 22:11:51 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 43, #cached-token: 14, token usage: 0.10, #running-req: 31, #queue-req: 702, 
[1,1]<stdout>:[2025-10-11 22:11:52 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 44, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 729, 
[1,0]<stdout>:[2025-10-11 22:11:52 DP0 TP0] Decode batch. #running-req: 32, #token: 16959, token usage: 0.10, accept len: 1.00, cuda graph: True, gen throughput (token/s): 329.54, #queue-req: 702, 
[1,1]<stdout>:[2025-10-11 22:11:52 DP1 TP8] Decode batch. #running-req: 32, #token: 13071, token usage: 0.08, accept len: 1.00, cuda graph: True, gen throughput (token/s): 329.54, #queue-req: 729, 
[1,1]<stdout>:[2025-10-11 22:11:52 DP1 TP8] Prefill batch. #new-seq: 2, #new-token: 1091, #cached-token: 6, token usage: 0.08, #running-req: 30, #queue-req: 727, 
[1,0]<stdout>:[2025-10-11 22:11:52 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 19, #cached-token: 3, token usage: 0.10, #running-req: 31, #queue-req: 701, 
[1,1]<stdout>:[2025-10-11 22:11:52 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 6, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 726, 
[1,0]<stdout>:[2025-10-11 22:11:52 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 16, #cached-token: 2, token usage: 0.10, #running-req: 31, #queue-req: 700, 
[1,0]<stdout>:[2025-10-11 22:11:53 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 356, #cached-token: 2, token usage: 0.10, #running-req: 31, #queue-req: 699, 
[1,1]<stdout>:[2025-10-11 22:11:53 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 376, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 725, 
[1,1]<stdout>:[2025-10-11 22:11:53 DP1 TP8] Prefill batch. #new-seq: 2, #new-token: 1975, #cached-token: 2, token usage: 0.08, #running-req: 30, #queue-req: 723, 
[1,1]<stdout>:[2025-10-11 22:11:54 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 160, #cached-token: 7, token usage: 0.08, #running-req: 31, #queue-req: 722, 
[1,0]<stdout>:[2025-10-11 22:11:54 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 930, #cached-token: 3, token usage: 0.10, #running-req: 31, #queue-req: 698, 
[1,0]<stdout>:[2025-10-11 22:11:54 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 5, #cached-token: 3, token usage: 0.10, #running-req: 31, #queue-req: 697, 
[1,0]<stdout>:[2025-10-11 22:11:55 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 825, #cached-token: 4, token usage: 0.10, #running-req: 31, #queue-req: 696, 
[1,0]<stdout>:[2025-10-11 22:11:55 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 420, #cached-token: 3, token usage: 0.10, #running-req: 31, #queue-req: 695, 
[1,1]<stdout>:[2025-10-11 22:11:55 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 405, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 721, 
[1,0]<stdout>:[2025-10-11 22:11:55 DP0 TP0] Prefill batch. #new-seq: 2, #new-token: 409, #cached-token: 2, token usage: 0.10, #running-req: 30, #queue-req: 693, 
[1,1]<stdout>:[2025-10-11 22:11:55 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 116, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 720, 
[1,1]<stdout>:[2025-10-11 22:11:56 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 127, #cached-token: 2, token usage: 0.08, #running-req: 31, #queue-req: 719, 
[1,0]<stdout>:[2025-10-11 22:11:56 DP0 TP0] Decode batch. #running-req: 32, #token: 17503, token usage: 0.10, accept len: 1.00, cuda graph: True, gen throughput (token/s): 304.25, #queue-req: 693, 
[1,1]<stdout>:[2025-10-11 22:11:56 DP1 TP8] Decode batch. #running-req: 31, #token: 13769, token usage: 0.08, accept len: 1.00, cuda graph: True, gen throughput (token/s): 303.77, #queue-req: 719, 
[1,1]<stdout>:[2025-10-11 22:11:56 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 662, #cached-token: 3, token usage: 0.08, #running-req: 31, #queue-req: 718, 
[1,0]<stdout>:[2025-10-11 22:11:56 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 398, #cached-token: 5, token usage: 0.10, #running-req: 31, #queue-req: 692, 
[1,1]<stdout>:[2025-10-11 22:11:56 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 1292, #cached-token: 2, token usage: 0.08, #running-req: 31, #queue-req: 717, 
[1,1]<stdout>:[2025-10-11 22:11:57 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 9, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 716, 
[1,0]<stdout>:[2025-10-11 22:11:57 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 610, #cached-token: 3, token usage: 0.10, #running-req: 31, #queue-req: 691, 
[1,0]<stdout>:[2025-10-11 22:11:57 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 958, #cached-token: 4, token usage: 0.11, #running-req: 31, #queue-req: 690, 
[1,1]<stdout>:[2025-10-11 22:11:57 DP1 TP8] Prefill batch. #new-seq: 2, #new-token: 433, #cached-token: 3, token usage: 0.09, #running-req: 30, #queue-req: 714, 
[1,1]<stdout>:[2025-10-11 22:11:58 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 808, #cached-token: 5, token usage: 0.09, #running-req: 31, #queue-req: 713, 
[1,1]<stdout>:[2025-10-11 22:11:58 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 154, #cached-token: 2, token usage: 0.09, #running-req: 31, #queue-req: 712, 
[1,1]<stdout>:[2025-10-11 22:11:58 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 694, #cached-token: 3, token usage: 0.09, #running-req: 31, #queue-req: 711, 
[1,0]<stdout>:[2025-10-11 22:11:58 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 245, #cached-token: 2, token usage: 0.11, #running-req: 31, #queue-req: 689, 
[1,1]<stdout>:[2025-10-11 22:11:59 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 53, #cached-token: 3, token usage: 0.10, #running-req: 31, #queue-req: 710, 
[1,1]<stdout>:[2025-10-11 22:11:59 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 106, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 709, 
[1,0]<stdout>:[2025-10-11 22:11:59 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 769, #cached-token: 1, token usage: 0.10, #running-req: 31, #queue-req: 688, 
[1,0]<stdout>:[2025-10-11 22:11:59 DP0 TP0] Prefill batch. #new-seq: 2, #new-token: 348, #cached-token: 5, token usage: 0.10, #running-req: 30, #queue-req: 686, 
[1,1]<stdout>:[2025-10-11 22:12:00 DP1 TP8] Decode batch. #running-req: 31, #token: 15375, token usage: 0.09, accept len: 1.00, cuda graph: True, gen throughput (token/s): 336.46, #queue-req: 709, 
[1,0]<stdout>:[2025-10-11 22:12:00 DP0 TP0] Decode batch. #running-req: 32, #token: 18161, token usage: 0.11, accept len: 1.00, cuda graph: True, gen throughput (token/s): 337.22, #queue-req: 686, 
[1,1]<stdout>:[2025-10-11 22:12:00 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 708, 
[1,0]<stdout>:[2025-10-11 22:12:00 DP0 TP0] Prefill batch. #new-seq: 2, #new-token: 143, #cached-token: 4, token usage: 0.10, #running-req: 30, #queue-req: 684, 
[1,1]<stdout>:[2025-10-11 22:12:00 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 25, #cached-token: 3, token usage: 0.08, #running-req: 31, #queue-req: 707, 
[1,1]<stdout>:[2025-10-11 22:12:01 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 310, #cached-token: 3, token usage: 0.08, #running-req: 31, #queue-req: 706, 
[1,0]<stdout>:[2025-10-11 22:12:01 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 931, #cached-token: 6, token usage: 0.10, #running-req: 31, #queue-req: 683, 
[1,1]<stdout>:[2025-10-11 22:12:01 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 8, #cached-token: 4, token usage: 0.09, #running-req: 31, #queue-req: 705, 
[1,0]<stdout>:[2025-10-11 22:12:01 DP0 TP0] Prefill batch. #new-seq: 2, #new-token: 217, #cached-token: 4, token usage: 0.10, #running-req: 30, #queue-req: 681, 
[1,0]<stdout>:[2025-10-11 22:12:01 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 239, #cached-token: 6, token usage: 0.10, #running-req: 31, #queue-req: 680, 
[1,0]<stdout>:[2025-10-11 22:12:02 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 24, #cached-token: 1, token usage: 0.10, #running-req: 31, #queue-req: 679, 
[1,1]<stdout>:[2025-10-11 22:12:02 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 12, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 704, 
[1,0]<stdout>:[2025-10-11 22:12:02 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 328, #cached-token: 1, token usage: 0.10, #running-req: 31, #queue-req: 678, 
[1,1]<stdout>:[2025-10-11 22:12:03 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 10, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 703, 
[1,0]<stdout>:[2025-10-11 22:12:03 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 908, #cached-token: 5, token usage: 0.10, #running-req: 31, #queue-req: 677, 
[1,1]<stdout>:[2025-10-11 22:12:03 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 642, #cached-token: 4, token usage: 0.08, #running-req: 31, #queue-req: 702, 
[1,0]<stdout>:[2025-10-11 22:12:04 DP0 TP0] Decode batch. #running-req: 32, #token: 18159, token usage: 0.11, accept len: 1.00, cuda graph: True, gen throughput (token/s): 323.71, #queue-req: 677, 
[1,1]<stdout>:[2025-10-11 22:12:04 DP1 TP8] Decode batch. #running-req: 32, #token: 13542, token usage: 0.08, accept len: 1.00, cuda graph: True, gen throughput (token/s): 324.42, #queue-req: 702, 
[1,1]<stdout>:[2025-10-11 22:12:04 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 805, #cached-token: 4, token usage: 0.08, #running-req: 31, #queue-req: 701, 
[1,0]<stdout>:[2025-10-11 22:12:04 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 334, #cached-token: 3, token usage: 0.11, #running-req: 31, #queue-req: 676, 
[1,1]<stdout>:[2025-10-11 22:12:04 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 468, #cached-token: 2, token usage: 0.09, #running-req: 31, #queue-req: 700, 
[1,1]<stdout>:[2025-10-11 22:12:05 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 707, #cached-token: 6, token usage: 0.09, #running-req: 31, #queue-req: 699, 
[1,0]<stdout>:[2025-10-11 22:12:05 DP0 TP0] Prefill batch. #new-seq: 2, #new-token: 27, #cached-token: 5, token usage: 0.10, #running-req: 30, #queue-req: 674, 
[1,0]<stdout>:[2025-10-11 22:12:05 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 4, #cached-token: 4, token usage: 0.10, #running-req: 31, #queue-req: 673, 
[1,1]<stdout>:[2025-10-11 22:12:06 DP1 TP8] Prefill batch. #new-seq: 2, #new-token: 556, #cached-token: 6, token usage: 0.08, #running-req: 30, #queue-req: 697, 
[1,0]<stdout>:[2025-10-11 22:12:06 DP0 TP0] Prefill batch. #new-seq: 2, #new-token: 700, #cached-token: 9, token usage: 0.10, #running-req: 30, #queue-req: 671, 
[1,1]<stdout>:[2025-10-11 22:12:06 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 208, #cached-token: 3, token usage: 0.08, #running-req: 31, #queue-req: 696, 
[1,0]<stdout>:[2025-10-11 22:12:07 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 765, #cached-token: 1, token usage: 0.10, #running-req: 31, #queue-req: 670, 
[1,0]<stdout>:[2025-10-11 22:12:07 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 3, #cached-token: 3, token usage: 0.10, #running-req: 31, #queue-req: 669, 
[1,1]<stdout>:[2025-10-11 22:12:07 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 15, #cached-token: 3, token usage: 0.08, #running-req: 31, #queue-req: 695, 
[1,0]<stdout>:[2025-10-11 22:12:07 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 8, #cached-token: 1, token usage: 0.10, #running-req: 31, #queue-req: 668, 
[1,1]<stdout>:[2025-10-11 22:12:07 DP1 TP8] Decode batch. #running-req: 31, #token: 14009, token usage: 0.08, accept len: 1.00, cuda graph: True, gen throughput (token/s): 343.33, #queue-req: 695, 
[1,0]<stdout>:[2025-10-11 22:12:07 DP0 TP0] Decode batch. #running-req: 32, #token: 16512, token usage: 0.10, accept len: 1.00, cuda graph: True, gen throughput (token/s): 342.99, #queue-req: 668, 
[1,1]<stdout>:[2025-10-11 22:12:07 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 2, #cached-token: 2, token usage: 0.08, #running-req: 31, #queue-req: 694, 
[1,0]<stdout>:[2025-10-11 22:12:08 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 211, #cached-token: 5, token usage: 0.09, #running-req: 31, #queue-req: 667, 
[1,0]<stdout>:[2025-10-11 22:12:08 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 29, #cached-token: 3, token usage: 0.09, #running-req: 31, #queue-req: 666, 
[1,1]<stdout>:[2025-10-11 22:12:08 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 190, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 693, 
[1,0]<stdout>:[2025-10-11 22:12:08 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 115, #cached-token: 3, token usage: 0.09, #running-req: 31, #queue-req: 665, 
[1,0]<stdout>:[2025-10-11 22:12:09 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 10, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 664, 
[1,0]<stdout>:[2025-10-11 22:12:09 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 5, #cached-token: 4, token usage: 0.09, #running-req: 31, #queue-req: 663, 
[1,1]<stdout>:[2025-10-11 22:12:10 DP1 TP8] Prefill batch. #new-seq: 2, #new-token: 608, #cached-token: 6, token usage: 0.08, #running-req: 30, #queue-req: 691, 
[1,0]<stdout>:[2025-10-11 22:12:10 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 8, #cached-token: 2, token usage: 0.09, #running-req: 31, #queue-req: 662, 
[1,0]<stdout>:[2025-10-11 22:12:10 DP0 TP0] Decode batch. #running-req: 31, #token: 14282, token usage: 0.08, accept len: 1.00, cuda graph: True, gen throughput (token/s): 403.98, #queue-req: 662, 
[1,1]<stdout>:[2025-10-11 22:12:10 DP1 TP8] Decode batch. #running-req: 32, #token: 14897, token usage: 0.09, accept len: 1.00, cuda graph: True, gen throughput (token/s): 405.21, #queue-req: 691, 
[1,0]<stdout>:[2025-10-11 22:12:10 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 189, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 661, 
[1,1]<stdout>:[2025-10-11 22:12:11 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 57, #cached-token: 2, token usage: 0.09, #running-req: 31, #queue-req: 690, 
[1,1]<stdout>:[2025-10-11 22:12:11 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 3, #cached-token: 3, token usage: 0.08, #running-req: 31, #queue-req: 689, 
[1,0]<stdout>:[2025-10-11 22:12:12 DP0 TP0] Prefill batch. #new-seq: 2, #new-token: 463, #cached-token: 3, token usage: 0.09, #running-req: 30, #queue-req: 659, 
[1,0]<stdout>:[2025-10-11 22:12:12 DP0 TP0] Prefill batch. #new-seq: 2, #new-token: 807, #cached-token: 4, token usage: 0.09, #running-req: 30, #queue-req: 657, 
[1,1]<stdout>:[2025-10-11 22:12:13 DP1 TP8] Prefill batch. #new-seq: 2, #new-token: 1036, #cached-token: 7, token usage: 0.07, #running-req: 30, #queue-req: 687, 
[1,0]<stdout>:[2025-10-11 22:12:13 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 72, #cached-token: 2, token usage: 0.09, #running-req: 31, #queue-req: 656, 
[1,1]<stdout>:[2025-10-11 22:12:13 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 189, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 686, 
[1,1]<stdout>:[2025-10-11 22:12:13 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 171, #cached-token: 3, token usage: 0.08, #running-req: 31, #queue-req: 685, 
[1,1]<stdout>:[2025-10-11 22:12:14 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 885, #cached-token: 6, token usage: 0.08, #running-req: 31, #queue-req: 684, 
[1,0]<stdout>:[2025-10-11 22:12:14 DP0 TP0] Decode batch. #running-req: 32, #token: 16374, token usage: 0.10, accept len: 1.00, cuda graph: True, gen throughput (token/s): 386.06, #queue-req: 656, 
[1,1]<stdout>:[2025-10-11 22:12:14 DP1 TP8] Decode batch. #running-req: 32, #token: 13750, token usage: 0.08, accept len: 1.00, cuda graph: True, gen throughput (token/s): 385.46, #queue-req: 684, 
[1,0]<stdout>:[2025-10-11 22:12:14 DP0 TP0] Prefill batch. #new-seq: 3, #new-token: 1099, #cached-token: 7, token usage: 0.09, #running-req: 29, #queue-req: 653, 
[1,1]<stdout>:[2025-10-11 22:12:14 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 428, #cached-token: 2, token usage: 0.08, #running-req: 31, #queue-req: 683, 
[1,0]<stdout>:[2025-10-11 22:12:15 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 7, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 652, 
[1,1]<stdout>:[2025-10-11 22:12:15 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 173, #cached-token: 2, token usage: 0.08, #running-req: 31, #queue-req: 682, 
[1,0]<stdout>:[2025-10-11 22:12:15 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 380, #cached-token: 3, token usage: 0.09, #running-req: 31, #queue-req: 651, 
[1,1]<stdout>:[2025-10-11 22:12:15 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 366, #cached-token: 6, token usage: 0.08, #running-req: 31, #queue-req: 681, 
[1,0]<stdout>:[2025-10-11 22:12:16 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 390, #cached-token: 6, token usage: 0.10, #running-req: 31, #queue-req: 650, 
[1,1]<stdout>:[2025-10-11 22:12:16 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 7, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 680, 
[1,0]<stdout>:[2025-10-11 22:12:16 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 754, #cached-token: 1, token usage: 0.10, #running-req: 31, #queue-req: 649, 
[1,1]<stdout>:[2025-10-11 22:12:16 DP1 TP8] Prefill batch. #new-seq: 2, #new-token: 851, #cached-token: 8, token usage: 0.07, #running-req: 30, #queue-req: 678, 
[1,1]<stdout>:[2025-10-11 22:12:17 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 5, #cached-token: 3, token usage: 0.08, #running-req: 31, #queue-req: 677, 
[1,0]<stdout>:[2025-10-11 22:12:17 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 290, #cached-token: 2, token usage: 0.10, #running-req: 31, #queue-req: 648, 
[1,0]<stdout>:[2025-10-11 22:12:17 DP0 TP0] Prefill batch. #new-seq: 2, #new-token: 1030, #cached-token: 5, token usage: 0.09, #running-req: 30, #queue-req: 646, 
[1,1]<stdout>:[2025-10-11 22:12:17 DP1 TP8] Decode batch. #running-req: 32, #token: 13118, token usage: 0.08, accept len: 1.00, cuda graph: True, gen throughput (token/s): 366.58, #queue-req: 677, 
[1,0]<stdout>:[2025-10-11 22:12:17 DP0 TP0] Decode batch. #running-req: 31, #token: 15937, token usage: 0.09, accept len: 1.00, cuda graph: True, gen throughput (token/s): 365.38, #queue-req: 646, 
[1,0]<stdout>:[2025-10-11 22:12:17 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 8, #cached-token: 3, token usage: 0.09, #running-req: 31, #queue-req: 645, 
[1,0]<stdout>:[2025-10-11 22:12:17 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 131, #cached-token: 4, token usage: 0.09, #running-req: 31, #queue-req: 644, 
[1,0]<stdout>:[2025-10-11 22:12:18 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 5, #cached-token: 3, token usage: 0.09, #running-req: 31, #queue-req: 643, 
[1,1]<stdout>:[2025-10-11 22:12:18 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 43, #cached-token: 3, token usage: 0.08, #running-req: 31, #queue-req: 676, 
[1,0]<stdout>:[2025-10-11 22:12:18 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 243, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 642, 
[1,1]<stdout>:[2025-10-11 22:12:18 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 636, #cached-token: 3, token usage: 0.08, #running-req: 31, #queue-req: 675, 
[1,0]<stdout>:[2025-10-11 22:12:19 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 101, #cached-token: 6, token usage: 0.09, #running-req: 31, #queue-req: 641, 
[1,1]<stdout>:[2025-10-11 22:12:19 DP1 TP8] Prefill batch. #new-seq: 2, #new-token: 15, #cached-token: 5, token usage: 0.08, #running-req: 30, #queue-req: 673, 
[1,0]<stdout>:[2025-10-11 22:12:19 DP0 TP0] Prefill batch. #new-seq: 2, #new-token: 968, #cached-token: 3, token usage: 0.08, #running-req: 30, #queue-req: 639, 
[1,0]<stdout>:[2025-10-11 22:12:20 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 123, #cached-token: 2, token usage: 0.09, #running-req: 31, #queue-req: 638, 
[1,0]<stdout>:[2025-10-11 22:12:20 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 862, #cached-token: 5, token usage: 0.09, #running-req: 31, #queue-req: 637, 
[1,0]<stdout>:[2025-10-11 22:12:20 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 14, #cached-token: 2, token usage: 0.09, #running-req: 31, #queue-req: 636, 
[1,1]<stdout>:[2025-10-11 22:12:20 DP1 TP8] Prefill batch. #new-seq: 2, #new-token: 884, #cached-token: 7, token usage: 0.08, #running-req: 30, #queue-req: 671, 
[1,1]<stdout>:[2025-10-11 22:12:20 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 185, #cached-token: 2, token usage: 0.08, #running-req: 31, #queue-req: 670, 
[1,0]<stdout>:[2025-10-11 22:12:21 DP0 TP0] Prefill batch. #new-seq: 2, #new-token: 42, #cached-token: 3, token usage: 0.09, #running-req: 30, #queue-req: 634, 
[1,1]<stdout>:[2025-10-11 22:12:21 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 255, #cached-token: 4, token usage: 0.08, #running-req: 31, #queue-req: 669, 
[1,1]<stdout>:[2025-10-11 22:12:21 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 15, #cached-token: 6, token usage: 0.08, #running-req: 31, #queue-req: 668, 
[1,1]<stdout>:[2025-10-11 22:12:22 DP1 TP8] Decode batch. #running-req: 32, #token: 14076, token usage: 0.08, accept len: 1.00, cuda graph: True, gen throughput (token/s): 295.51, #queue-req: 668, 
[1,0]<stdout>:[2025-10-11 22:12:22 DP0 TP0] Decode batch. #running-req: 32, #token: 15141, token usage: 0.09, accept len: 1.00, cuda graph: True, gen throughput (token/s): 295.07, #queue-req: 634, 
[1,1]<stdout>:[2025-10-11 22:12:22 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 113, #cached-token: 7, token usage: 0.08, #running-req: 31, #queue-req: 667, 
[1,0]<stdout>:[2025-10-11 22:12:22 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 2048, #cached-token: 3, token usage: 0.09, #running-req: 31, #queue-req: 633, 
[1,0]<stdout>:[2025-10-11 22:12:22 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 2048, #cached-token: 0, token usage: 0.10, #running-req: 31, #queue-req: 633, 
[1,0]<stdout>:[2025-10-11 22:12:22 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 2048, #cached-token: 0, token usage: 0.11, #running-req: 31, #queue-req: 633, 
[1,0]<stdout>:[2025-10-11 22:12:22 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 257, #cached-token: 0, token usage: 0.12, #running-req: 31, #queue-req: 633, 
[1,0]<stdout>:[2025-10-11 22:12:23 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 1, token usage: 0.13, #running-req: 31, #queue-req: 632, 
[1,1]<stdout>:[2025-10-11 22:12:23 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 710, #cached-token: 5, token usage: 0.08, #running-req: 31, #queue-req: 666, 
[1,1]<stdout>:[2025-10-11 22:12:23 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 12, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 665, 
[1,0]<stdout>:[2025-10-11 22:12:23 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 253, #cached-token: 1, token usage: 0.12, #running-req: 31, #queue-req: 631, 
[1,0]<stdout>:[2025-10-11 22:12:24 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 107, #cached-token: 4, token usage: 0.12, #running-req: 31, #queue-req: 630, 
[1,0]<stdout>:[2025-10-11 22:12:25 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 283, #cached-token: 3, token usage: 0.12, #running-req: 31, #queue-req: 629, 
[1,0]<stdout>:[2025-10-11 22:12:25 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 9, #cached-token: 3, token usage: 0.13, #running-req: 31, #queue-req: 628, 
[1,0]<stdout>:[2025-10-11 22:12:25 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 1966, #cached-token: 3, token usage: 0.13, #running-req: 31, #queue-req: 627, 
[1,0]<stdout>:[2025-10-11 22:12:25 DP0 TP0] Decode batch. #running-req: 32, #token: 23384, token usage: 0.14, accept len: 1.00, cuda graph: True, gen throughput (token/s): 347.91, #queue-req: 627, 
[1,1]<stdout>:[2025-10-11 22:12:25 DP1 TP8] Decode batch. #running-req: 31, #token: 14398, token usage: 0.08, accept len: 1.00, cuda graph: True, gen throughput (token/s): 348.73, #queue-req: 665, 
[1,1]<stdout>:[2025-10-11 22:12:25 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 9, #cached-token: 4, token usage: 0.08, #running-req: 31, #queue-req: 664, 
[1,1]<stdout>:[2025-10-11 22:12:25 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 19, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 663, 
[1,0]<stdout>:[2025-10-11 22:12:26 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 201, #cached-token: 3, token usage: 0.13, #running-req: 31, #queue-req: 626, 
[1,0]<stdout>:[2025-10-11 22:12:26 DP0 TP0] Prefill batch. #new-seq: 2, #new-token: 556, #cached-token: 5, token usage: 0.13, #running-req: 30, #queue-req: 624, 
[1,1]<stdout>:[2025-10-11 22:12:26 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 428, #cached-token: 2, token usage: 0.08, #running-req: 31, #queue-req: 662, 
[1,1]<stdout>:[2025-10-11 22:12:27 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 248, #cached-token: 2, token usage: 0.09, #running-req: 31, #queue-req: 661, 
[1,1]<stdout>:[2025-10-11 22:12:27 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 407, #cached-token: 5, token usage: 0.09, #running-req: 31, #queue-req: 660, 
[1,0]<stdout>:[2025-10-11 22:12:27 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 525, #cached-token: 6, token usage: 0.13, #running-req: 31, #queue-req: 623, 
[1,1]<stdout>:[2025-10-11 22:12:27 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 353, #cached-token: 2, token usage: 0.09, #running-req: 31, #queue-req: 659, 
[1,1]<stdout>:[2025-10-11 22:12:28 DP1 TP8] Prefill batch. #new-seq: 2, #new-token: 266, #cached-token: 4, token usage: 0.08, #running-req: 30, #queue-req: 657, 
[1,1]<stdout>:[2025-10-11 22:12:28 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 277, #cached-token: 2, token usage: 0.08, #running-req: 31, #queue-req: 656, 
[1,0]<stdout>:[2025-10-11 22:12:28 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 10, #cached-token: 4, token usage: 0.13, #running-req: 31, #queue-req: 622, 
[1,1]<stdout>:[2025-10-11 22:12:28 DP1 TP8] Prefill batch. #new-seq: 2, #new-token: 893, #cached-token: 3, token usage: 0.07, #running-req: 30, #queue-req: 654, 
[1,0]<stdout>:[2025-10-11 22:12:29 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 2048, #cached-token: 1, token usage: 0.13, #running-req: 31, #queue-req: 621, 
[1,0]<stdout>:[2025-10-11 22:12:29 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 202, #cached-token: 0, token usage: 0.15, #running-req: 31, #queue-req: 621, 
[1,0]<stdout>:[2025-10-11 22:12:29 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 1012, #cached-token: 1, token usage: 0.15, #running-req: 31, #queue-req: 620, 
[1,0]<stdout>:[2025-10-11 22:12:29 DP0 TP0] Decode batch. #running-req: 32, #token: 25887, token usage: 0.15, accept len: 1.00, cuda graph: True, gen throughput (token/s): 302.22, #queue-req: 620, 
[1,1]<stdout>:[2025-10-11 22:12:29 DP1 TP8] Decode batch. #running-req: 31, #token: 13221, token usage: 0.08, accept len: 1.00, cuda graph: True, gen throughput (token/s): 301.25, #queue-req: 654, 
[1,1]<stdout>:[2025-10-11 22:12:29 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 291, #cached-token: 3, token usage: 0.08, #running-req: 31, #queue-req: 653, 
[1,1]<stdout>:[2025-10-11 22:12:30 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 6, #cached-token: 3, token usage: 0.08, #running-req: 31, #queue-req: 652, 
[1,0]<stdout>:[2025-10-11 22:12:30 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 26, #cached-token: 1, token usage: 0.15, #running-req: 31, #queue-req: 619, 
[1,0]<stdout>:[2025-10-11 22:12:30 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 3, #cached-token: 2, token usage: 0.15, #running-req: 31, #queue-req: 618, 
[1,1]<stdout>:[2025-10-11 22:12:30 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 1021, #cached-token: 3, token usage: 0.08, #running-req: 31, #queue-req: 651, 
[1,1]<stdout>:[2025-10-11 22:12:31 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 9, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 650, 
[1,1]<stdout>:[2025-10-11 22:12:31 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 5, #cached-token: 5, token usage: 0.08, #running-req: 31, #queue-req: 649, 
[1,0]<stdout>:[2025-10-11 22:12:31 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 35, #cached-token: 3, token usage: 0.15, #running-req: 31, #queue-req: 617, 
[1,0]<stdout>:[2025-10-11 22:12:31 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 1, token usage: 0.15, #running-req: 31, #queue-req: 616, 
[1,1]<stdout>:[2025-10-11 22:12:32 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 2, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 648, 
[1,1]<stdout>:[2025-10-11 22:12:32 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 11, #cached-token: 3, token usage: 0.07, #running-req: 31, #queue-req: 647, 
[1,0]<stdout>:[2025-10-11 22:12:32 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 303, #cached-token: 4, token usage: 0.15, #running-req: 31, #queue-req: 615, 
[1,1]<stdout>:[2025-10-11 22:12:32 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 35, #cached-token: 1, token usage: 0.07, #running-req: 31, #queue-req: 646, 
[1,0]<stdout>:[2025-10-11 22:12:33 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 328, #cached-token: 2, token usage: 0.15, #running-req: 31, #queue-req: 614, 
[1,0]<stdout>:[2025-10-11 22:12:33 DP0 TP0] Prefill batch. #new-seq: 2, #new-token: 534, #cached-token: 5, token usage: 0.15, #running-req: 30, #queue-req: 612, 
[1,0]<stdout>:[2025-10-11 22:12:33 DP0 TP0] Decode batch. #running-req: 32, #token: 25652, token usage: 0.15, accept len: 1.00, cuda graph: True, gen throughput (token/s): 341.27, #queue-req: 612, 
[1,1]<stdout>:[2025-10-11 22:12:33 DP1 TP8] Decode batch. #running-req: 32, #token: 12973, token usage: 0.08, accept len: 1.00, cuda graph: True, gen throughput (token/s): 341.56, #queue-req: 646, 
[1,1]<stdout>:[2025-10-11 22:12:33 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 1386, #cached-token: 5, token usage: 0.08, #running-req: 31, #queue-req: 645, 
[1,0]<stdout>:[2025-10-11 22:12:34 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 763, #cached-token: 2, token usage: 0.15, #running-req: 31, #queue-req: 611, 
[1,0]<stdout>:[2025-10-11 22:12:34 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 28, #cached-token: 1, token usage: 0.15, #running-req: 31, #queue-req: 610, 
[1,1]<stdout>:[2025-10-11 22:12:34 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 332, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 644, 
[1,1]<stdout>:[2025-10-11 22:12:35 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 25, #cached-token: 4, token usage: 0.09, #running-req: 31, #queue-req: 643, 
[1,0]<stdout>:[2025-10-11 22:12:35 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 414, #cached-token: 2, token usage: 0.15, #running-req: 31, #queue-req: 609, 
[1,1]<stdout>:[2025-10-11 22:12:36 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 708, #cached-token: 3, token usage: 0.09, #running-req: 31, #queue-req: 642, 
[1,0]<stdout>:[2025-10-11 22:12:36 DP0 TP0] Decode batch. #running-req: 32, #token: 26451, token usage: 0.16, accept len: 1.00, cuda graph: True, gen throughput (token/s): 435.87, #queue-req: 609, 
[1,1]<stdout>:[2025-10-11 22:12:36 DP1 TP8] Decode batch. #running-req: 32, #token: 15753, token usage: 0.09, accept len: 1.00, cuda graph: True, gen throughput (token/s): 435.49, #queue-req: 642, 
[1,0]<stdout>:[2025-10-11 22:12:36 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 6, #cached-token: 1, token usage: 0.16, #running-req: 31, #queue-req: 608, 
[1,0]<stdout>:[2025-10-11 22:12:37 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 3, #cached-token: 2, token usage: 0.15, #running-req: 31, #queue-req: 607, 
[1,0]<stdout>:[2025-10-11 22:12:37 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 16, #cached-token: 2, token usage: 0.15, #running-req: 31, #queue-req: 606, 
[1,0]<stdout>:[2025-10-11 22:12:37 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 469, #cached-token: 2, token usage: 0.15, #running-req: 31, #queue-req: 605, 
[1,1]<stdout>:[2025-10-11 22:12:37 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 249, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 641, 
[1,1]<stdout>:[2025-10-11 22:12:38 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 373, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 640, 
[1,1]<stdout>:[2025-10-11 22:12:38 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 58, #cached-token: 4, token usage: 0.09, #running-req: 31, #queue-req: 639, 
[1,0]<stdout>:[2025-10-11 22:12:38 DP0 TP0] Prefill batch. #new-seq: 2, #new-token: 786, #cached-token: 5, token usage: 0.13, #running-req: 30, #queue-req: 603, 
[1,0]<stdout>:[2025-10-11 22:12:38 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 158, #cached-token: 1, token usage: 0.13, #running-req: 31, #queue-req: 602, 
[1,1]<stdout>:[2025-10-11 22:12:39 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 292, #cached-token: 3, token usage: 0.09, #running-req: 31, #queue-req: 638, 
[1,0]<stdout>:[2025-10-11 22:12:39 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 7, #cached-token: 3, token usage: 0.13, #running-req: 31, #queue-req: 601, 
[1,0]<stdout>:[2025-10-11 22:12:39 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 350, #cached-token: 1, token usage: 0.13, #running-req: 31, #queue-req: 600, 
[1,0]<stdout>:[2025-10-11 22:12:39 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 307, #cached-token: 1, token usage: 0.13, #running-req: 31, #queue-req: 599, 
[1,1]<stdout>:[2025-10-11 22:12:39 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 592, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 637, 
[1,1]<stdout>:[2025-10-11 22:12:40 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 138, #cached-token: 2, token usage: 0.09, #running-req: 31, #queue-req: 636, 
[1,1]<stdout>:[2025-10-11 22:12:40 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 181, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 635, 
[1,0]<stdout>:[2025-10-11 22:12:40 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 97, #cached-token: 8, token usage: 0.13, #running-req: 31, #queue-req: 598, 
[1,0]<stdout>:[2025-10-11 22:12:40 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 223, #cached-token: 3, token usage: 0.13, #running-req: 31, #queue-req: 597, 
[1,1]<stdout>:[2025-10-11 22:12:41 DP1 TP8] Decode batch. #running-req: 32, #token: 15835, token usage: 0.09, accept len: 1.00, cuda graph: True, gen throughput (token/s): 285.84, #queue-req: 635, 
[1,0]<stdout>:[2025-10-11 22:12:41 DP0 TP0] Decode batch. #running-req: 31, #token: 22229, token usage: 0.13, accept len: 1.00, cuda graph: True, gen throughput (token/s): 284.48, #queue-req: 597, 
[1,0]<stdout>:[2025-10-11 22:12:41 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 136, #cached-token: 3, token usage: 0.13, #running-req: 31, #queue-req: 596, 
[1,0]<stdout>:[2025-10-11 22:12:41 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 1119, #cached-token: 1, token usage: 0.13, #running-req: 31, #queue-req: 595, 
[1,1]<stdout>:[2025-10-11 22:12:41 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3, token usage: 0.09, #running-req: 31, #queue-req: 634, 
[1,1]<stdout>:[2025-10-11 22:12:41 DP1 TP8] Prefill batch. #new-seq: 2, #new-token: 707, #cached-token: 6, token usage: 0.09, #running-req: 30, #queue-req: 632, 
[1,1]<stdout>:[2025-10-11 22:12:42 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 38, #cached-token: 2, token usage: 0.09, #running-req: 31, #queue-req: 631, 
[1,1]<stdout>:[2025-10-11 22:12:42 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 630, 
[1,0]<stdout>:[2025-10-11 22:12:42 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 231, #cached-token: 2, token usage: 0.14, #running-req: 31, #queue-req: 594, 
[1,0]<stdout>:[2025-10-11 22:12:42 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 377, #cached-token: 1, token usage: 0.14, #running-req: 31, #queue-req: 593, 
[1,1]<stdout>:[2025-10-11 22:12:42 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 159, #cached-token: 4, token usage: 0.08, #running-req: 31, #queue-req: 629, 
[1,1]<stdout>:[2025-10-11 22:12:43 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 199, #cached-token: 2, token usage: 0.08, #running-req: 31, #queue-req: 628, 
[1,0]<stdout>:[2025-10-11 22:12:43 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 1538, #cached-token: 1, token usage: 0.14, #running-req: 31, #queue-req: 592, 
[1,1]<stdout>:[2025-10-11 22:12:43 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 350, #cached-token: 4, token usage: 0.08, #running-req: 31, #queue-req: 627, 
[1,1]<stdout>:[2025-10-11 22:12:43 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 749, #cached-token: 6, token usage: 0.09, #running-req: 31, #queue-req: 626, 
[1,1]<stdout>:[2025-10-11 22:12:44 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 781, #cached-token: 4, token usage: 0.09, #running-req: 31, #queue-req: 625, 
[1,0]<stdout>:[2025-10-11 22:12:44 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 642, #cached-token: 11, token usage: 0.15, #running-req: 31, #queue-req: 591, 
[1,0]<stdout>:[2025-10-11 22:12:44 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 16, #cached-token: 3, token usage: 0.15, #running-req: 31, #queue-req: 590, 
[1,0]<stdout>:[2025-10-11 22:12:45 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 1474, #cached-token: 1, token usage: 0.15, #running-req: 31, #queue-req: 589, 
[1,1]<stdout>:[2025-10-11 22:12:45 DP1 TP8] Decode batch. #running-req: 32, #token: 15863, token usage: 0.09, accept len: 1.00, cuda graph: True, gen throughput (token/s): 292.05, #queue-req: 625, 
[1,0]<stdout>:[2025-10-11 22:12:45 DP0 TP0] Decode batch. #running-req: 32, #token: 26685, token usage: 0.16, accept len: 1.00, cuda graph: True, gen throughput (token/s): 292.74, #queue-req: 589, 
[1,0]<stdout>:[2025-10-11 22:12:45 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 31, #cached-token: 8, token usage: 0.16, #running-req: 31, #queue-req: 588, 
[1,1]<stdout>:[2025-10-11 22:12:46 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 7, #cached-token: 3, token usage: 0.09, #running-req: 31, #queue-req: 624, 
[1,1]<stdout>:[2025-10-11 22:12:46 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 63, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 623, 
[1,1]<stdout>:[2025-10-11 22:12:46 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 3, #cached-token: 3, token usage: 0.09, #running-req: 31, #queue-req: 622, 
[1,0]<stdout>:[2025-10-11 22:12:47 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 4, #cached-token: 2, token usage: 0.16, #running-req: 31, #queue-req: 587, 
[1,1]<stdout>:[2025-10-11 22:12:47 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 57, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 621, 
[1,1]<stdout>:[2025-10-11 22:12:47 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 21, #cached-token: 3, token usage: 0.09, #running-req: 31, #queue-req: 620, 
[1,1]<stdout>:[2025-10-11 22:12:48 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 44, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 619, 
[1,1]<stdout>:[2025-10-11 22:12:48 DP1 TP8] Decode batch. #running-req: 32, #token: 14885, token usage: 0.09, accept len: 1.00, cuda graph: True, gen throughput (token/s): 422.14, #queue-req: 619, 
[1,0]<stdout>:[2025-10-11 22:12:48 DP0 TP0] Decode batch. #running-req: 32, #token: 27538, token usage: 0.16, accept len: 1.00, cuda graph: True, gen throughput (token/s): 423.45, #queue-req: 587, 
[1,0]<stdout>:[2025-10-11 22:12:48 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 4, #cached-token: 4, token usage: 0.15, #running-req: 31, #queue-req: 586, 
[1,1]<stdout>:[2025-10-11 22:12:49 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 20, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 618, 
[1,0]<stdout>:[2025-10-11 22:12:49 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 3, #cached-token: 1, token usage: 0.15, #running-req: 31, #queue-req: 585, 
[1,1]<stdout>:[2025-10-11 22:12:49 DP1 TP8] Prefill batch. #new-seq: 2, #new-token: 422, #cached-token: 2, token usage: 0.08, #running-req: 30, #queue-req: 616, 
[1,0]<stdout>:[2025-10-11 22:12:50 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 45, #cached-token: 1, token usage: 0.14, #running-req: 31, #queue-req: 584, 
[1,0]<stdout>:[2025-10-11 22:12:50 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 691, #cached-token: 6, token usage: 0.14, #running-req: 31, #queue-req: 583, 
[1,0]<stdout>:[2025-10-11 22:12:51 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 283, #cached-token: 2, token usage: 0.14, #running-req: 31, #queue-req: 582, 
[1,1]<stdout>:[2025-10-11 22:12:51 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 19, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 615, 
[1,1]<stdout>:[2025-10-11 22:12:51 DP1 TP8] Decode batch. #running-req: 31, #token: 14637, token usage: 0.09, accept len: 1.00, cuda graph: True, gen throughput (token/s): 411.59, #queue-req: 615, 
[1,0]<stdout>:[2025-10-11 22:12:51 DP0 TP0] Decode batch. #running-req: 32, #token: 23799, token usage: 0.14, accept len: 1.00, cuda graph: True, gen throughput (token/s): 411.60, #queue-req: 582, 
[1,1]<stdout>:[2025-10-11 22:12:51 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 6, #cached-token: 5, token usage: 0.09, #running-req: 31, #queue-req: 614, 
[1,0]<stdout>:[2025-10-11 22:12:51 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 255, #cached-token: 2, token usage: 0.14, #running-req: 31, #queue-req: 581, 
[1,1]<stdout>:[2025-10-11 22:12:52 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 820, #cached-token: 4, token usage: 0.08, #running-req: 31, #queue-req: 613, 
[1,1]<stdout>:[2025-10-11 22:12:52 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 1894, #cached-token: 6, token usage: 0.09, #running-req: 31, #queue-req: 612, 
[1,1]<stdout>:[2025-10-11 22:12:52 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 10, #cached-token: 4, token usage: 0.09, #running-req: 31, #queue-req: 611, 
[1,1]<stdout>:[2025-10-11 22:12:53 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 148, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 610, 
[1,0]<stdout>:[2025-10-11 22:12:53 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 2048, #cached-token: 1, token usage: 0.14, #running-req: 31, #queue-req: 580, 
[1,0]<stdout>:[2025-10-11 22:12:53 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 375, #cached-token: 0, token usage: 0.15, #running-req: 31, #queue-req: 580, 
[1,1]<stdout>:[2025-10-11 22:12:53 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 653, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 609, 
[1,1]<stdout>:[2025-10-11 22:12:54 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 7, #cached-token: 1, token usage: 0.10, #running-req: 31, #queue-req: 608, 
[1,0]<stdout>:[2025-10-11 22:12:54 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 67, #cached-token: 4, token usage: 0.15, #running-req: 31, #queue-req: 579, 
[1,0]<stdout>:[2025-10-11 22:12:54 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 5, #cached-token: 3, token usage: 0.15, #running-req: 31, #queue-req: 578, 
[1,0]<stdout>:[2025-10-11 22:12:54 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 164, #cached-token: 8, token usage: 0.15, #running-req: 31, #queue-req: 577, 
[1,0]<stdout>:[2025-10-11 22:12:54 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 125, #cached-token: 1, token usage: 0.15, #running-req: 31, #queue-req: 576, 
[1,0]<stdout>:[2025-10-11 22:12:55 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 1, token usage: 0.15, #running-req: 31, #queue-req: 575, 
[1,0]<stdout>:[2025-10-11 22:12:55 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 221, #cached-token: 10, token usage: 0.15, #running-req: 31, #queue-req: 574, 
[1,1]<stdout>:[2025-10-11 22:12:55 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 9, #cached-token: 2, token usage: 0.10, #running-req: 31, #queue-req: 607, 
[1,0]<stdout>:[2025-10-11 22:12:55 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 1734, #cached-token: 1, token usage: 0.15, #running-req: 31, #queue-req: 573, 
[1,1]<stdout>:[2025-10-11 22:12:55 DP1 TP8] Decode batch. #running-req: 31, #token: 16121, token usage: 0.09, accept len: 1.00, cuda graph: True, gen throughput (token/s): 302.61, #queue-req: 607, 
[1,0]<stdout>:[2025-10-11 22:12:55 DP0 TP0] Decode batch. #running-req: 32, #token: 26473, token usage: 0.16, accept len: 1.00, cuda graph: True, gen throughput (token/s): 302.37, #queue-req: 573, 
[1,1]<stdout>:[2025-10-11 22:12:55 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 359, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 606, 
[1,1]<stdout>:[2025-10-11 22:12:55 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 2044, #cached-token: 1, token usage: 0.10, #running-req: 31, #queue-req: 605, 
[1,0]<stdout>:[2025-10-11 22:12:56 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 626, #cached-token: 6, token usage: 0.15, #running-req: 31, #queue-req: 572, 
[1,0]<stdout>:[2025-10-11 22:12:56 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 91, #cached-token: 1, token usage: 0.15, #running-req: 31, #queue-req: 571, 
[1,0]<stdout>:[2025-10-11 22:12:56 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 126, #cached-token: 1, token usage: 0.15, #running-req: 31, #queue-req: 570, 
[1,0]<stdout>:[2025-10-11 22:12:56 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 609, #cached-token: 2, token usage: 0.15, #running-req: 31, #queue-req: 569, 
[1,0]<stdout>:[2025-10-11 22:12:57 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 272, #cached-token: 3, token usage: 0.15, #running-req: 31, #queue-req: 568, 
[1,1]<stdout>:[2025-10-11 22:12:57 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 14, #cached-token: 2, token usage: 0.11, #running-req: 31, #queue-req: 604, 
[1,0]<stdout>:[2025-10-11 22:12:57 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 470, #cached-token: 2, token usage: 0.15, #running-req: 31, #queue-req: 567, 
[1,1]<stdout>:[2025-10-11 22:12:57 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 637, #cached-token: 4, token usage: 0.11, #running-req: 31, #queue-req: 603, 
[1,0]<stdout>:[2025-10-11 22:12:58 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 578, #cached-token: 1, token usage: 0.16, #running-req: 31, #queue-req: 566, 
[1,1]<stdout>:[2025-10-11 22:12:58 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 251, #cached-token: 8, token usage: 0.11, #running-req: 31, #queue-req: 602, 
[1,0]<stdout>:[2025-10-11 22:12:58 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 6, #cached-token: 3, token usage: 0.16, #running-req: 31, #queue-req: 565, 
[1,1]<stdout>:[2025-10-11 22:12:59 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 737, #cached-token: 7, token usage: 0.11, #running-req: 31, #queue-req: 601, 
[1,1]<stdout>:[2025-10-11 22:12:59 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 13, #cached-token: 3, token usage: 0.11, #running-req: 31, #queue-req: 600, 
[1,0]<stdout>:[2025-10-11 22:12:59 DP0 TP0] Decode batch. #running-req: 32, #token: 26829, token usage: 0.16, accept len: 1.00, cuda graph: True, gen throughput (token/s): 315.68, #queue-req: 565, 
[1,1]<stdout>:[2025-10-11 22:12:59 DP1 TP8] Decode batch. #running-req: 31, #token: 18910, token usage: 0.11, accept len: 1.00, cuda graph: True, gen throughput (token/s): 315.92, #queue-req: 600, 
[1,1]<stdout>:[2025-10-11 22:12:59 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 392, #cached-token: 7, token usage: 0.11, #running-req: 31, #queue-req: 599, 
[1,0]<stdout>:[2025-10-11 22:12:59 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 7, #cached-token: 4, token usage: 0.15, #running-req: 31, #queue-req: 564, 
[1,0]<stdout>:[2025-10-11 22:13:00 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 220, #cached-token: 1, token usage: 0.11, #running-req: 31, #queue-req: 563, 
[1,0]<stdout>:[2025-10-11 22:13:00 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 357, #cached-token: 3, token usage: 0.11, #running-req: 31, #queue-req: 562, 
[1,1]<stdout>:[2025-10-11 22:13:00 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 17, #cached-token: 1, token usage: 0.11, #running-req: 31, #queue-req: 598, 
[1,0]<stdout>:[2025-10-11 22:13:00 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 15, #cached-token: 1, token usage: 0.11, #running-req: 31, #queue-req: 561, 
[1,1]<stdout>:[2025-10-11 22:13:00 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 340, #cached-token: 2, token usage: 0.11, #running-req: 31, #queue-req: 597, 
[1,1]<stdout>:[2025-10-11 22:13:01 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 690, #cached-token: 2, token usage: 0.11, #running-req: 31, #queue-req: 596, 
[1,0]<stdout>:[2025-10-11 22:13:01 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 548, #cached-token: 2, token usage: 0.11, #running-req: 31, #queue-req: 560, 
[1,0]<stdout>:[2025-10-11 22:13:01 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 6, #cached-token: 1, token usage: 0.11, #running-req: 31, #queue-req: 559, 
[1,1]<stdout>:[2025-10-11 22:13:02 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 20, #cached-token: 5, token usage: 0.11, #running-req: 31, #queue-req: 595, 
[1,1]<stdout>:[2025-10-11 22:13:02 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 542, #cached-token: 5, token usage: 0.11, #running-req: 31, #queue-req: 594, 
[1,1]<stdout>:[2025-10-11 22:13:02 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 11, #cached-token: 1, token usage: 0.11, #running-req: 31, #queue-req: 593, 
[1,1]<stdout>:[2025-10-11 22:13:03 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 225, #cached-token: 2, token usage: 0.11, #running-req: 31, #queue-req: 592, 
[1,0]<stdout>:[2025-10-11 22:13:03 DP0 TP0] Decode batch. #running-req: 32, #token: 19077, token usage: 0.11, accept len: 1.00, cuda graph: True, gen throughput (token/s): 340.46, #queue-req: 559, 
[1,1]<stdout>:[2025-10-11 22:13:03 DP1 TP8] Decode batch. #running-req: 32, #token: 19557, token usage: 0.12, accept len: 1.00, cuda graph: True, gen throughput (token/s): 340.19, #queue-req: 592, 
[1,1]<stdout>:[2025-10-11 22:13:03 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 11, #cached-token: 2, token usage: 0.11, #running-req: 31, #queue-req: 591, 
[1,1]<stdout>:[2025-10-11 22:13:03 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 11, #cached-token: 2, token usage: 0.11, #running-req: 31, #queue-req: 590, 
[1,0]<stdout>:[2025-10-11 22:13:04 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 625, #cached-token: 6, token usage: 0.11, #running-req: 31, #queue-req: 558, 
[1,1]<stdout>:[2025-10-11 22:13:04 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 5, #cached-token: 2, token usage: 0.11, #running-req: 31, #queue-req: 589, 
[1,0]<stdout>:[2025-10-11 22:13:04 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 767, #cached-token: 5, token usage: 0.11, #running-req: 31, #queue-req: 557, 
[1,0]<stdout>:[2025-10-11 22:13:05 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 14, #cached-token: 1, token usage: 0.11, #running-req: 31, #queue-req: 556, 
[1,0]<stdout>:[2025-10-11 22:13:05 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 662, #cached-token: 2, token usage: 0.11, #running-req: 31, #queue-req: 555, 
[1,1]<stdout>:[2025-10-11 22:13:05 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 9, #cached-token: 2, token usage: 0.11, #running-req: 31, #queue-req: 588, 
[1,1]<stdout>:[2025-10-11 22:13:05 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 130, #cached-token: 1, token usage: 0.11, #running-req: 31, #queue-req: 587, 
[1,0]<stdout>:[2025-10-11 22:13:05 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 138, #cached-token: 3, token usage: 0.12, #running-req: 31, #queue-req: 554, 
[1,0]<stdout>:[2025-10-11 22:13:06 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 9, #cached-token: 4, token usage: 0.12, #running-req: 31, #queue-req: 553, 
[1,1]<stdout>:[2025-10-11 22:13:06 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 8, #cached-token: 2, token usage: 0.10, #running-req: 31, #queue-req: 586, 
[1,0]<stdout>:[2025-10-11 22:13:06 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 8, #cached-token: 2, token usage: 0.11, #running-req: 31, #queue-req: 552, 
[1,1]<stdout>:[2025-10-11 22:13:07 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 83, #cached-token: 2, token usage: 0.09, #running-req: 31, #queue-req: 585, 
[1,0]<stdout>:[2025-10-11 22:13:07 DP0 TP0] Decode batch. #running-req: 32, #token: 19235, token usage: 0.11, accept len: 1.00, cuda graph: True, gen throughput (token/s): 341.85, #queue-req: 552, 
[1,1]<stdout>:[2025-10-11 22:13:07 DP1 TP8] Decode batch. #running-req: 31, #token: 13975, token usage: 0.08, accept len: 1.00, cuda graph: True, gen throughput (token/s): 341.58, #queue-req: 585, 
[1,1]<stdout>:[2025-10-11 22:13:07 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 10, #cached-token: 3, token usage: 0.08, #running-req: 31, #queue-req: 584, 
[1,1]<stdout>:[2025-10-11 22:13:07 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 246, #cached-token: 5, token usage: 0.08, #running-req: 31, #queue-req: 583, 
[1,1]<stdout>:[2025-10-11 22:13:07 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 372, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 582, 
[1,1]<stdout>:[2025-10-11 22:13:08 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 340, #cached-token: 2, token usage: 0.09, #running-req: 31, #queue-req: 581, 
[1,1]<stdout>:[2025-10-11 22:13:08 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 8, #cached-token: 2, token usage: 0.09, #running-req: 31, #queue-req: 580, 
[1,1]<stdout>:[2025-10-11 22:13:08 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 39, #cached-token: 3, token usage: 0.08, #running-req: 31, #queue-req: 579, 
[1,0]<stdout>:[2025-10-11 22:13:09 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 116, #cached-token: 4, token usage: 0.11, #running-req: 31, #queue-req: 551, 
[1,0]<stdout>:[2025-10-11 22:13:09 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 70, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 550, 
[1,1]<stdout>:[2025-10-11 22:13:10 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 485, #cached-token: 3, token usage: 0.09, #running-req: 31, #queue-req: 578, 
[1,0]<stdout>:[2025-10-11 22:13:10 DP0 TP0] Decode batch. #running-req: 32, #token: 16381, token usage: 0.10, accept len: 1.00, cuda graph: True, gen throughput (token/s): 405.53, #queue-req: 550, 
[1,1]<stdout>:[2025-10-11 22:13:10 DP1 TP8] Decode batch. #running-req: 32, #token: 15128, token usage: 0.09, accept len: 1.00, cuda graph: True, gen throughput (token/s): 404.25, #queue-req: 578, 
[1,1]<stdout>:[2025-10-11 22:13:10 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 844, #cached-token: 3, token usage: 0.09, #running-req: 31, #queue-req: 577, 
[1,1]<stdout>:[2025-10-11 22:13:10 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 61, #cached-token: 3, token usage: 0.08, #running-req: 31, #queue-req: 576, 
[1,0]<stdout>:[2025-10-11 22:13:11 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 2, #cached-token: 3, token usage: 0.10, #running-req: 31, #queue-req: 549, 
[1,0]<stdout>:[2025-10-11 22:13:11 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 13, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 548, 
[1,1]<stdout>:[2025-10-11 22:13:11 DP1 TP8] Prefill batch. #new-seq: 2, #new-token: 706, #cached-token: 3, token usage: 0.08, #running-req: 30, #queue-req: 574, 
[1,0]<stdout>:[2025-10-11 22:13:12 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 4, #cached-token: 4, token usage: 0.09, #running-req: 31, #queue-req: 547, 
[1,1]<stdout>:[2025-10-11 22:13:12 DP1 TP8] Prefill batch. #new-seq: 2, #new-token: 107, #cached-token: 6, token usage: 0.08, #running-req: 30, #queue-req: 572, 
[1,1]<stdout>:[2025-10-11 22:13:13 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 10, #cached-token: 3, token usage: 0.08, #running-req: 31, #queue-req: 571, 
[1,0]<stdout>:[2025-10-11 22:13:13 DP0 TP0] Decode batch. #running-req: 32, #token: 16458, token usage: 0.10, accept len: 1.00, cuda graph: True, gen throughput (token/s): 442.43, #queue-req: 547, 
[1,1]<stdout>:[2025-10-11 22:13:13 DP1 TP8] Decode batch. #running-req: 30, #token: 12952, token usage: 0.08, accept len: 1.00, cuda graph: True, gen throughput (token/s): 440.38, #queue-req: 571, 
[1,1]<stdout>:[2025-10-11 22:13:13 DP1 TP8] Prefill batch. #new-seq: 2, #new-token: 36, #cached-token: 6, token usage: 0.08, #running-req: 30, #queue-req: 569, 
[1,1]<stdout>:[2025-10-11 22:13:13 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 537, #cached-token: 3, token usage: 0.08, #running-req: 31, #queue-req: 568, 
[1,1]<stdout>:[2025-10-11 22:13:14 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 671, #cached-token: 1, token usage: 0.07, #running-req: 31, #queue-req: 567, 
[1,1]<stdout>:[2025-10-11 22:13:14 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 3, #cached-token: 2, token usage: 0.08, #running-req: 31, #queue-req: 566, 
[1,0]<stdout>:[2025-10-11 22:13:14 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 219, #cached-token: 2, token usage: 0.10, #running-req: 31, #queue-req: 546, 
[1,1]<stdout>:[2025-10-11 22:13:15 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 414, #cached-token: 3, token usage: 0.08, #running-req: 31, #queue-req: 565, 
[1,1]<stdout>:[2025-10-11 22:13:15 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 485, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 564, 
[1,1]<stdout>:[2025-10-11 22:13:15 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 105, #cached-token: 3, token usage: 0.08, #running-req: 31, #queue-req: 563, 
[1,1]<stdout>:[2025-10-11 22:13:15 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 365, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 562, 
[1,0]<stdout>:[2025-10-11 22:13:16 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 347, #cached-token: 3, token usage: 0.09, #running-req: 31, #queue-req: 545, 
[1,0]<stdout>:[2025-10-11 22:13:16 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 14, token usage: 0.09, #running-req: 31, #queue-req: 544, 
[1,0]<stdout>:[2025-10-11 22:13:16 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 60, #cached-token: 2, token usage: 0.09, #running-req: 31, #queue-req: 543, 
[1,0]<stdout>:[2025-10-11 22:13:16 DP0 TP0] Decode batch. #running-req: 32, #token: 15275, token usage: 0.09, accept len: 1.00, cuda graph: True, gen throughput (token/s): 357.89, #queue-req: 543, 
[1,1]<stdout>:[2025-10-11 22:13:16 DP1 TP8] Decode batch. #running-req: 32, #token: 13495, token usage: 0.08, accept len: 1.00, cuda graph: True, gen throughput (token/s): 357.06, #queue-req: 562, 
[1,1]<stdout>:[2025-10-11 22:13:16 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 137, #cached-token: 3, token usage: 0.08, #running-req: 31, #queue-req: 561, 
[1,0]<stdout>:[2025-10-11 22:13:17 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 15, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 542, 
[1,1]<stdout>:[2025-10-11 22:13:17 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 324, #cached-token: 2, token usage: 0.07, #running-req: 31, #queue-req: 560, 
[1,0]<stdout>:[2025-10-11 22:13:17 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 59, #cached-token: 3, token usage: 0.08, #running-req: 31, #queue-req: 541, 
[1,1]<stdout>:[2025-10-11 22:13:17 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 197, #cached-token: 1, token usage: 0.07, #running-req: 31, #queue-req: 559, 
[1,0]<stdout>:[2025-10-11 22:13:17 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 528, #cached-token: 10, token usage: 0.08, #running-req: 31, #queue-req: 540, 
[1,0]<stdout>:[2025-10-11 22:13:18 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 15, #cached-token: 3, token usage: 0.09, #running-req: 31, #queue-req: 539, 
[1,1]<stdout>:[2025-10-11 22:13:19 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 56, #cached-token: 4, token usage: 0.07, #running-req: 31, #queue-req: 558, 
[1,0]<stdout>:[2025-10-11 22:13:19 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 66, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 538, 
[1,0]<stdout>:[2025-10-11 22:13:19 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 29, #cached-token: 3, token usage: 0.08, #running-req: 31, #queue-req: 537, 
[1,0]<stdout>:[2025-10-11 22:13:19 DP0 TP0] Decode batch. #running-req: 32, #token: 14625, token usage: 0.09, accept len: 1.00, cuda graph: True, gen throughput (token/s): 416.71, #queue-req: 537, 
[1,1]<stdout>:[2025-10-11 22:13:19 DP1 TP8] Decode batch. #running-req: 31, #token: 10693, token usage: 0.06, accept len: 1.00, cuda graph: True, gen throughput (token/s): 417.03, #queue-req: 558, 
[1,1]<stdout>:[2025-10-11 22:13:19 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 563, #cached-token: 7, token usage: 0.06, #running-req: 31, #queue-req: 557, 
[1,0]<stdout>:[2025-10-11 22:13:20 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 1427, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 536, 
[1,0]<stdout>:[2025-10-11 22:13:20 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 141, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 535, 
[1,1]<stdout>:[2025-10-11 22:13:20 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 986, #cached-token: 1, token usage: 0.05, #running-req: 31, #queue-req: 556, 
[1,1]<stdout>:[2025-10-11 22:13:20 DP1 TP8] Prefill batch. #new-seq: 2, #new-token: 1294, #cached-token: 9, token usage: 0.06, #running-req: 30, #queue-req: 554, 
[1,0]<stdout>:[2025-10-11 22:13:21 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 22, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 534, 
[1,0]<stdout>:[2025-10-11 22:13:21 DP0 TP0] Prefill batch. #new-seq: 2, #new-token: 870, #cached-token: 4, token usage: 0.08, #running-req: 30, #queue-req: 532, 
[1,1]<stdout>:[2025-10-11 22:13:21 DP1 TP8] Prefill batch. #new-seq: 2, #new-token: 390, #cached-token: 5, token usage: 0.06, #running-req: 30, #queue-req: 552, 
[1,0]<stdout>:[2025-10-11 22:13:21 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 258, #cached-token: 3, token usage: 0.08, #running-req: 31, #queue-req: 531, 
[1,1]<stdout>:[2025-10-11 22:13:21 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 31, #cached-token: 3, token usage: 0.06, #running-req: 31, #queue-req: 551, 
[1,1]<stdout>:[2025-10-11 22:13:22 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 357, #cached-token: 2, token usage: 0.06, #running-req: 31, #queue-req: 550, 
[1,0]<stdout>:[2025-10-11 22:13:22 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 109, #cached-token: 3, token usage: 0.08, #running-req: 31, #queue-req: 530, 
[1,0]<stdout>:[2025-10-11 22:13:22 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 275, #cached-token: 5, token usage: 0.08, #running-req: 31, #queue-req: 529, 
[1,1]<stdout>:[2025-10-11 22:13:23 DP1 TP8] Prefill batch. #new-seq: 2, #new-token: 27, #cached-token: 4, token usage: 0.06, #running-req: 30, #queue-req: 548, 
[1,0]<stdout>:[2025-10-11 22:13:23 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 9, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 528, 
[1,0]<stdout>:[2025-10-11 22:13:23 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 4, token usage: 0.08, #running-req: 31, #queue-req: 527, 
[1,0]<stdout>:[2025-10-11 22:13:23 DP0 TP0] Decode batch. #running-req: 32, #token: 13763, token usage: 0.08, accept len: 1.00, cuda graph: True, gen throughput (token/s): 326.27, #queue-req: 527, 
[1,1]<stdout>:[2025-10-11 22:13:23 DP1 TP8] Decode batch. #running-req: 32, #token: 10835, token usage: 0.06, accept len: 1.00, cuda graph: True, gen throughput (token/s): 326.52, #queue-req: 548, 
[1,1]<stdout>:[2025-10-11 22:13:24 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 376, token usage: 0.06, #running-req: 31, #queue-req: 547, 
[1,1]<stdout>:[2025-10-11 22:13:24 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 177, #cached-token: 2, token usage: 0.06, #running-req: 31, #queue-req: 546, 
[1,1]<stdout>:[2025-10-11 22:13:24 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 1399, #cached-token: 1, token usage: 0.06, #running-req: 31, #queue-req: 545, 
[1,0]<stdout>:[2025-10-11 22:13:25 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 52, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 526, 
[1,0]<stdout>:[2025-10-11 22:13:25 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 829, #cached-token: 10, token usage: 0.08, #running-req: 31, #queue-req: 525, 
[1,1]<stdout>:[2025-10-11 22:13:25 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 15, #cached-token: 2, token usage: 0.07, #running-req: 31, #queue-req: 544, 
[1,0]<stdout>:[2025-10-11 22:13:25 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 132, #cached-token: 3, token usage: 0.09, #running-req: 31, #queue-req: 524, 
[1,1]<stdout>:[2025-10-11 22:13:26 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 242, #cached-token: 2, token usage: 0.07, #running-req: 31, #queue-req: 543, 
[1,1]<stdout>:[2025-10-11 22:13:26 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 305, #cached-token: 8, token usage: 0.07, #running-req: 31, #queue-req: 542, 
[1,0]<stdout>:[2025-10-11 22:13:26 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 101, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 523, 
[1,0]<stdout>:[2025-10-11 22:13:26 DP0 TP0] Decode batch. #running-req: 31, #token: 13992, token usage: 0.08, accept len: 1.00, cuda graph: True, gen throughput (token/s): 405.52, #queue-req: 523, 
[1,1]<stdout>:[2025-10-11 22:13:26 DP1 TP8] Decode batch. #running-req: 32, #token: 12534, token usage: 0.07, accept len: 1.00, cuda graph: True, gen throughput (token/s): 405.21, #queue-req: 542, 
[1,0]<stdout>:[2025-10-11 22:13:26 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 3, #cached-token: 4, token usage: 0.08, #running-req: 31, #queue-req: 522, 
[1,0]<stdout>:[2025-10-11 22:13:27 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 367, #cached-token: 3, token usage: 0.08, #running-req: 31, #queue-req: 521, 
[1,0]<stdout>:[2025-10-11 22:13:27 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 240, #cached-token: 1, token usage: 0.07, #running-req: 31, #queue-req: 520, 
[1,0]<stdout>:[2025-10-11 22:13:28 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 8, #cached-token: 1, token usage: 0.07, #running-req: 31, #queue-req: 519, 
[1,1]<stdout>:[2025-10-11 22:13:28 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 23, #cached-token: 3, token usage: 0.08, #running-req: 31, #queue-req: 541, 
[1,1]<stdout>:[2025-10-11 22:13:28 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 238, #cached-token: 2, token usage: 0.07, #running-req: 31, #queue-req: 540, 
[1,0]<stdout>:[2025-10-11 22:13:29 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 363, #cached-token: 2, token usage: 0.08, #running-req: 31, #queue-req: 518, 
[1,0]<stdout>:[2025-10-11 22:13:29 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 684, #cached-token: 3, token usage: 0.08, #running-req: 31, #queue-req: 517, 
[1,1]<stdout>:[2025-10-11 22:13:29 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 262, #cached-token: 3, token usage: 0.07, #running-req: 31, #queue-req: 539, 
[1,0]<stdout>:[2025-10-11 22:13:30 DP0 TP0] Decode batch. #running-req: 32, #token: 13908, token usage: 0.08, accept len: 1.00, cuda graph: True, gen throughput (token/s): 404.81, #queue-req: 517, 
[1,1]<stdout>:[2025-10-11 22:13:30 DP1 TP8] Decode batch. #running-req: 32, #token: 12728, token usage: 0.07, accept len: 1.00, cuda graph: True, gen throughput (token/s): 405.45, #queue-req: 539, 
[1,0]<stdout>:[2025-10-11 22:13:30 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 311, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 516, 
[1,1]<stdout>:[2025-10-11 22:13:30 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 10, #cached-token: 3, token usage: 0.07, #running-req: 31, #queue-req: 538, 
[1,1]<stdout>:[2025-10-11 22:13:30 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 522, #cached-token: 2, token usage: 0.07, #running-req: 31, #queue-req: 537, 
[1,0]<stdout>:[2025-10-11 22:13:30 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 6, token usage: 0.08, #running-req: 31, #queue-req: 515, 
[1,0]<stdout>:[2025-10-11 22:13:31 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 10, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 514, 
[1,1]<stdout>:[2025-10-11 22:13:31 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 740, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 536, 
[1,1]<stdout>:[2025-10-11 22:13:31 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 599, #cached-token: 2, token usage: 0.08, #running-req: 31, #queue-req: 535, 
[1,0]<stdout>:[2025-10-11 22:13:32 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 2048, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 513, 
[1,0]<stdout>:[2025-10-11 22:13:32 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 1167, #cached-token: 0, token usage: 0.09, #running-req: 31, #queue-req: 513, 
[1,1]<stdout>:[2025-10-11 22:13:32 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 6, #cached-token: 5, token usage: 0.08, #running-req: 31, #queue-req: 534, 
[1,1]<stdout>:[2025-10-11 22:13:33 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 123, #cached-token: 5, token usage: 0.08, #running-req: 31, #queue-req: 533, 
[1,1]<stdout>:[2025-10-11 22:13:33 DP1 TP8] Decode batch. #running-req: 32, #token: 13763, token usage: 0.08, accept len: 1.00, cuda graph: True, gen throughput (token/s): 369.64, #queue-req: 533, 
[1,0]<stdout>:[2025-10-11 22:13:33 DP0 TP0] Decode batch. #running-req: 31, #token: 17212, token usage: 0.10, accept len: 1.00, cuda graph: True, gen throughput (token/s): 369.93, #queue-req: 513, 
[1,0]<stdout>:[2025-10-11 22:13:33 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 34, #cached-token: 5, token usage: 0.10, #running-req: 31, #queue-req: 512, 
[1,0]<stdout>:[2025-10-11 22:13:33 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 29, #cached-token: 3, token usage: 0.10, #running-req: 31, #queue-req: 511, 
[1,1]<stdout>:[2025-10-11 22:13:33 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 451, #cached-token: 2, token usage: 0.08, #running-req: 31, #queue-req: 532, 
[1,1]<stdout>:[2025-10-11 22:13:33 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 5, #cached-token: 2, token usage: 0.08, #running-req: 31, #queue-req: 531, 
[1,0]<stdout>:[2025-10-11 22:13:34 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 3, #cached-token: 1, token usage: 0.10, #running-req: 31, #queue-req: 510, 
[1,0]<stdout>:[2025-10-11 22:13:34 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 378, #cached-token: 1, token usage: 0.10, #running-req: 31, #queue-req: 509, 
[1,0]<stdout>:[2025-10-11 22:13:34 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 13, #cached-token: 2, token usage: 0.10, #running-req: 31, #queue-req: 508, 
[1,0]<stdout>:[2025-10-11 22:13:35 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 7, #cached-token: 3, token usage: 0.10, #running-req: 31, #queue-req: 507, 
[1,0]<stdout>:[2025-10-11 22:13:35 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 246, #cached-token: 8, token usage: 0.10, #running-req: 31, #queue-req: 506, 
[1,0]<stdout>:[2025-10-11 22:13:35 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 503, #cached-token: 2, token usage: 0.10, #running-req: 31, #queue-req: 505, 
[1,1]<stdout>:[2025-10-11 22:13:36 DP1 TP8] Prefill batch. #new-seq: 2, #new-token: 587, #cached-token: 4, token usage: 0.08, #running-req: 30, #queue-req: 529, 
[1,1]<stdout>:[2025-10-11 22:13:36 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 11, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 528, 
[1,0]<stdout>:[2025-10-11 22:13:36 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 1951, #cached-token: 2, token usage: 0.10, #running-req: 31, #queue-req: 504, 
[1,0]<stdout>:[2025-10-11 22:13:36 DP0 TP0] Prefill batch. #new-seq: 2, #new-token: 755, #cached-token: 8, token usage: 0.11, #running-req: 30, #queue-req: 502, 
[1,1]<stdout>:[2025-10-11 22:13:36 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 8, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 527, 
[1,0]<stdout>:[2025-10-11 22:13:37 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 241, #cached-token: 1, token usage: 0.11, #running-req: 31, #queue-req: 501, 
[1,0]<stdout>:[2025-10-11 22:13:37 DP0 TP0] Prefill batch. #new-seq: 2, #new-token: 57, #cached-token: 3, token usage: 0.10, #running-req: 30, #queue-req: 499, 
[1,0]<stdout>:[2025-10-11 22:13:37 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 15, #cached-token: 1, token usage: 0.11, #running-req: 31, #queue-req: 498, 
[1,0]<stdout>:[2025-10-11 22:13:37 DP0 TP0] Decode batch. #running-req: 31, #token: 17827, token usage: 0.10, accept len: 1.00, cuda graph: True, gen throughput (token/s): 295.60, #queue-req: 498, 
[1,1]<stdout>:[2025-10-11 22:13:37 DP1 TP8] Decode batch. #running-req: 31, #token: 14112, token usage: 0.08, accept len: 1.00, cuda graph: True, gen throughput (token/s): 297.46, #queue-req: 527, 
[1,1]<stdout>:[2025-10-11 22:13:37 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 397, #cached-token: 9, token usage: 0.08, #running-req: 31, #queue-req: 526, 
[1,0]<stdout>:[2025-10-11 22:13:37 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 204, #cached-token: 3, token usage: 0.10, #running-req: 31, #queue-req: 497, 
[1,0]<stdout>:[2025-10-11 22:13:38 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 2048, #cached-token: 2, token usage: 0.11, #running-req: 31, #queue-req: 496, 
[1,0]<stdout>:[2025-10-11 22:13:38 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 40, #cached-token: 0, token usage: 0.12, #running-req: 31, #queue-req: 496, 
[1,1]<stdout>:[2025-10-11 22:13:38 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 528, #cached-token: 2, token usage: 0.09, #running-req: 31, #queue-req: 525, 
[1,1]<stdout>:[2025-10-11 22:13:38 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 22, #cached-token: 3, token usage: 0.09, #running-req: 31, #queue-req: 524, 
[1,1]<stdout>:[2025-10-11 22:13:39 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 386, #cached-token: 2, token usage: 0.09, #running-req: 31, #queue-req: 523, 
[1,0]<stdout>:[2025-10-11 22:13:40 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 15, #cached-token: 2, token usage: 0.12, #running-req: 31, #queue-req: 495, 
[1,1]<stdout>:[2025-10-11 22:13:40 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 10, #cached-token: 2, token usage: 0.09, #running-req: 31, #queue-req: 522, 
[1,1]<stdout>:[2025-10-11 22:13:40 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 97, #cached-token: 4, token usage: 0.08, #running-req: 31, #queue-req: 521, 
[1,1]<stdout>:[2025-10-11 22:13:40 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 278, #cached-token: 4, token usage: 0.08, #running-req: 31, #queue-req: 520, 
[1,1]<stdout>:[2025-10-11 22:13:41 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 35, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 519, 
[1,1]<stdout>:[2025-10-11 22:13:41 DP1 TP8] Decode batch. #running-req: 32, #token: 13905, token usage: 0.08, accept len: 1.00, cuda graph: True, gen throughput (token/s): 365.42, #queue-req: 519, 
[1,0]<stdout>:[2025-10-11 22:13:41 DP0 TP0] Decode batch. #running-req: 32, #token: 20801, token usage: 0.12, accept len: 1.00, cuda graph: True, gen throughput (token/s): 366.84, #queue-req: 495, 
[1,0]<stdout>:[2025-10-11 22:13:41 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 109, #cached-token: 3, token usage: 0.12, #running-req: 31, #queue-req: 494, 
[1,1]<stdout>:[2025-10-11 22:13:41 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 40, #cached-token: 3, token usage: 0.08, #running-req: 31, #queue-req: 518, 
[1,1]<stdout>:[2025-10-11 22:13:42 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 334, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 517, 
[1,1]<stdout>:[2025-10-11 22:13:42 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 25, #cached-token: 2, token usage: 0.08, #running-req: 31, #queue-req: 516, 
[1,1]<stdout>:[2025-10-11 22:13:42 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 642, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 515, 
[1,1]<stdout>:[2025-10-11 22:13:42 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 277, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 514, 
[1,0]<stdout>:[2025-10-11 22:13:43 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 15, #cached-token: 3, token usage: 0.12, #running-req: 31, #queue-req: 493, 
[1,1]<stdout>:[2025-10-11 22:13:43 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 485, #cached-token: 2, token usage: 0.08, #running-req: 31, #queue-req: 513, 
[1,0]<stdout>:[2025-10-11 22:13:43 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 1, token usage: 0.12, #running-req: 31, #queue-req: 492, 
[1,1]<stdout>:[2025-10-11 22:13:44 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 107, #cached-token: 3, token usage: 0.08, #running-req: 31, #queue-req: 512, 
[1,1]<stdout>:[2025-10-11 22:13:44 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 740, #cached-token: 3, token usage: 0.08, #running-req: 31, #queue-req: 511, 
[1,1]<stdout>:[2025-10-11 22:13:44 DP1 TP8] Decode batch. #running-req: 31, #token: 14292, token usage: 0.08, accept len: 1.00, cuda graph: True, gen throughput (token/s): 364.61, #queue-req: 511, 
[1,0]<stdout>:[2025-10-11 22:13:44 DP0 TP0] Decode batch. #running-req: 32, #token: 21304, token usage: 0.13, accept len: 1.00, cuda graph: True, gen throughput (token/s): 366.33, #queue-req: 492, 
[1,1]<stdout>:[2025-10-11 22:13:44 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 182, #cached-token: 2, token usage: 0.08, #running-req: 31, #queue-req: 510, 
[1,1]<stdout>:[2025-10-11 22:13:44 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 1016, #cached-token: 2, token usage: 0.08, #running-req: 31, #queue-req: 509, 
[1,1]<stdout>:[2025-10-11 22:13:45 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 259, #cached-token: 4, token usage: 0.09, #running-req: 31, #queue-req: 508, 
[1,1]<stdout>:[2025-10-11 22:13:45 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 209, #cached-token: 2, token usage: 0.09, #running-req: 31, #queue-req: 507, 
[1,0]<stdout>:[2025-10-11 22:13:45 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 108, #cached-token: 3, token usage: 0.13, #running-req: 31, #queue-req: 491, 
[1,1]<stdout>:[2025-10-11 22:13:45 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 48, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 506, 
[1,1]<stdout>:[2025-10-11 22:13:46 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 566, #cached-token: 2, token usage: 0.09, #running-req: 31, #queue-req: 505, 
[1,1]<stdout>:[2025-10-11 22:13:46 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 165, #cached-token: 2, token usage: 0.09, #running-req: 31, #queue-req: 504, 
[1,0]<stdout>:[2025-10-11 22:13:46 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 90, #cached-token: 3, token usage: 0.12, #running-req: 31, #queue-req: 490, 
[1,0]<stdout>:[2025-10-11 22:13:47 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 6, #cached-token: 4, token usage: 0.11, #running-req: 31, #queue-req: 489, 
[1,0]<stdout>:[2025-10-11 22:13:47 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 97, #cached-token: 2, token usage: 0.12, #running-req: 31, #queue-req: 488, 
[1,0]<stdout>:[2025-10-11 22:13:47 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 263, #cached-token: 2, token usage: 0.11, #running-req: 31, #queue-req: 487, 
[1,0]<stdout>:[2025-10-11 22:13:47 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 252, #cached-token: 1, token usage: 0.11, #running-req: 31, #queue-req: 486, 
[1,1]<stdout>:[2025-10-11 22:13:48 DP1 TP8] Decode batch. #running-req: 32, #token: 16859, token usage: 0.10, accept len: 1.00, cuda graph: True, gen throughput (token/s): 354.90, #queue-req: 504, 
[1,0]<stdout>:[2025-10-11 22:13:48 DP0 TP0] Decode batch. #running-req: 32, #token: 19763, token usage: 0.12, accept len: 1.00, cuda graph: True, gen throughput (token/s): 354.86, #queue-req: 486, 
[1,0]<stdout>:[2025-10-11 22:13:48 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 2, #cached-token: 3, token usage: 0.12, #running-req: 31, #queue-req: 485, 
[1,1]<stdout>:[2025-10-11 22:13:48 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 42, #cached-token: 1, token usage: 0.10, #running-req: 31, #queue-req: 503, 
[1,0]<stdout>:[2025-10-11 22:13:48 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 257, #cached-token: 1, token usage: 0.11, #running-req: 31, #queue-req: 484, 
[1,1]<stdout>:[2025-10-11 22:13:48 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 15, #cached-token: 3, token usage: 0.10, #running-req: 31, #queue-req: 502, 
[1,0]<stdout>:[2025-10-11 22:13:48 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 6, #cached-token: 4, token usage: 0.11, #running-req: 31, #queue-req: 483, 
[1,1]<stdout>:[2025-10-11 22:13:49 DP1 TP8] Prefill batch. #new-seq: 2, #new-token: 295, #cached-token: 3, token usage: 0.09, #running-req: 30, #queue-req: 500, 
[1,0]<stdout>:[2025-10-11 22:13:49 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 9, #cached-token: 1, token usage: 0.11, #running-req: 31, #queue-req: 482, 
[1,0]<stdout>:[2025-10-11 22:13:49 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 2032, #cached-token: 2, token usage: 0.11, #running-req: 31, #queue-req: 481, 
[1,1]<stdout>:[2025-10-11 22:13:49 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 779, #cached-token: 4, token usage: 0.09, #running-req: 31, #queue-req: 499, 
[1,0]<stdout>:[2025-10-11 22:13:50 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 619, #cached-token: 8, token usage: 0.12, #running-req: 31, #queue-req: 480, 
[1,0]<stdout>:[2025-10-11 22:13:50 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 1101, #cached-token: 1, token usage: 0.12, #running-req: 31, #queue-req: 479, 
[1,0]<stdout>:[2025-10-11 22:13:50 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 263, #cached-token: 1, token usage: 0.13, #running-req: 31, #queue-req: 478, 
[1,1]<stdout>:[2025-10-11 22:13:50 DP1 TP8] Prefill batch. #new-seq: 2, #new-token: 1396, #cached-token: 8, token usage: 0.09, #running-req: 30, #queue-req: 497, 
[1,0]<stdout>:[2025-10-11 22:13:50 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 409, #cached-token: 4, token usage: 0.12, #running-req: 31, #queue-req: 477, 
[1,1]<stdout>:[2025-10-11 22:13:51 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 36, #cached-token: 1, token usage: 0.10, #running-req: 31, #queue-req: 496, 
[1,0]<stdout>:[2025-10-11 22:13:51 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 9, #cached-token: 2, token usage: 0.12, #running-req: 31, #queue-req: 476, 
[1,0]<stdout>:[2025-10-11 22:13:51 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 105, #cached-token: 2, token usage: 0.11, #running-req: 31, #queue-req: 475, 
[1,1]<stdout>:[2025-10-11 22:13:51 DP1 TP8] Prefill batch. #new-seq: 2, #new-token: 211, #cached-token: 4, token usage: 0.09, #running-req: 30, #queue-req: 494, 
[1,0]<stdout>:[2025-10-11 22:13:51 DP0 TP0] Prefill batch. #new-seq: 2, #new-token: 852, #cached-token: 3, token usage: 0.11, #running-req: 30, #queue-req: 473, 
[1,0]<stdout>:[2025-10-11 22:13:52 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 769, #cached-token: 2, token usage: 0.11, #running-req: 31, #queue-req: 472, 
[1,1]<stdout>:[2025-10-11 22:13:52 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 273, #cached-token: 2, token usage: 0.09, #running-req: 31, #queue-req: 493, 
[1,0]<stdout>:[2025-10-11 22:13:52 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 519, #cached-token: 4, token usage: 0.11, #running-req: 31, #queue-req: 471, 
[1,1]<stdout>:[2025-10-11 22:13:52 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 807, #cached-token: 4, token usage: 0.09, #running-req: 31, #queue-req: 492, 
[1,0]<stdout>:[2025-10-11 22:13:53 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 768, #cached-token: 1, token usage: 0.11, #running-req: 31, #queue-req: 470, 
[1,0]<stdout>:[2025-10-11 22:13:53 DP0 TP0] Decode batch. #running-req: 32, #token: 19184, token usage: 0.11, accept len: 1.00, cuda graph: True, gen throughput (token/s): 257.12, #queue-req: 470, 
[1,1]<stdout>:[2025-10-11 22:13:53 DP1 TP8] Decode batch. #running-req: 32, #token: 15946, token usage: 0.09, accept len: 1.00, cuda graph: True, gen throughput (token/s): 257.93, #queue-req: 492, 
[1,0]<stdout>:[2025-10-11 22:13:53 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 565, #cached-token: 2, token usage: 0.11, #running-req: 31, #queue-req: 469, 
[1,0]<stdout>:[2025-10-11 22:13:53 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 8, #cached-token: 1, token usage: 0.11, #running-req: 31, #queue-req: 468, 
[1,1]<stdout>:[2025-10-11 22:13:53 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 661, #cached-token: 4, token usage: 0.09, #running-req: 31, #queue-req: 491, 
[1,1]<stdout>:[2025-10-11 22:13:54 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 25, #cached-token: 3, token usage: 0.09, #running-req: 31, #queue-req: 490, 
[1,0]<stdout>:[2025-10-11 22:13:54 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 1117, #cached-token: 1, token usage: 0.11, #running-req: 31, #queue-req: 467, 
[1,0]<stdout>:[2025-10-11 22:13:54 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 375, #cached-token: 2, token usage: 0.12, #running-req: 31, #queue-req: 466, 
[1,0]<stdout>:[2025-10-11 22:13:54 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 22, #cached-token: 3, token usage: 0.12, #running-req: 31, #queue-req: 465, 
[1,1]<stdout>:[2025-10-11 22:13:54 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 544, #cached-token: 2, token usage: 0.08, #running-req: 31, #queue-req: 489, 
[1,0]<stdout>:[2025-10-11 22:13:55 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 184, #cached-token: 3, token usage: 0.12, #running-req: 31, #queue-req: 464, 
[1,1]<stdout>:[2025-10-11 22:13:55 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 2048, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 488, 
[1,1]<stdout>:[2025-10-11 22:13:55 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 380, #cached-token: 0, token usage: 0.10, #running-req: 31, #queue-req: 488, 
[1,0]<stdout>:[2025-10-11 22:13:55 DP0 TP0] Prefill batch. #new-seq: 2, #new-token: 1347, #cached-token: 3, token usage: 0.12, #running-req: 30, #queue-req: 462, 
[1,0]<stdout>:[2025-10-11 22:13:55 DP0 TP0] Prefill batch. #new-seq: 2, #new-token: 131, #cached-token: 3, token usage: 0.13, #running-req: 30, #queue-req: 460, 
[1,1]<stdout>:[2025-10-11 22:13:56 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 480, #cached-token: 3, token usage: 0.09, #running-req: 31, #queue-req: 487, 
[1,1]<stdout>:[2025-10-11 22:13:56 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 49, #cached-token: 8, token usage: 0.10, #running-req: 31, #queue-req: 486, 
[1,1]<stdout>:[2025-10-11 22:13:56 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 313, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 485, 
[1,1]<stdout>:[2025-10-11 22:13:57 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 552, #cached-token: 5, token usage: 0.09, #running-req: 31, #queue-req: 484, 
[1,0]<stdout>:[2025-10-11 22:13:57 DP0 TP0] Decode batch. #running-req: 32, #token: 21973, token usage: 0.13, accept len: 1.00, cuda graph: True, gen throughput (token/s): 302.44, #queue-req: 460, 
[1,1]<stdout>:[2025-10-11 22:13:57 DP1 TP8] Decode batch. #running-req: 32, #token: 15164, token usage: 0.09, accept len: 1.00, cuda graph: True, gen throughput (token/s): 302.86, #queue-req: 484, 
[1,1]<stdout>:[2025-10-11 22:13:57 DP1 TP8] Prefill batch. #new-seq: 2, #new-token: 44, #cached-token: 5, token usage: 0.09, #running-req: 30, #queue-req: 482, 
[1,1]<stdout>:[2025-10-11 22:13:57 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 342, #cached-token: 3, token usage: 0.09, #running-req: 31, #queue-req: 481, 
[1,1]<stdout>:[2025-10-11 22:13:58 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 4, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 480, 
[1,0]<stdout>:[2025-10-11 22:13:58 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 441, #cached-token: 1, token usage: 0.13, #running-req: 31, #queue-req: 459, 
[1,1]<stdout>:[2025-10-11 22:13:58 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 15, #cached-token: 4, token usage: 0.09, #running-req: 31, #queue-req: 479, 
[1,0]<stdout>:[2025-10-11 22:13:58 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 1558, #cached-token: 3, token usage: 0.13, #running-req: 31, #queue-req: 458, 
[1,1]<stdout>:[2025-10-11 22:13:59 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 21, #cached-token: 2, token usage: 0.09, #running-req: 31, #queue-req: 478, 
[1,1]<stdout>:[2025-10-11 22:13:59 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 172, #cached-token: 4, token usage: 0.09, #running-req: 31, #queue-req: 477, 
[1,1]<stdout>:[2025-10-11 22:13:59 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 474, #cached-token: 3, token usage: 0.09, #running-req: 31, #queue-req: 476, 
[1,0]<stdout>:[2025-10-11 22:14:00 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 10, #cached-token: 5, token usage: 0.14, #running-req: 31, #queue-req: 457, 
[1,1]<stdout>:[2025-10-11 22:14:00 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 52, #cached-token: 2, token usage: 0.09, #running-req: 31, #queue-req: 475, 
[1,1]<stdout>:[2025-10-11 22:14:00 DP1 TP8] Decode batch. #running-req: 32, #token: 15481, token usage: 0.09, accept len: 1.00, cuda graph: True, gen throughput (token/s): 366.73, #queue-req: 475, 
[1,0]<stdout>:[2025-10-11 22:14:00 DP0 TP0] Decode batch. #running-req: 32, #token: 24324, token usage: 0.14, accept len: 1.00, cuda graph: True, gen throughput (token/s): 368.37, #queue-req: 457, 
[1,0]<stdout>:[2025-10-11 22:14:01 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 13, #cached-token: 3, token usage: 0.13, #running-req: 31, #queue-req: 456, 
[1,0]<stdout>:[2025-10-11 22:14:01 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 2, #cached-token: 1, token usage: 0.13, #running-req: 31, #queue-req: 455, 
[1,1]<stdout>:[2025-10-11 22:14:01 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 1424, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 474, 
[1,0]<stdout>:[2025-10-11 22:14:02 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 17, #cached-token: 2, token usage: 0.13, #running-req: 31, #queue-req: 454, 
[1,0]<stdout>:[2025-10-11 22:14:02 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 409, #cached-token: 1, token usage: 0.12, #running-req: 31, #queue-req: 453, 
[1,1]<stdout>:[2025-10-11 22:14:02 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 60, #cached-token: 1, token usage: 0.10, #running-req: 31, #queue-req: 473, 
[1,0]<stdout>:[2025-10-11 22:14:02 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 152, #cached-token: 4, token usage: 0.13, #running-req: 31, #queue-req: 452, 
[1,0]<stdout>:[2025-10-11 22:14:02 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 780, #cached-token: 4, token usage: 0.13, #running-req: 31, #queue-req: 451, 
[1,0]<stdout>:[2025-10-11 22:14:03 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 225, #cached-token: 5, token usage: 0.13, #running-req: 31, #queue-req: 450, 
[1,0]<stdout>:[2025-10-11 22:14:03 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 17, #cached-token: 2, token usage: 0.13, #running-req: 31, #queue-req: 449, 
[1,0]<stdout>:[2025-10-11 22:14:03 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 336, #cached-token: 1, token usage: 0.13, #running-req: 31, #queue-req: 448, 
[1,1]<stdout>:[2025-10-11 22:14:04 DP1 TP8] Decode batch. #running-req: 32, #token: 17145, token usage: 0.10, accept len: 1.00, cuda graph: True, gen throughput (token/s): 383.37, #queue-req: 473, 
[1,0]<stdout>:[2025-10-11 22:14:04 DP0 TP0] Decode batch. #running-req: 32, #token: 22567, token usage: 0.13, accept len: 1.00, cuda graph: True, gen throughput (token/s): 381.25, #queue-req: 448, 
[1,0]<stdout>:[2025-10-11 22:14:04 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 5, #cached-token: 2, token usage: 0.13, #running-req: 31, #queue-req: 447, 
[1,0]<stdout>:[2025-10-11 22:14:04 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 567, #cached-token: 1, token usage: 0.13, #running-req: 31, #queue-req: 446, 
[1,0]<stdout>:[2025-10-11 22:14:04 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 72, #cached-token: 8, token usage: 0.13, #running-req: 31, #queue-req: 445, 
[1,1]<stdout>:[2025-10-11 22:14:05 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 145, #cached-token: 4, token usage: 0.10, #running-req: 31, #queue-req: 472, 
[1,1]<stdout>:[2025-10-11 22:14:06 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 17, #cached-token: 19, token usage: 0.11, #running-req: 31, #queue-req: 471, 
[1,0]<stdout>:[2025-10-11 22:14:06 DP0 TP0] Decode batch. #running-req: 32, #token: 23645, token usage: 0.14, accept len: 1.00, cuda graph: True, gen throughput (token/s): 491.32, #queue-req: 445, 
[1,1]<stdout>:[2025-10-11 22:14:06 DP1 TP8] Decode batch. #running-req: 32, #token: 18378, token usage: 0.11, accept len: 1.00, cuda graph: True, gen throughput (token/s): 491.64, #queue-req: 471, 
[1,1]<stdout>:[2025-10-11 22:14:07 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 152, #cached-token: 1, token usage: 0.11, #running-req: 31, #queue-req: 470, 
[1,0]<stdout>:[2025-10-11 22:14:07 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 186, #cached-token: 4, token usage: 0.14, #running-req: 31, #queue-req: 444, 
[1,0]<stdout>:[2025-10-11 22:14:07 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 4, #cached-token: 3, token usage: 0.14, #running-req: 31, #queue-req: 443, 
[1,0]<stdout>:[2025-10-11 22:14:08 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 414, #cached-token: 3, token usage: 0.13, #running-req: 31, #queue-req: 442, 
[1,1]<stdout>:[2025-10-11 22:14:08 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 9, #cached-token: 2, token usage: 0.11, #running-req: 31, #queue-req: 469, 
[1,0]<stdout>:[2025-10-11 22:14:08 DP0 TP0] Prefill batch. #new-seq: 2, #new-token: 252, #cached-token: 7, token usage: 0.11, #running-req: 30, #queue-req: 440, 
[1,1]<stdout>:[2025-10-11 22:14:09 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 331, #cached-token: 2, token usage: 0.11, #running-req: 31, #queue-req: 468, 
[1,0]<stdout>:[2025-10-11 22:14:09 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 194, #cached-token: 2, token usage: 0.11, #running-req: 31, #queue-req: 439, 
[1,0]<stdout>:[2025-10-11 22:14:09 DP0 TP0] Decode batch. #running-req: 32, #token: 19684, token usage: 0.12, accept len: 1.00, cuda graph: True, gen throughput (token/s): 420.72, #queue-req: 439, 
[1,1]<stdout>:[2025-10-11 22:14:09 DP1 TP8] Decode batch. #running-req: 32, #token: 18927, token usage: 0.11, accept len: 1.00, cuda graph: True, gen throughput (token/s): 421.73, #queue-req: 468, 
[1,0]<stdout>:[2025-10-11 22:14:10 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 23, #cached-token: 1, token usage: 0.11, #running-req: 31, #queue-req: 438, 
[1,1]<stdout>:[2025-10-11 22:14:10 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 780, #cached-token: 3, token usage: 0.11, #running-req: 31, #queue-req: 467, 
[1,1]<stdout>:[2025-10-11 22:14:10 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 24, #cached-token: 1, token usage: 0.11, #running-req: 31, #queue-req: 466, 
[1,0]<stdout>:[2025-10-11 22:14:10 DP0 TP0] Prefill batch. #new-seq: 2, #new-token: 524, #cached-token: 2, token usage: 0.10, #running-req: 30, #queue-req: 436, 
[1,0]<stdout>:[2025-10-11 22:14:11 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 20, #cached-token: 3, token usage: 0.10, #running-req: 31, #queue-req: 435, 
[1,1]<stdout>:[2025-10-11 22:14:11 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 417, #cached-token: 7, token usage: 0.10, #running-req: 31, #queue-req: 465, 
[1,0]<stdout>:[2025-10-11 22:14:11 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 22, #cached-token: 2, token usage: 0.10, #running-req: 31, #queue-req: 434, 
[1,0]<stdout>:[2025-10-11 22:14:11 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 8, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 433, 
[1,0]<stdout>:[2025-10-11 22:14:12 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 8, #cached-token: 3, token usage: 0.09, #running-req: 31, #queue-req: 432, 
[1,0]<stdout>:[2025-10-11 22:14:12 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 241, #cached-token: 2, token usage: 0.09, #running-req: 31, #queue-req: 431, 
[1,0]<stdout>:[2025-10-11 22:14:12 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 388, #cached-token: 3, token usage: 0.09, #running-req: 31, #queue-req: 430, 
[1,0]<stdout>:[2025-10-11 22:14:12 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 84, #cached-token: 3, token usage: 0.09, #running-req: 31, #queue-req: 429, 
[1,1]<stdout>:[2025-10-11 22:14:12 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 14, #cached-token: 3, token usage: 0.10, #running-req: 31, #queue-req: 464, 
[1,0]<stdout>:[2025-10-11 22:14:13 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 428, 
[1,0]<stdout>:[2025-10-11 22:14:13 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 23, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 427, 
[1,1]<stdout>:[2025-10-11 22:14:13 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 768, #cached-token: 3, token usage: 0.10, #running-req: 31, #queue-req: 463, 
[1,0]<stdout>:[2025-10-11 22:14:13 DP0 TP0] Decode batch. #running-req: 32, #token: 14660, token usage: 0.09, accept len: 1.00, cuda graph: True, gen throughput (token/s): 337.10, #queue-req: 427, 
[1,1]<stdout>:[2025-10-11 22:14:13 DP1 TP8] Decode batch. #running-req: 32, #token: 17962, token usage: 0.11, accept len: 1.00, cuda graph: True, gen throughput (token/s): 338.93, #queue-req: 463, 
[1,0]<stdout>:[2025-10-11 22:14:13 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 647, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 426, 
[1,0]<stdout>:[2025-10-11 22:14:14 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 8, #cached-token: 3, token usage: 0.09, #running-req: 31, #queue-req: 425, 
[1,0]<stdout>:[2025-10-11 22:14:14 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 455, #cached-token: 2, token usage: 0.09, #running-req: 31, #queue-req: 424, 
[1,0]<stdout>:[2025-10-11 22:14:14 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 445, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 423, 
[1,1]<stdout>:[2025-10-11 22:14:14 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 301, #cached-token: 1, token usage: 0.11, #running-req: 31, #queue-req: 462, 
[1,0]<stdout>:[2025-10-11 22:14:15 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 674, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 422, 
[1,0]<stdout>:[2025-10-11 22:14:15 DP0 TP0] Prefill batch. #new-seq: 2, #new-token: 852, #cached-token: 6, token usage: 0.09, #running-req: 30, #queue-req: 420, 
[1,1]<stdout>:[2025-10-11 22:14:15 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 30, #cached-token: 2, token usage: 0.10, #running-req: 31, #queue-req: 461, 
[1,0]<stdout>:[2025-10-11 22:14:15 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 221, #cached-token: 2, token usage: 0.08, #running-req: 31, #queue-req: 419, 
[1,1]<stdout>:[2025-10-11 22:14:15 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 31, #cached-token: 2, token usage: 0.10, #running-req: 31, #queue-req: 460, 
[1,0]<stdout>:[2025-10-11 22:14:16 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 5, #cached-token: 4, token usage: 0.08, #running-req: 31, #queue-req: 418, 
[1,1]<stdout>:[2025-10-11 22:14:16 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 1779, #cached-token: 1, token usage: 0.10, #running-req: 31, #queue-req: 459, 
[1,0]<stdout>:[2025-10-11 22:14:16 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 106, #cached-token: 8, token usage: 0.08, #running-req: 31, #queue-req: 417, 
[1,1]<stdout>:[2025-10-11 22:14:17 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 598, #cached-token: 4, token usage: 0.11, #running-req: 31, #queue-req: 458, 
[1,0]<stdout>:[2025-10-11 22:14:17 DP0 TP0] Decode batch. #running-req: 32, #token: 14388, token usage: 0.08, accept len: 1.00, cuda graph: True, gen throughput (token/s): 347.55, #queue-req: 417, 
[1,1]<stdout>:[2025-10-11 22:14:17 DP1 TP8] Decode batch. #running-req: 32, #token: 19253, token usage: 0.11, accept len: 1.00, cuda graph: True, gen throughput (token/s): 348.93, #queue-req: 458, 
[1,0]<stdout>:[2025-10-11 22:14:17 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 37, #cached-token: 4, token usage: 0.08, #running-req: 31, #queue-req: 416, 
[1,1]<stdout>:[2025-10-11 22:14:17 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 812, #cached-token: 3, token usage: 0.11, #running-req: 31, #queue-req: 457, 
[1,0]<stdout>:[2025-10-11 22:14:18 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 11, #cached-token: 2, token usage: 0.08, #running-req: 31, #queue-req: 415, 
[1,1]<stdout>:[2025-10-11 22:14:18 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 2048, #cached-token: 3, token usage: 0.10, #running-req: 31, #queue-req: 456, 
[1,1]<stdout>:[2025-10-11 22:14:18 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 1361, #cached-token: 0, token usage: 0.12, #running-req: 31, #queue-req: 456, 
[1,1]<stdout>:[2025-10-11 22:14:18 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 109, #cached-token: 1, token usage: 0.12, #running-req: 31, #queue-req: 455, 
[1,0]<stdout>:[2025-10-11 22:14:19 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 280, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 414, 
[1,0]<stdout>:[2025-10-11 22:14:19 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 11, #cached-token: 3, token usage: 0.08, #running-req: 31, #queue-req: 413, 
[1,0]<stdout>:[2025-10-11 22:14:19 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 498, #cached-token: 3, token usage: 0.08, #running-req: 31, #queue-req: 412, 
[1,1]<stdout>:[2025-10-11 22:14:19 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 727, #cached-token: 3, token usage: 0.12, #running-req: 31, #queue-req: 454, 
[1,1]<stdout>:[2025-10-11 22:14:19 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 131, #cached-token: 2, token usage: 0.13, #running-req: 31, #queue-req: 453, 
[1,1]<stdout>:[2025-10-11 22:14:20 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 458, #cached-token: 1, token usage: 0.12, #running-req: 31, #queue-req: 452, 
[1,0]<stdout>:[2025-10-11 22:14:20 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 332, #cached-token: 2, token usage: 0.08, #running-req: 31, #queue-req: 411, 
[1,1]<stdout>:[2025-10-11 22:14:20 DP1 TP8] Decode batch. #running-req: 32, #token: 21789, token usage: 0.13, accept len: 1.00, cuda graph: True, gen throughput (token/s): 349.32, #queue-req: 452, 
[1,0]<stdout>:[2025-10-11 22:14:20 DP0 TP0] Decode batch. #running-req: 32, #token: 14007, token usage: 0.08, accept len: 1.00, cuda graph: True, gen throughput (token/s): 349.31, #queue-req: 411, 
[1,0]<stdout>:[2025-10-11 22:14:20 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 840, #cached-token: 3, token usage: 0.08, #running-req: 31, #queue-req: 410, 
[1,1]<stdout>:[2025-10-11 22:14:21 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 13, #cached-token: 2, token usage: 0.12, #running-req: 31, #queue-req: 451, 
[1,1]<stdout>:[2025-10-11 22:14:21 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 310, #cached-token: 1, token usage: 0.12, #running-req: 31, #queue-req: 450, 
[1,1]<stdout>:[2025-10-11 22:14:21 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 7, #cached-token: 1, token usage: 0.12, #running-req: 31, #queue-req: 449, 
[1,1]<stdout>:[2025-10-11 22:14:22 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 36, #cached-token: 3, token usage: 0.12, #running-req: 31, #queue-req: 448, 
[1,1]<stdout>:[2025-10-11 22:14:22 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 19, #cached-token: 3, token usage: 0.12, #running-req: 31, #queue-req: 447, 
[1,1]<stdout>:[2025-10-11 22:14:22 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 3, #cached-token: 2, token usage: 0.11, #running-req: 31, #queue-req: 446, 
[1,1]<stdout>:[2025-10-11 22:14:23 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 14, #cached-token: 3, token usage: 0.11, #running-req: 31, #queue-req: 445, 
[1,0]<stdout>:[2025-10-11 22:14:23 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 53, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 409, 
[1,1]<stdout>:[2025-10-11 22:14:23 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 572, #cached-token: 3, token usage: 0.11, #running-req: 31, #queue-req: 444, 
[1,1]<stdout>:[2025-10-11 22:14:23 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 284, #cached-token: 2, token usage: 0.11, #running-req: 31, #queue-req: 443, 
[1,0]<stdout>:[2025-10-11 22:14:24 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 200, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 408, 
[1,1]<stdout>:[2025-10-11 22:14:24 DP1 TP8] Decode batch. #running-req: 32, #token: 19508, token usage: 0.11, accept len: 1.00, cuda graph: True, gen throughput (token/s): 368.41, #queue-req: 443, 
[1,0]<stdout>:[2025-10-11 22:14:24 DP0 TP0] Decode batch. #running-req: 32, #token: 14843, token usage: 0.09, accept len: 1.00, cuda graph: True, gen throughput (token/s): 370.12, #queue-req: 408, 
[1,0]<stdout>:[2025-10-11 22:14:24 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 18, #cached-token: 3, token usage: 0.09, #running-req: 31, #queue-req: 407, 
[1,1]<stdout>:[2025-10-11 22:14:24 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 691, #cached-token: 2, token usage: 0.12, #running-req: 31, #queue-req: 442, 
[1,1]<stdout>:[2025-10-11 22:14:25 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 766, #cached-token: 6, token usage: 0.11, #running-req: 31, #queue-req: 441, 
[1,0]<stdout>:[2025-10-11 22:14:25 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 13, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 406, 
[1,1]<stdout>:[2025-10-11 22:14:25 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 8, #cached-token: 3, token usage: 0.11, #running-req: 31, #queue-req: 440, 
[1,1]<stdout>:[2025-10-11 22:14:25 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 11, #cached-token: 2, token usage: 0.11, #running-req: 31, #queue-req: 439, 
[1,0]<stdout>:[2025-10-11 22:14:25 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 41, #cached-token: 2, token usage: 0.09, #running-req: 31, #queue-req: 405, 
[1,0]<stdout>:[2025-10-11 22:14:26 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 664, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 404, 
[1,0]<stdout>:[2025-10-11 22:14:26 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 5, #cached-token: 3, token usage: 0.08, #running-req: 31, #queue-req: 403, 
[1,1]<stdout>:[2025-10-11 22:14:26 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 1080, #cached-token: 1, token usage: 0.11, #running-req: 31, #queue-req: 438, 
[1,0]<stdout>:[2025-10-11 22:14:27 DP0 TP0] Prefill batch. #new-seq: 2, #new-token: 336, #cached-token: 2, token usage: 0.07, #running-req: 30, #queue-req: 401, 
[1,0]<stdout>:[2025-10-11 22:14:27 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 8, #cached-token: 2, token usage: 0.06, #running-req: 31, #queue-req: 400, 
[1,0]<stdout>:[2025-10-11 22:14:27 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 633, #cached-token: 1, token usage: 0.06, #running-req: 31, #queue-req: 399, 
[1,0]<stdout>:[2025-10-11 22:14:28 DP0 TP0] Decode batch. #running-req: 32, #token: 11673, token usage: 0.07, accept len: 1.00, cuda graph: True, gen throughput (token/s): 337.83, #queue-req: 399, 
[1,1]<stdout>:[2025-10-11 22:14:28 DP1 TP8] Decode batch. #running-req: 32, #token: 19832, token usage: 0.12, accept len: 1.00, cuda graph: True, gen throughput (token/s): 338.86, #queue-req: 438, 
[1,1]<stdout>:[2025-10-11 22:14:28 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 8, #cached-token: 3, token usage: 0.12, #running-req: 31, #queue-req: 437, 
[1,0]<stdout>:[2025-10-11 22:14:28 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 462, #cached-token: 2, token usage: 0.07, #running-req: 31, #queue-req: 398, 
[1,0]<stdout>:[2025-10-11 22:14:28 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 154, #cached-token: 3, token usage: 0.06, #running-req: 31, #queue-req: 397, 
[1,0]<stdout>:[2025-10-11 22:14:28 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 322, #cached-token: 2, token usage: 0.06, #running-req: 31, #queue-req: 396, 
[1,1]<stdout>:[2025-10-11 22:14:28 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 2048, #cached-token: 3, token usage: 0.11, #running-req: 31, #queue-req: 436, 
[1,1]<stdout>:[2025-10-11 22:14:29 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 1444, #cached-token: 0, token usage: 0.12, #running-req: 31, #queue-req: 436, 
[1,0]<stdout>:[2025-10-11 22:14:29 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 136, #cached-token: 1, token usage: 0.06, #running-req: 31, #queue-req: 395, 
[1,0]<stdout>:[2025-10-11 22:14:29 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 25, #cached-token: 1, token usage: 0.07, #running-req: 31, #queue-req: 394, 
[1,0]<stdout>:[2025-10-11 22:14:30 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 20, #cached-token: 4, token usage: 0.07, #running-req: 31, #queue-req: 393, 
[1,1]<stdout>:[2025-10-11 22:14:30 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 121, #cached-token: 3, token usage: 0.13, #running-req: 31, #queue-req: 435, 
[1,0]<stdout>:[2025-10-11 22:14:30 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 40, #cached-token: 3, token usage: 0.06, #running-req: 31, #queue-req: 392, 
[1,0]<stdout>:[2025-10-11 22:14:31 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 76, #cached-token: 1, token usage: 0.06, #running-req: 31, #queue-req: 391, 
[1,1]<stdout>:[2025-10-11 22:14:31 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 64, #cached-token: 3, token usage: 0.13, #running-req: 31, #queue-req: 434, 
[1,1]<stdout>:[2025-10-11 22:14:31 DP1 TP8] Decode batch. #running-req: 32, #token: 22241, token usage: 0.13, accept len: 1.00, cuda graph: True, gen throughput (token/s): 365.83, #queue-req: 434, 
[1,0]<stdout>:[2025-10-11 22:14:31 DP0 TP0] Decode batch. #running-req: 31, #token: 10689, token usage: 0.06, accept len: 1.00, cuda graph: True, gen throughput (token/s): 364.38, #queue-req: 391, 
[1,0]<stdout>:[2025-10-11 22:14:31 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 53, #cached-token: 1, token usage: 0.06, #running-req: 31, #queue-req: 390, 
[1,0]<stdout>:[2025-10-11 22:14:32 DP0 TP0] Prefill batch. #new-seq: 2, #new-token: 333, #cached-token: 2, token usage: 0.06, #running-req: 30, #queue-req: 388, 
[1,1]<stdout>:[2025-10-11 22:14:32 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 47, #cached-token: 2, token usage: 0.12, #running-req: 31, #queue-req: 433, 
[1,0]<stdout>:[2025-10-11 22:14:32 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 228, #cached-token: 4, token usage: 0.06, #running-req: 31, #queue-req: 387, 
[1,1]<stdout>:[2025-10-11 22:14:33 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 48, #cached-token: 1, token usage: 0.12, #running-req: 31, #queue-req: 432, 
[1,0]<stdout>:[2025-10-11 22:14:33 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 6, #cached-token: 1, token usage: 0.07, #running-req: 31, #queue-req: 386, 
[1,1]<stdout>:[2025-10-11 22:14:33 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 424, #cached-token: 9, token usage: 0.12, #running-req: 31, #queue-req: 431, 
[1,0]<stdout>:[2025-10-11 22:14:34 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 237, #cached-token: 2, token usage: 0.07, #running-req: 31, #queue-req: 385, 
[1,0]<stdout>:[2025-10-11 22:14:34 DP0 TP0] Decode batch. #running-req: 32, #token: 11459, token usage: 0.07, accept len: 1.00, cuda graph: True, gen throughput (token/s): 423.12, #queue-req: 385, 
[1,1]<stdout>:[2025-10-11 22:14:34 DP1 TP8] Decode batch. #running-req: 32, #token: 20791, token usage: 0.12, accept len: 1.00, cuda graph: True, gen throughput (token/s): 423.74, #queue-req: 431, 
[1,0]<stdout>:[2025-10-11 22:14:34 DP0 TP0] Prefill batch. #new-seq: 2, #new-token: 388, #cached-token: 13, token usage: 0.06, #running-req: 30, #queue-req: 383, 
[1,1]<stdout>:[2025-10-11 22:14:35 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 388, #cached-token: 1, token usage: 0.12, #running-req: 31, #queue-req: 430, 
[1,0]<stdout>:[2025-10-11 22:14:35 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 46, #cached-token: 2, token usage: 0.06, #running-req: 31, #queue-req: 382, 
[1,0]<stdout>:[2025-10-11 22:14:36 DP0 TP0] Prefill batch. #new-seq: 2, #new-token: 24, #cached-token: 4, token usage: 0.06, #running-req: 30, #queue-req: 380, 
[1,0]<stdout>:[2025-10-11 22:14:36 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 22, #cached-token: 2, token usage: 0.06, #running-req: 31, #queue-req: 379, 
[1,1]<stdout>:[2025-10-11 22:14:37 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 289, #cached-token: 1, token usage: 0.12, #running-req: 31, #queue-req: 429, 
[1,0]<stdout>:[2025-10-11 22:14:37 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 162, #cached-token: 3, token usage: 0.06, #running-req: 31, #queue-req: 378, 
[1,0]<stdout>:[2025-10-11 22:14:37 DP0 TP0] Decode batch. #running-req: 32, #token: 10937, token usage: 0.06, accept len: 1.00, cuda graph: True, gen throughput (token/s): 461.07, #queue-req: 378, 
[1,1]<stdout>:[2025-10-11 22:14:37 DP1 TP8] Decode batch. #running-req: 32, #token: 21285, token usage: 0.13, accept len: 1.00, cuda graph: True, gen throughput (token/s): 462.95, #queue-req: 429, 
[1,1]<stdout>:[2025-10-11 22:14:37 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 251, #cached-token: 5, token usage: 0.12, #running-req: 31, #queue-req: 428, 
[1,1]<stdout>:[2025-10-11 22:14:38 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 266, #cached-token: 1, token usage: 0.13, #running-req: 31, #queue-req: 427, 
[1,1]<stdout>:[2025-10-11 22:14:38 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 311, #cached-token: 1, token usage: 0.13, #running-req: 31, #queue-req: 426, 
[1,1]<stdout>:[2025-10-11 22:14:38 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 6, #cached-token: 2, token usage: 0.13, #running-req: 31, #queue-req: 425, 
[1,0]<stdout>:[2025-10-11 22:14:38 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 21, #cached-token: 2, token usage: 0.07, #running-req: 31, #queue-req: 377, 
[1,1]<stdout>:[2025-10-11 22:14:39 DP1 TP8] Prefill batch. #new-seq: 2, #new-token: 444, #cached-token: 3, token usage: 0.12, #running-req: 30, #queue-req: 423, 
[1,1]<stdout>:[2025-10-11 22:14:39 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 93, #cached-token: 4, token usage: 0.12, #running-req: 31, #queue-req: 422, 
[1,1]<stdout>:[2025-10-11 22:14:39 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 369, #cached-token: 3, token usage: 0.12, #running-req: 31, #queue-req: 421, 
[1,1]<stdout>:[2025-10-11 22:14:39 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 871, #cached-token: 7, token usage: 0.12, #running-req: 31, #queue-req: 420, 
[1,1]<stdout>:[2025-10-11 22:14:39 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 18, #cached-token: 4, token usage: 0.12, #running-req: 31, #queue-req: 419, 
[1,0]<stdout>:[2025-10-11 22:14:40 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 30, #cached-token: 1, token usage: 0.07, #running-req: 31, #queue-req: 376, 
[1,1]<stdout>:[2025-10-11 22:14:40 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 62, #cached-token: 4, token usage: 0.12, #running-req: 31, #queue-req: 418, 
[1,1]<stdout>:[2025-10-11 22:14:41 DP1 TP8] Decode batch. #running-req: 32, #token: 20483, token usage: 0.12, accept len: 1.00, cuda graph: True, gen throughput (token/s): 352.57, #queue-req: 418, 
[1,0]<stdout>:[2025-10-11 22:14:41 DP0 TP0] Decode batch. #running-req: 32, #token: 11739, token usage: 0.07, accept len: 1.00, cuda graph: True, gen throughput (token/s): 355.06, #queue-req: 376, 
[1,0]<stdout>:[2025-10-11 22:14:41 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 651, #cached-token: 4, token usage: 0.07, #running-req: 31, #queue-req: 375, 
[1,0]<stdout>:[2025-10-11 22:14:41 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 655, #cached-token: 1, token usage: 0.07, #running-req: 31, #queue-req: 374, 
[1,0]<stdout>:[2025-10-11 22:14:42 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 3, #cached-token: 1, token usage: 0.07, #running-req: 31, #queue-req: 373, 
[1,0]<stdout>:[2025-10-11 22:14:42 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 336, #cached-token: 3, token usage: 0.07, #running-req: 31, #queue-req: 372, 
[1,1]<stdout>:[2025-10-11 22:14:43 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 984, #cached-token: 2, token usage: 0.12, #running-req: 31, #queue-req: 417, 
[1,1]<stdout>:[2025-10-11 22:14:43 DP1 TP8] Decode batch. #running-req: 32, #token: 22515, token usage: 0.13, accept len: 1.00, cuda graph: True, gen throughput (token/s): 492.14, #queue-req: 417, 
[1,0]<stdout>:[2025-10-11 22:14:43 DP0 TP0] Decode batch. #running-req: 32, #token: 12700, token usage: 0.07, accept len: 1.00, cuda graph: True, gen throughput (token/s): 491.03, #queue-req: 372, 
[1,0]<stdout>:[2025-10-11 22:14:43 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 196, #cached-token: 3, token usage: 0.07, #running-req: 31, #queue-req: 371, 
[1,1]<stdout>:[2025-10-11 22:14:44 DP1 TP8] Prefill batch. #new-seq: 2, #new-token: 598, #cached-token: 2, token usage: 0.13, #running-req: 30, #queue-req: 415, 
[1,0]<stdout>:[2025-10-11 22:14:44 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 456, #cached-token: 3, token usage: 0.07, #running-req: 31, #queue-req: 370, 
[1,1]<stdout>:[2025-10-11 22:14:44 DP1 TP8] Prefill batch. #new-seq: 2, #new-token: 236, #cached-token: 4, token usage: 0.13, #running-req: 30, #queue-req: 413, 
[1,1]<stdout>:[2025-10-11 22:14:44 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 36, #cached-token: 2, token usage: 0.13, #running-req: 31, #queue-req: 412, 
[1,0]<stdout>:[2025-10-11 22:14:45 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 3, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 369, 
[1,0]<stdout>:[2025-10-11 22:14:45 DP0 TP0] Prefill batch. #new-seq: 2, #new-token: 326, #cached-token: 4, token usage: 0.07, #running-req: 30, #queue-req: 367, 
[1,0]<stdout>:[2025-10-11 22:14:46 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 1156, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 366, 
[1,0]<stdout>:[2025-10-11 22:14:46 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 276, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 365, 
[1,0]<stdout>:[2025-10-11 22:14:46 DP0 TP0] Decode batch. #running-req: 31, #token: 14282, token usage: 0.08, accept len: 1.00, cuda graph: True, gen throughput (token/s): 399.40, #queue-req: 365, 
[1,1]<stdout>:[2025-10-11 22:14:46 DP1 TP8] Decode batch. #running-req: 32, #token: 22696, token usage: 0.13, accept len: 1.00, cuda graph: True, gen throughput (token/s): 400.34, #queue-req: 412, 
[1,0]<stdout>:[2025-10-11 22:14:46 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 24, #cached-token: 2, token usage: 0.08, #running-req: 31, #queue-req: 364, 
[1,0]<stdout>:[2025-10-11 22:14:47 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 689, #cached-token: 2, token usage: 0.08, #running-req: 31, #queue-req: 363, 
[1,1]<stdout>:[2025-10-11 22:14:47 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 13, #cached-token: 3, token usage: 0.13, #running-req: 31, #queue-req: 411, 
[1,0]<stdout>:[2025-10-11 22:14:47 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 523, #cached-token: 6, token usage: 0.08, #running-req: 31, #queue-req: 362, 
[1,1]<stdout>:[2025-10-11 22:14:47 DP1 TP8] Prefill batch. #new-seq: 2, #new-token: 1068, #cached-token: 4, token usage: 0.13, #running-req: 30, #queue-req: 409, 
[1,1]<stdout>:[2025-10-11 22:14:48 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 78, #cached-token: 1, token usage: 0.13, #running-req: 31, #queue-req: 408, 
[1,0]<stdout>:[2025-10-11 22:14:48 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 46, #cached-token: 2, token usage: 0.08, #running-req: 31, #queue-req: 361, 
[1,1]<stdout>:[2025-10-11 22:14:48 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 798, #cached-token: 3, token usage: 0.13, #running-req: 31, #queue-req: 407, 
[1,0]<stdout>:[2025-10-11 22:14:48 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 121, #cached-token: 4, token usage: 0.08, #running-req: 31, #queue-req: 360, 
[1,1]<stdout>:[2025-10-11 22:14:49 DP1 TP8] Prefill batch. #new-seq: 2, #new-token: 1456, #cached-token: 9, token usage: 0.12, #running-req: 30, #queue-req: 405, 
[1,1]<stdout>:[2025-10-11 22:14:49 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 1, token usage: 0.13, #running-req: 31, #queue-req: 404, 
[1,1]<stdout>:[2025-10-11 22:14:50 DP1 TP8] Decode batch. #running-req: 32, #token: 22052, token usage: 0.13, accept len: 1.00, cuda graph: True, gen throughput (token/s): 380.97, #queue-req: 404, 
[1,0]<stdout>:[2025-10-11 22:14:50 DP0 TP0] Decode batch. #running-req: 32, #token: 14575, token usage: 0.09, accept len: 1.00, cuda graph: True, gen throughput (token/s): 382.12, #queue-req: 360, 
[1,0]<stdout>:[2025-10-11 22:14:50 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 124, #cached-token: 2, token usage: 0.09, #running-req: 31, #queue-req: 359, 
[1,1]<stdout>:[2025-10-11 22:14:50 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 11, #cached-token: 1, token usage: 0.13, #running-req: 31, #queue-req: 403, 
[1,1]<stdout>:[2025-10-11 22:14:51 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 210, #cached-token: 6, token usage: 0.13, #running-req: 31, #queue-req: 402, 
[1,0]<stdout>:[2025-10-11 22:14:51 DP0 TP0] Prefill batch. #new-seq: 2, #new-token: 394, #cached-token: 13, token usage: 0.09, #running-req: 30, #queue-req: 357, 
[1,1]<stdout>:[2025-10-11 22:14:52 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 723, #cached-token: 8, token usage: 0.12, #running-req: 31, #queue-req: 401, 
[1,1]<stdout>:[2025-10-11 22:14:52 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 804, #cached-token: 5, token usage: 0.13, #running-req: 31, #queue-req: 400, 
[1,0]<stdout>:[2025-10-11 22:14:52 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 356, 
[1,1]<stdout>:[2025-10-11 22:14:52 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 388, #cached-token: 8, token usage: 0.13, #running-req: 31, #queue-req: 399, 
[1,0]<stdout>:[2025-10-11 22:14:52 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 305, #cached-token: 5, token usage: 0.09, #running-req: 31, #queue-req: 355, 
[1,1]<stdout>:[2025-10-11 22:14:53 DP1 TP8] Prefill batch. #new-seq: 2, #new-token: 27, #cached-token: 4, token usage: 0.13, #running-req: 30, #queue-req: 397, 
[1,0]<stdout>:[2025-10-11 22:14:53 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 318, #cached-token: 3, token usage: 0.08, #running-req: 31, #queue-req: 354, 
[1,1]<stdout>:[2025-10-11 22:14:53 DP1 TP8] Decode batch. #running-req: 32, #token: 21541, token usage: 0.13, accept len: 1.00, cuda graph: True, gen throughput (token/s): 380.57, #queue-req: 397, 
[1,0]<stdout>:[2025-10-11 22:14:53 DP0 TP0] Decode batch. #running-req: 32, #token: 13315, token usage: 0.08, accept len: 1.00, cuda graph: True, gen throughput (token/s): 380.91, #queue-req: 354, 
[1,0]<stdout>:[2025-10-11 22:14:53 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 38, #cached-token: 5, token usage: 0.08, #running-req: 31, #queue-req: 353, 
[1,1]<stdout>:[2025-10-11 22:14:53 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 32, #cached-token: 3, token usage: 0.13, #running-req: 31, #queue-req: 396, 
[1,0]<stdout>:[2025-10-11 22:14:53 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 9, #cached-token: 2, token usage: 0.08, #running-req: 31, #queue-req: 352, 
[1,1]<stdout>:[2025-10-11 22:14:53 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 155, #cached-token: 2, token usage: 0.12, #running-req: 31, #queue-req: 395, 
[1,1]<stdout>:[2025-10-11 22:14:54 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 186, #cached-token: 2, token usage: 0.11, #running-req: 31, #queue-req: 394, 
[1,1]<stdout>:[2025-10-11 22:14:54 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 241, #cached-token: 2, token usage: 0.12, #running-req: 31, #queue-req: 393, 
[1,1]<stdout>:[2025-10-11 22:14:54 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 747, #cached-token: 3, token usage: 0.12, #running-req: 31, #queue-req: 392, 
[1,0]<stdout>:[2025-10-11 22:14:54 DP0 TP0] Prefill batch. #new-seq: 2, #new-token: 700, #cached-token: 3, token usage: 0.07, #running-req: 30, #queue-req: 350, 
[1,1]<stdout>:[2025-10-11 22:14:55 DP1 TP8] Prefill batch. #new-seq: 2, #new-token: 769, #cached-token: 7, token usage: 0.11, #running-req: 30, #queue-req: 390, 
[1,0]<stdout>:[2025-10-11 22:14:55 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 321, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 349, 
[1,1]<stdout>:[2025-10-11 22:14:55 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 609, #cached-token: 1, token usage: 0.12, #running-req: 31, #queue-req: 389, 
[1,1]<stdout>:[2025-10-11 22:14:56 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 283, #cached-token: 2, token usage: 0.12, #running-req: 31, #queue-req: 388, 
[1,1]<stdout>:[2025-10-11 22:14:56 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 378, #cached-token: 4, token usage: 0.12, #running-req: 31, #queue-req: 387, 
[1,0]<stdout>:[2025-10-11 22:14:56 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 9, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 348, 
[1,1]<stdout>:[2025-10-11 22:14:56 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 640, #cached-token: 4, token usage: 0.12, #running-req: 31, #queue-req: 386, 
[1,0]<stdout>:[2025-10-11 22:14:57 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 1543, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 347, 
[1,1]<stdout>:[2025-10-11 22:14:57 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 145, #cached-token: 4, token usage: 0.12, #running-req: 31, #queue-req: 385, 
[1,0]<stdout>:[2025-10-11 22:14:57 DP0 TP0] Decode batch. #running-req: 32, #token: 14866, token usage: 0.09, accept len: 1.00, cuda graph: True, gen throughput (token/s): 316.65, #queue-req: 347, 
[1,1]<stdout>:[2025-10-11 22:14:57 DP1 TP8] Decode batch. #running-req: 32, #token: 20981, token usage: 0.12, accept len: 1.00, cuda graph: True, gen throughput (token/s): 315.42, #queue-req: 385, 
[1,0]<stdout>:[2025-10-11 22:14:57 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 2048, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 346, 
[1,1]<stdout>:[2025-10-11 22:14:57 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 31, #cached-token: 1, token usage: 0.12, #running-req: 31, #queue-req: 384, 
[1,0]<stdout>:[2025-10-11 22:14:57 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 2048, #cached-token: 0, token usage: 0.10, #running-req: 31, #queue-req: 346, 
[1,0]<stdout>:[2025-10-11 22:14:57 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 279, #cached-token: 0, token usage: 0.11, #running-req: 31, #queue-req: 346, 
[1,0]<stdout>:[2025-10-11 22:14:58 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 20, #cached-token: 1, token usage: 0.11, #running-req: 31, #queue-req: 345, 
[1,1]<stdout>:[2025-10-11 22:14:58 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 423, #cached-token: 7, token usage: 0.12, #running-req: 31, #queue-req: 383, 
[1,0]<stdout>:[2025-10-11 22:14:58 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 1098, #cached-token: 1, token usage: 0.11, #running-req: 31, #queue-req: 344, 
[1,1]<stdout>:[2025-10-11 22:14:58 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 749, #cached-token: 6, token usage: 0.12, #running-req: 31, #queue-req: 382, 
[1,1]<stdout>:[2025-10-11 22:14:58 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 7, #cached-token: 1, token usage: 0.12, #running-req: 31, #queue-req: 381, 
[1,0]<stdout>:[2025-10-11 22:14:58 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 140, #cached-token: 2, token usage: 0.11, #running-req: 31, #queue-req: 343, 
[1,1]<stdout>:[2025-10-11 22:14:59 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 12, #cached-token: 1, token usage: 0.12, #running-req: 31, #queue-req: 380, 
[1,0]<stdout>:[2025-10-11 22:14:59 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 456, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 342, 
[1,0]<stdout>:[2025-10-11 22:14:59 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 217, #cached-token: 4, token usage: 0.09, #running-req: 31, #queue-req: 341, 
[1,1]<stdout>:[2025-10-11 22:14:59 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 616, #cached-token: 2, token usage: 0.12, #running-req: 31, #queue-req: 379, 
[1,0]<stdout>:[2025-10-11 22:15:00 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 41, #cached-token: 2, token usage: 0.09, #running-req: 31, #queue-req: 340, 
[1,1]<stdout>:[2025-10-11 22:15:00 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 33, #cached-token: 3, token usage: 0.12, #running-req: 31, #queue-req: 378, 
[1,0]<stdout>:[2025-10-11 22:15:00 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 221, #cached-token: 4, token usage: 0.09, #running-req: 31, #queue-req: 339, 
[1,0]<stdout>:[2025-10-11 22:15:01 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 176, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 338, 
[1,0]<stdout>:[2025-10-11 22:15:01 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 38, #cached-token: 3, token usage: 0.08, #running-req: 31, #queue-req: 337, 
[1,1]<stdout>:[2025-10-11 22:15:01 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 94, #cached-token: 2, token usage: 0.11, #running-req: 31, #queue-req: 377, 
[1,0]<stdout>:[2025-10-11 22:15:01 DP0 TP0] Decode batch. #running-req: 32, #token: 14500, token usage: 0.09, accept len: 1.00, cuda graph: True, gen throughput (token/s): 292.12, #queue-req: 337, 
[1,1]<stdout>:[2025-10-11 22:15:01 DP1 TP8] Decode batch. #running-req: 32, #token: 19264, token usage: 0.11, accept len: 1.00, cuda graph: True, gen throughput (token/s): 292.58, #queue-req: 377, 
[1,1]<stdout>:[2025-10-11 22:15:02 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 301, #cached-token: 4, token usage: 0.11, #running-req: 31, #queue-req: 376, 
[1,0]<stdout>:[2025-10-11 22:15:02 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 401, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 336, 
[1,1]<stdout>:[2025-10-11 22:15:02 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 222, #cached-token: 1, token usage: 0.11, #running-req: 31, #queue-req: 375, 
[1,1]<stdout>:[2025-10-11 22:15:03 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 617, #cached-token: 9, token usage: 0.11, #running-req: 31, #queue-req: 374, 
[1,1]<stdout>:[2025-10-11 22:15:03 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 398, #cached-token: 1, token usage: 0.11, #running-req: 31, #queue-req: 373, 
[1,0]<stdout>:[2025-10-11 22:15:03 DP0 TP0] Prefill batch. #new-seq: 2, #new-token: 794, #cached-token: 6, token usage: 0.08, #running-req: 30, #queue-req: 334, 
[1,1]<stdout>:[2025-10-11 22:15:04 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 14, #cached-token: 2, token usage: 0.11, #running-req: 31, #queue-req: 372, 
[1,0]<stdout>:[2025-10-11 22:15:04 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 383, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 333, 
[1,1]<stdout>:[2025-10-11 22:15:04 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 365, #cached-token: 1, token usage: 0.11, #running-req: 31, #queue-req: 371, 
[1,0]<stdout>:[2025-10-11 22:15:04 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 8, #cached-token: 3, token usage: 0.09, #running-req: 31, #queue-req: 332, 
[1,0]<stdout>:[2025-10-11 22:15:04 DP0 TP0] Prefill batch. #new-seq: 2, #new-token: 1290, #cached-token: 2, token usage: 0.09, #running-req: 30, #queue-req: 330, 
[1,1]<stdout>:[2025-10-11 22:15:05 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 15, #cached-token: 1, token usage: 0.11, #running-req: 31, #queue-req: 370, 
[1,0]<stdout>:[2025-10-11 22:15:05 DP0 TP0] Decode batch. #running-req: 32, #token: 15952, token usage: 0.09, accept len: 1.00, cuda graph: True, gen throughput (token/s): 366.19, #queue-req: 330, 
[1,1]<stdout>:[2025-10-11 22:15:05 DP1 TP8] Decode batch. #running-req: 32, #token: 18723, token usage: 0.11, accept len: 1.00, cuda graph: True, gen throughput (token/s): 366.17, #queue-req: 370, 
[1,1]<stdout>:[2025-10-11 22:15:05 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 75, #cached-token: 3, token usage: 0.11, #running-req: 31, #queue-req: 369, 
[1,1]<stdout>:[2025-10-11 22:15:05 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 49, #cached-token: 1, token usage: 0.11, #running-req: 31, #queue-req: 368, 
[1,0]<stdout>:[2025-10-11 22:15:05 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 16, #cached-token: 3, token usage: 0.09, #running-req: 31, #queue-req: 329, 
[1,1]<stdout>:[2025-10-11 22:15:05 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 26, #cached-token: 4, token usage: 0.10, #running-req: 31, #queue-req: 367, 
[1,1]<stdout>:[2025-10-11 22:15:06 DP1 TP8] Prefill batch. #new-seq: 2, #new-token: 39, #cached-token: 4, token usage: 0.10, #running-req: 30, #queue-req: 365, 
[1,1]<stdout>:[2025-10-11 22:15:06 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 35, #cached-token: 4, token usage: 0.11, #running-req: 31, #queue-req: 364, 
[1,0]<stdout>:[2025-10-11 22:15:06 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 74, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 328, 
[1,0]<stdout>:[2025-10-11 22:15:07 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 12, #cached-token: 3, token usage: 0.09, #running-req: 31, #queue-req: 327, 
[1,1]<stdout>:[2025-10-11 22:15:07 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 43, #cached-token: 2, token usage: 0.11, #running-req: 31, #queue-req: 363, 
[1,0]<stdout>:[2025-10-11 22:15:07 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 32, #cached-token: 2, token usage: 0.09, #running-req: 31, #queue-req: 326, 
[1,1]<stdout>:[2025-10-11 22:15:08 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 903, #cached-token: 4, token usage: 0.10, #running-req: 31, #queue-req: 362, 
[1,0]<stdout>:[2025-10-11 22:15:08 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 422, #cached-token: 2, token usage: 0.09, #running-req: 31, #queue-req: 325, 
[1,0]<stdout>:[2025-10-11 22:15:08 DP0 TP0] Decode batch. #running-req: 32, #token: 15289, token usage: 0.09, accept len: 1.00, cuda graph: True, gen throughput (token/s): 365.07, #queue-req: 325, 
[1,1]<stdout>:[2025-10-11 22:15:08 DP1 TP8] Decode batch. #running-req: 32, #token: 18749, token usage: 0.11, accept len: 1.00, cuda graph: True, gen throughput (token/s): 364.20, #queue-req: 362, 
[1,1]<stdout>:[2025-10-11 22:15:08 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 510, #cached-token: 1, token usage: 0.11, #running-req: 31, #queue-req: 361, 
[1,0]<stdout>:[2025-10-11 22:15:09 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 324, 
[1,1]<stdout>:[2025-10-11 22:15:10 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 331, #cached-token: 1, token usage: 0.11, #running-req: 31, #queue-req: 360, 
[1,1]<stdout>:[2025-10-11 22:15:10 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 17, #cached-token: 2, token usage: 0.11, #running-req: 31, #queue-req: 359, 
[1,0]<stdout>:[2025-10-11 22:15:10 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 2048, #cached-token: 2, token usage: 0.09, #running-req: 31, #queue-req: 323, 
[1,0]<stdout>:[2025-10-11 22:15:10 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 341, #cached-token: 0, token usage: 0.10, #running-req: 31, #queue-req: 323, 
[1,0]<stdout>:[2025-10-11 22:15:11 DP0 TP0] Decode batch. #running-req: 32, #token: 17684, token usage: 0.10, accept len: 1.00, cuda graph: True, gen throughput (token/s): 464.80, #queue-req: 323, 
[1,1]<stdout>:[2025-10-11 22:15:11 DP1 TP8] Decode batch. #running-req: 31, #token: 19788, token usage: 0.12, accept len: 1.00, cuda graph: True, gen throughput (token/s): 464.06, #queue-req: 359, 
[1,1]<stdout>:[2025-10-11 22:15:11 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 18, #cached-token: 5, token usage: 0.12, #running-req: 31, #queue-req: 358, 
[1,1]<stdout>:[2025-10-11 22:15:11 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 24, #cached-token: 1, token usage: 0.12, #running-req: 31, #queue-req: 357, 
[1,0]<stdout>:[2025-10-11 22:15:12 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 24, #cached-token: 1, token usage: 0.10, #running-req: 31, #queue-req: 322, 
[1,0]<stdout>:[2025-10-11 22:15:12 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 11, #cached-token: 1, token usage: 0.10, #running-req: 31, #queue-req: 321, 
[1,1]<stdout>:[2025-10-11 22:15:12 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 9, #cached-token: 3, token usage: 0.12, #running-req: 31, #queue-req: 356, 
[1,0]<stdout>:[2025-10-11 22:15:13 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 245, #cached-token: 1, token usage: 0.10, #running-req: 31, #queue-req: 320, 
[1,1]<stdout>:[2025-10-11 22:15:13 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 343, #cached-token: 1, token usage: 0.12, #running-req: 31, #queue-req: 355, 
[1,0]<stdout>:[2025-10-11 22:15:13 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 6, #cached-token: 4, token usage: 0.09, #running-req: 31, #queue-req: 319, 
[1,1]<stdout>:[2025-10-11 22:15:14 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 512, #cached-token: 2, token usage: 0.12, #running-req: 31, #queue-req: 354, 
[1,1]<stdout>:[2025-10-11 22:15:14 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 12, #cached-token: 3, token usage: 0.12, #running-req: 31, #queue-req: 353, 
[1,0]<stdout>:[2025-10-11 22:15:14 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 230, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 318, 
[1,1]<stdout>:[2025-10-11 22:15:14 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 830, #cached-token: 4, token usage: 0.12, #running-req: 31, #queue-req: 352, 
[1,0]<stdout>:[2025-10-11 22:15:14 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 56, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 317, 
[1,0]<stdout>:[2025-10-11 22:15:15 DP0 TP0] Decode batch. #running-req: 32, #token: 15467, token usage: 0.09, accept len: 1.00, cuda graph: True, gen throughput (token/s): 345.94, #queue-req: 317, 
[1,1]<stdout>:[2025-10-11 22:15:15 DP1 TP8] Decode batch. #running-req: 32, #token: 20554, token usage: 0.12, accept len: 1.00, cuda graph: True, gen throughput (token/s): 345.91, #queue-req: 352, 
[1,1]<stdout>:[2025-10-11 22:15:15 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 541, #cached-token: 2, token usage: 0.12, #running-req: 31, #queue-req: 351, 
[1,1]<stdout>:[2025-10-11 22:15:15 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 343, #cached-token: 2, token usage: 0.12, #running-req: 31, #queue-req: 350, 
[1,1]<stdout>:[2025-10-11 22:15:15 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 85, #cached-token: 1, token usage: 0.12, #running-req: 31, #queue-req: 349, 
[1,0]<stdout>:[2025-10-11 22:15:16 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 75, #cached-token: 3, token usage: 0.09, #running-req: 31, #queue-req: 316, 
[1,1]<stdout>:[2025-10-11 22:15:16 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 286, #cached-token: 4, token usage: 0.12, #running-req: 31, #queue-req: 348, 
[1,1]<stdout>:[2025-10-11 22:15:16 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 5, #cached-token: 1, token usage: 0.12, #running-req: 31, #queue-req: 347, 
[1,0]<stdout>:[2025-10-11 22:15:17 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 9, #cached-token: 3, token usage: 0.09, #running-req: 31, #queue-req: 315, 
[1,1]<stdout>:[2025-10-11 22:15:17 DP1 TP8] Prefill batch. #new-seq: 2, #new-token: 1818, #cached-token: 3, token usage: 0.12, #running-req: 30, #queue-req: 345, 
[1,0]<stdout>:[2025-10-11 22:15:17 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 2, #cached-token: 3, token usage: 0.08, #running-req: 31, #queue-req: 314, 
[1,1]<stdout>:[2025-10-11 22:15:17 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 225, #cached-token: 1, token usage: 0.12, #running-req: 31, #queue-req: 344, 
[1,1]<stdout>:[2025-10-11 22:15:17 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 814, #cached-token: 10, token usage: 0.12, #running-req: 31, #queue-req: 343, 
[1,0]<stdout>:[2025-10-11 22:15:18 DP0 TP0] Decode batch. #running-req: 32, #token: 13679, token usage: 0.08, accept len: 1.00, cuda graph: True, gen throughput (token/s): 417.61, #queue-req: 314, 
[1,1]<stdout>:[2025-10-11 22:15:18 DP1 TP8] Decode batch. #running-req: 32, #token: 22179, token usage: 0.13, accept len: 1.00, cuda graph: True, gen throughput (token/s): 415.70, #queue-req: 343, 
[1,1]<stdout>:[2025-10-11 22:15:19 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 2048, #cached-token: 1, token usage: 0.13, #running-req: 31, #queue-req: 342, 
[1,1]<stdout>:[2025-10-11 22:15:19 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 25, #cached-token: 0, token usage: 0.14, #running-req: 31, #queue-req: 342, 
[1,1]<stdout>:[2025-10-11 22:15:19 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 10, #cached-token: 1, token usage: 0.14, #running-req: 31, #queue-req: 341, 
[1,1]<stdout>:[2025-10-11 22:15:20 DP1 TP8] Decode batch. #running-req: 32, #token: 25149, token usage: 0.15, accept len: 1.00, cuda graph: True, gen throughput (token/s): 551.99, #queue-req: 341, 
[1,0]<stdout>:[2025-10-11 22:15:20 DP0 TP0] Decode batch. #running-req: 32, #token: 13877, token usage: 0.08, accept len: 1.00, cuda graph: True, gen throughput (token/s): 552.69, #queue-req: 314, 
[1,0]<stdout>:[2025-10-11 22:15:20 DP0 TP0] Prefill batch. #new-seq: 2, #new-token: 483, #cached-token: 6, token usage: 0.08, #running-req: 30, #queue-req: 312, 
[1,0]<stdout>:[2025-10-11 22:15:20 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 590, #cached-token: 5, token usage: 0.08, #running-req: 31, #queue-req: 311, 
[1,0]<stdout>:[2025-10-11 22:15:21 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 33, #cached-token: 2, token usage: 0.08, #running-req: 31, #queue-req: 310, 
[1,0]<stdout>:[2025-10-11 22:15:21 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 29, #cached-token: 2, token usage: 0.08, #running-req: 31, #queue-req: 309, 
[1,1]<stdout>:[2025-10-11 22:15:22 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 20, #cached-token: 3, token usage: 0.15, #running-req: 31, #queue-req: 340, 
[1,1]<stdout>:[2025-10-11 22:15:22 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 519, #cached-token: 7, token usage: 0.14, #running-req: 31, #queue-req: 339, 
[1,0]<stdout>:[2025-10-11 22:15:22 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 12, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 308, 
[1,1]<stdout>:[2025-10-11 22:15:22 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 44, #cached-token: 3, token usage: 0.14, #running-req: 31, #queue-req: 338, 
[1,1]<stdout>:[2025-10-11 22:15:23 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 44, #cached-token: 2, token usage: 0.14, #running-req: 31, #queue-req: 337, 
[1,0]<stdout>:[2025-10-11 22:15:23 DP0 TP0] Decode batch. #running-req: 32, #token: 14387, token usage: 0.08, accept len: 1.00, cuda graph: True, gen throughput (token/s): 396.43, #queue-req: 308, 
[1,1]<stdout>:[2025-10-11 22:15:23 DP1 TP8] Decode batch. #running-req: 32, #token: 24573, token usage: 0.14, accept len: 1.00, cuda graph: True, gen throughput (token/s): 396.97, #queue-req: 337, 
[1,0]<stdout>:[2025-10-11 22:15:24 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 7, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 307, 
[1,1]<stdout>:[2025-10-11 22:15:24 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 641, #cached-token: 3, token usage: 0.12, #running-req: 31, #queue-req: 336, 
[1,0]<stdout>:[2025-10-11 22:15:25 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 23, #cached-token: 3, token usage: 0.09, #running-req: 31, #queue-req: 306, 
[1,0]<stdout>:[2025-10-11 22:15:25 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 408, #cached-token: 2, token usage: 0.09, #running-req: 31, #queue-req: 305, 
[1,1]<stdout>:[2025-10-11 22:15:25 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 678, #cached-token: 5, token usage: 0.12, #running-req: 31, #queue-req: 335, 
[1,1]<stdout>:[2025-10-11 22:15:25 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 78, #cached-token: 8, token usage: 0.12, #running-req: 31, #queue-req: 334, 
[1,1]<stdout>:[2025-10-11 22:15:25 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 349, #cached-token: 2, token usage: 0.12, #running-req: 31, #queue-req: 333, 
[1,1]<stdout>:[2025-10-11 22:15:26 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 268, #cached-token: 5, token usage: 0.12, #running-req: 31, #queue-req: 332, 
[1,1]<stdout>:[2025-10-11 22:15:26 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 63, #cached-token: 4, token usage: 0.12, #running-req: 31, #queue-req: 331, 
[1,0]<stdout>:[2025-10-11 22:15:26 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 35, #cached-token: 2, token usage: 0.09, #running-req: 31, #queue-req: 304, 
[1,1]<stdout>:[2025-10-11 22:15:26 DP1 TP8] Prefill batch. #new-seq: 2, #new-token: 119, #cached-token: 4, token usage: 0.11, #running-req: 30, #queue-req: 329, 
[1,0]<stdout>:[2025-10-11 22:15:27 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 291, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 303, 
[1,1]<stdout>:[2025-10-11 22:15:27 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 45, #cached-token: 3, token usage: 0.11, #running-req: 31, #queue-req: 328, 
[1,0]<stdout>:[2025-10-11 22:15:27 DP0 TP0] Decode batch. #running-req: 32, #token: 15437, token usage: 0.09, accept len: 1.00, cuda graph: True, gen throughput (token/s): 366.52, #queue-req: 303, 
[1,1]<stdout>:[2025-10-11 22:15:27 DP1 TP8] Decode batch. #running-req: 32, #token: 18974, token usage: 0.11, accept len: 1.00, cuda graph: True, gen throughput (token/s): 365.38, #queue-req: 328, 
[1,1]<stdout>:[2025-10-11 22:15:27 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 365, #cached-token: 3, token usage: 0.11, #running-req: 31, #queue-req: 327, 
[1,0]<stdout>:[2025-10-11 22:15:27 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 22, #cached-token: 3, token usage: 0.09, #running-req: 31, #queue-req: 302, 
[1,0]<stdout>:[2025-10-11 22:15:27 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 414, #cached-token: 3, token usage: 0.09, #running-req: 31, #queue-req: 301, 
[1,0]<stdout>:[2025-10-11 22:15:28 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 11, #cached-token: 4, token usage: 0.09, #running-req: 31, #queue-req: 300, 
[1,1]<stdout>:[2025-10-11 22:15:28 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 778, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 326, 
[1,1]<stdout>:[2025-10-11 22:15:28 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 12, #cached-token: 2, token usage: 0.09, #running-req: 31, #queue-req: 325, 
[1,0]<stdout>:[2025-10-11 22:15:28 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 589, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 299, 
[1,1]<stdout>:[2025-10-11 22:15:29 DP1 TP8] Prefill batch. #new-seq: 2, #new-token: 29, #cached-token: 2, token usage: 0.09, #running-req: 30, #queue-req: 323, 
[1,1]<stdout>:[2025-10-11 22:15:29 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 8, #cached-token: 3, token usage: 0.09, #running-req: 31, #queue-req: 322, 
[1,1]<stdout>:[2025-10-11 22:15:29 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 18, #cached-token: 4, token usage: 0.08, #running-req: 31, #queue-req: 321, 
[1,0]<stdout>:[2025-10-11 22:15:29 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 506, #cached-token: 2, token usage: 0.09, #running-req: 31, #queue-req: 298, 
[1,0]<stdout>:[2025-10-11 22:15:30 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 19, #cached-token: 2, token usage: 0.09, #running-req: 31, #queue-req: 297, 
[1,1]<stdout>:[2025-10-11 22:15:30 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 226, #cached-token: 2, token usage: 0.08, #running-req: 31, #queue-req: 320, 
[1,0]<stdout>:[2025-10-11 22:15:30 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 58, #cached-token: 2, token usage: 0.09, #running-req: 31, #queue-req: 296, 
[1,1]<stdout>:[2025-10-11 22:15:30 DP1 TP8] Decode batch. #running-req: 32, #token: 13860, token usage: 0.08, accept len: 1.00, cuda graph: True, gen throughput (token/s): 351.69, #queue-req: 320, 
[1,0]<stdout>:[2025-10-11 22:15:30 DP0 TP0] Decode batch. #running-req: 32, #token: 15341, token usage: 0.09, accept len: 1.00, cuda graph: True, gen throughput (token/s): 351.97, #queue-req: 296, 
[1,0]<stdout>:[2025-10-11 22:15:31 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 12, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 295, 
[1,1]<stdout>:[2025-10-11 22:15:31 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 767, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 319, 
[1,0]<stdout>:[2025-10-11 22:15:31 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 17, #cached-token: 2, token usage: 0.09, #running-req: 31, #queue-req: 294, 
[1,1]<stdout>:[2025-10-11 22:15:31 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 782, #cached-token: 6, token usage: 0.08, #running-req: 31, #queue-req: 318, 
[1,0]<stdout>:[2025-10-11 22:15:31 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 201, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 293, 
[1,1]<stdout>:[2025-10-11 22:15:32 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 19, #cached-token: 3, token usage: 0.08, #running-req: 31, #queue-req: 317, 
[1,0]<stdout>:[2025-10-11 22:15:32 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 209, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 292, 
[1,0]<stdout>:[2025-10-11 22:15:32 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 22, #cached-token: 3, token usage: 0.08, #running-req: 31, #queue-req: 291, 
[1,1]<stdout>:[2025-10-11 22:15:32 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 2048, #cached-token: 3, token usage: 0.09, #running-req: 31, #queue-req: 316, 
[1,1]<stdout>:[2025-10-11 22:15:33 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 911, #cached-token: 0, token usage: 0.10, #running-req: 31, #queue-req: 316, 
[1,0]<stdout>:[2025-10-11 22:15:33 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 12, #cached-token: 4, token usage: 0.08, #running-req: 31, #queue-req: 290, 
[1,1]<stdout>:[2025-10-11 22:15:33 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 233, #cached-token: 4, token usage: 0.10, #running-req: 31, #queue-req: 315, 
[1,0]<stdout>:[2025-10-11 22:15:33 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 477, #cached-token: 4, token usage: 0.08, #running-req: 31, #queue-req: 289, 
[1,1]<stdout>:[2025-10-11 22:15:34 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 93, #cached-token: 2, token usage: 0.10, #running-req: 31, #queue-req: 314, 
[1,0]<stdout>:[2025-10-11 22:15:34 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 318, #cached-token: 3, token usage: 0.08, #running-req: 31, #queue-req: 288, 
[1,0]<stdout>:[2025-10-11 22:15:34 DP0 TP0] Decode batch. #running-req: 32, #token: 13928, token usage: 0.08, accept len: 1.00, cuda graph: True, gen throughput (token/s): 336.33, #queue-req: 288, 
[1,1]<stdout>:[2025-10-11 22:15:34 DP1 TP8] Decode batch. #running-req: 32, #token: 18146, token usage: 0.11, accept len: 1.00, cuda graph: True, gen throughput (token/s): 336.85, #queue-req: 314, 
[1,1]<stdout>:[2025-10-11 22:15:34 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 194, #cached-token: 2, token usage: 0.10, #running-req: 31, #queue-req: 313, 
[1,0]<stdout>:[2025-10-11 22:15:35 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 861, #cached-token: 4, token usage: 0.07, #running-req: 31, #queue-req: 287, 
[1,0]<stdout>:[2025-10-11 22:15:35 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 101, #cached-token: 3, token usage: 0.08, #running-req: 31, #queue-req: 286, 
[1,0]<stdout>:[2025-10-11 22:15:35 DP0 TP0] Prefill batch. #new-seq: 2, #new-token: 1061, #cached-token: 11, token usage: 0.07, #running-req: 30, #queue-req: 284, 
[1,0]<stdout>:[2025-10-11 22:15:35 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 511, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 283, 
[1,1]<stdout>:[2025-10-11 22:15:36 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 442, #cached-token: 7, token usage: 0.11, #running-req: 31, #queue-req: 312, 
[1,0]<stdout>:[2025-10-11 22:15:36 DP0 TP0] Prefill batch. #new-seq: 3, #new-token: 1049, #cached-token: 7, token usage: 0.07, #running-req: 29, #queue-req: 280, 
[1,1]<stdout>:[2025-10-11 22:15:36 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 271, #cached-token: 2, token usage: 0.11, #running-req: 31, #queue-req: 311, 
[1,0]<stdout>:[2025-10-11 22:15:37 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 19, #cached-token: 2, token usage: 0.08, #running-req: 31, #queue-req: 279, 
[1,0]<stdout>:[2025-10-11 22:15:37 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 6, #cached-token: 2, token usage: 0.07, #running-req: 31, #queue-req: 278, 
[1,1]<stdout>:[2025-10-11 22:15:37 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 850, #cached-token: 7, token usage: 0.11, #running-req: 31, #queue-req: 310, 
[1,1]<stdout>:[2025-10-11 22:15:37 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 19, #cached-token: 3, token usage: 0.11, #running-req: 31, #queue-req: 309, 
[1,1]<stdout>:[2025-10-11 22:15:37 DP1 TP8] Prefill batch. #new-seq: 2, #new-token: 19, #cached-token: 6, token usage: 0.10, #running-req: 30, #queue-req: 307, 
[1,0]<stdout>:[2025-10-11 22:15:38 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 807, #cached-token: 2, token usage: 0.07, #running-req: 31, #queue-req: 277, 
[1,1]<stdout>:[2025-10-11 22:15:38 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 626, #cached-token: 3, token usage: 0.10, #running-req: 31, #queue-req: 306, 
[1,1]<stdout>:[2025-10-11 22:15:38 DP1 TP8] Decode batch. #running-req: 32, #token: 18380, token usage: 0.11, accept len: 1.00, cuda graph: True, gen throughput (token/s): 321.51, #queue-req: 306, 
[1,0]<stdout>:[2025-10-11 22:15:38 DP0 TP0] Decode batch. #running-req: 31, #token: 12756, token usage: 0.08, accept len: 1.00, cuda graph: True, gen throughput (token/s): 320.48, #queue-req: 277, 
[1,0]<stdout>:[2025-10-11 22:15:38 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 2, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 276, 
[1,1]<stdout>:[2025-10-11 22:15:39 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 2, #cached-token: 1, token usage: 0.10, #running-req: 31, #queue-req: 305, 
[1,0]<stdout>:[2025-10-11 22:15:39 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 23, #cached-token: 1, token usage: 0.07, #running-req: 31, #queue-req: 275, 
[1,0]<stdout>:[2025-10-11 22:15:39 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 9, #cached-token: 1, token usage: 0.07, #running-req: 31, #queue-req: 274, 
[1,1]<stdout>:[2025-10-11 22:15:39 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 35, #cached-token: 2, token usage: 0.10, #running-req: 31, #queue-req: 304, 
[1,1]<stdout>:[2025-10-11 22:15:40 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 12, #cached-token: 4, token usage: 0.10, #running-req: 31, #queue-req: 303, 
[1,1]<stdout>:[2025-10-11 22:15:40 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 18, #cached-token: 2, token usage: 0.10, #running-req: 31, #queue-req: 302, 
[1,1]<stdout>:[2025-10-11 22:15:40 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 33, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 301, 
[1,1]<stdout>:[2025-10-11 22:15:41 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 54, #cached-token: 3, token usage: 0.09, #running-req: 31, #queue-req: 300, 
[1,1]<stdout>:[2025-10-11 22:15:41 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 9, #cached-token: 4, token usage: 0.09, #running-req: 31, #queue-req: 299, 
[1,1]<stdout>:[2025-10-11 22:15:41 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 275, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 298, 
[1,0]<stdout>:[2025-10-11 22:15:42 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 6, #cached-token: 2, token usage: 0.08, #running-req: 31, #queue-req: 273, 
[1,1]<stdout>:[2025-10-11 22:15:42 DP1 TP8] Decode batch. #running-req: 32, #token: 15274, token usage: 0.09, accept len: 1.00, cuda graph: True, gen throughput (token/s): 355.83, #queue-req: 298, 
[1,0]<stdout>:[2025-10-11 22:15:42 DP0 TP0] Decode batch. #running-req: 32, #token: 11891, token usage: 0.07, accept len: 1.00, cuda graph: True, gen throughput (token/s): 357.21, #queue-req: 273, 
[1,0]<stdout>:[2025-10-11 22:15:42 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 118, #cached-token: 1, token usage: 0.07, #running-req: 31, #queue-req: 272, 
[1,1]<stdout>:[2025-10-11 22:15:42 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 119, #cached-token: 7, token usage: 0.08, #running-req: 31, #queue-req: 297, 
[1,1]<stdout>:[2025-10-11 22:15:42 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 393, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 296, 
[1,1]<stdout>:[2025-10-11 22:15:43 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 11, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 295, 
[1,1]<stdout>:[2025-10-11 22:15:43 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 9, #cached-token: 3, token usage: 0.08, #running-req: 31, #queue-req: 294, 
[1,0]<stdout>:[2025-10-11 22:15:43 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 310, #cached-token: 1, token usage: 0.07, #running-req: 31, #queue-req: 271, 
[1,1]<stdout>:[2025-10-11 22:15:43 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 1388, #cached-token: 2, token usage: 0.08, #running-req: 31, #queue-req: 293, 
[1,1]<stdout>:[2025-10-11 22:15:44 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 237, #cached-token: 3, token usage: 0.08, #running-req: 31, #queue-req: 292, 
[1,0]<stdout>:[2025-10-11 22:15:44 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 1117, #cached-token: 2, token usage: 0.07, #running-req: 31, #queue-req: 270, 
[1,0]<stdout>:[2025-10-11 22:15:45 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 32, #cached-token: 2, token usage: 0.08, #running-req: 31, #queue-req: 269, 
[1,1]<stdout>:[2025-10-11 22:15:45 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 458, #cached-token: 2, token usage: 0.08, #running-req: 31, #queue-req: 291, 
[1,1]<stdout>:[2025-10-11 22:15:45 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 877, token usage: 0.09, #running-req: 31, #queue-req: 290, 
[1,1]<stdout>:[2025-10-11 22:15:45 DP1 TP8] Prefill batch. #new-seq: 2, #new-token: 539, #cached-token: 5, token usage: 0.08, #running-req: 30, #queue-req: 288, 
[1,1]<stdout>:[2025-10-11 22:15:46 DP1 TP8] Decode batch. #running-req: 32, #token: 14921, token usage: 0.09, accept len: 1.00, cuda graph: True, gen throughput (token/s): 337.11, #queue-req: 288, 
[1,0]<stdout>:[2025-10-11 22:15:46 DP0 TP0] Decode batch. #running-req: 32, #token: 13954, token usage: 0.08, accept len: 1.00, cuda graph: True, gen throughput (token/s): 338.73, #queue-req: 269, 
[1,0]<stdout>:[2025-10-11 22:15:46 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 34, #cached-token: 3, token usage: 0.08, #running-req: 31, #queue-req: 268, 
[1,1]<stdout>:[2025-10-11 22:15:46 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 11, #cached-token: 3, token usage: 0.09, #running-req: 31, #queue-req: 287, 
[1,1]<stdout>:[2025-10-11 22:15:47 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 14, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 286, 
[1,1]<stdout>:[2025-10-11 22:15:48 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 187, #cached-token: 2, token usage: 0.09, #running-req: 31, #queue-req: 285, 
[1,1]<stdout>:[2025-10-11 22:15:48 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 2, #cached-token: 3, token usage: 0.09, #running-req: 31, #queue-req: 284, 
[1,0]<stdout>:[2025-10-11 22:15:48 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 667, #cached-token: 6, token usage: 0.08, #running-req: 31, #queue-req: 267, 
[1,1]<stdout>:[2025-10-11 22:15:48 DP1 TP8] Decode batch. #running-req: 32, #token: 15313, token usage: 0.09, accept len: 1.00, cuda graph: True, gen throughput (token/s): 466.77, #queue-req: 284, 
[1,0]<stdout>:[2025-10-11 22:15:48 DP0 TP0] Decode batch. #running-req: 32, #token: 14953, token usage: 0.09, accept len: 1.00, cuda graph: True, gen throughput (token/s): 467.49, #queue-req: 267, 
[1,0]<stdout>:[2025-10-11 22:15:48 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 16, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 266, 
[1,1]<stdout>:[2025-10-11 22:15:49 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 6, #cached-token: 2, token usage: 0.08, #running-req: 31, #queue-req: 283, 
[1,0]<stdout>:[2025-10-11 22:15:49 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 20, #cached-token: 3, token usage: 0.09, #running-req: 31, #queue-req: 265, 
[1,1]<stdout>:[2025-10-11 22:15:49 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 843, #cached-token: 9, token usage: 0.07, #running-req: 31, #queue-req: 282, 
[1,0]<stdout>:[2025-10-11 22:15:50 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 390, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 264, 
[1,0]<stdout>:[2025-10-11 22:15:50 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 51, #cached-token: 3, token usage: 0.09, #running-req: 31, #queue-req: 263, 
[1,1]<stdout>:[2025-10-11 22:15:50 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 18, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 281, 
[1,1]<stdout>:[2025-10-11 22:15:51 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 445, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 280, 
[1,1]<stdout>:[2025-10-11 22:15:51 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 2048, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 279, 
[1,1]<stdout>:[2025-10-11 22:15:51 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 148, #cached-token: 0, token usage: 0.09, #running-req: 31, #queue-req: 279, 
[1,1]<stdout>:[2025-10-11 22:15:52 DP1 TP8] Decode batch. #running-req: 31, #token: 15717, token usage: 0.09, accept len: 1.00, cuda graph: True, gen throughput (token/s): 383.45, #queue-req: 279, 
[1,0]<stdout>:[2025-10-11 22:15:52 DP0 TP0] Decode batch. #running-req: 32, #token: 15266, token usage: 0.09, accept len: 1.00, cuda graph: True, gen throughput (token/s): 384.06, #queue-req: 263, 
[1,1]<stdout>:[2025-10-11 22:15:52 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 11, #cached-token: 3, token usage: 0.09, #running-req: 31, #queue-req: 278, 
[1,0]<stdout>:[2025-10-11 22:15:52 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 17, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 262, 
[1,1]<stdout>:[2025-10-11 22:15:52 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 6, token usage: 0.08, #running-req: 31, #queue-req: 277, 
[1,0]<stdout>:[2025-10-11 22:15:53 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 10, #cached-token: 3, token usage: 0.08, #running-req: 31, #queue-req: 261, 
[1,0]<stdout>:[2025-10-11 22:15:53 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 8, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 260, 
[1,1]<stdout>:[2025-10-11 22:15:53 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 31, #cached-token: 2, token usage: 0.08, #running-req: 31, #queue-req: 276, 
[1,1]<stdout>:[2025-10-11 22:15:54 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 68, #cached-token: 1, token usage: 0.07, #running-req: 31, #queue-req: 275, 
[1,1]<stdout>:[2025-10-11 22:15:54 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 8, #cached-token: 4, token usage: 0.05, #running-req: 31, #queue-req: 274, 
[1,1]<stdout>:[2025-10-11 22:15:55 DP1 TP8] Decode batch. #running-req: 32, #token: 9413, token usage: 0.06, accept len: 1.00, cuda graph: True, gen throughput (token/s): 421.90, #queue-req: 274, 
[1,0]<stdout>:[2025-10-11 22:15:55 DP0 TP0] Decode batch. #running-req: 32, #token: 14910, token usage: 0.09, accept len: 1.00, cuda graph: True, gen throughput (token/s): 422.26, #queue-req: 260, 
[1,0]<stdout>:[2025-10-11 22:15:55 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 20, #cached-token: 3, token usage: 0.09, #running-req: 31, #queue-req: 259, 
[1,1]<stdout>:[2025-10-11 22:15:55 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 26, #cached-token: 3, token usage: 0.05, #running-req: 31, #queue-req: 273, 
[1,1]<stdout>:[2025-10-11 22:15:55 DP1 TP8] Prefill batch. #new-seq: 2, #new-token: 370, #cached-token: 6, token usage: 0.05, #running-req: 30, #queue-req: 271, 
[1,0]<stdout>:[2025-10-11 22:15:55 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 124, #cached-token: 2, token usage: 0.09, #running-req: 31, #queue-req: 258, 
[1,0]<stdout>:[2025-10-11 22:15:56 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 29, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 257, 
[1,0]<stdout>:[2025-10-11 22:15:56 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 10, #cached-token: 2, token usage: 0.08, #running-req: 31, #queue-req: 256, 
[1,1]<stdout>:[2025-10-11 22:15:56 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 259, #cached-token: 3, token usage: 0.05, #running-req: 31, #queue-req: 270, 
[1,1]<stdout>:[2025-10-11 22:15:57 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 473, #cached-token: 3, token usage: 0.05, #running-req: 31, #queue-req: 269, 
[1,1]<stdout>:[2025-10-11 22:15:57 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 23, #cached-token: 3, token usage: 0.05, #running-req: 31, #queue-req: 268, 
[1,1]<stdout>:[2025-10-11 22:15:57 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 586, #cached-token: 3, token usage: 0.05, #running-req: 31, #queue-req: 267, 
[1,0]<stdout>:[2025-10-11 22:15:57 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 257, #cached-token: 2, token usage: 0.07, #running-req: 31, #queue-req: 255, 
[1,1]<stdout>:[2025-10-11 22:15:57 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 69, #cached-token: 1, token usage: 0.05, #running-req: 31, #queue-req: 266, 
[1,0]<stdout>:[2025-10-11 22:15:58 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 539, #cached-token: 2, token usage: 0.07, #running-req: 31, #queue-req: 254, 
[1,1]<stdout>:[2025-10-11 22:15:58 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 25, #cached-token: 1, token usage: 0.05, #running-req: 31, #queue-req: 265, 
[1,1]<stdout>:[2025-10-11 22:15:58 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 117, #cached-token: 3, token usage: 0.04, #running-req: 31, #queue-req: 264, 
[1,1]<stdout>:[2025-10-11 22:15:58 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 300, #cached-token: 1, token usage: 0.05, #running-req: 31, #queue-req: 263, 
[1,1]<stdout>:[2025-10-11 22:15:58 DP1 TP8] Decode batch. #running-req: 32, #token: 8031, token usage: 0.05, accept len: 1.00, cuda graph: True, gen throughput (token/s): 336.94, #queue-req: 263, 
[1,0]<stdout>:[2025-10-11 22:15:58 DP0 TP0] Decode batch. #running-req: 31, #token: 11977, token usage: 0.07, accept len: 1.00, cuda graph: True, gen throughput (token/s): 337.98, #queue-req: 254, 
[1,0]<stdout>:[2025-10-11 22:15:58 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 17, #cached-token: 2, token usage: 0.07, #running-req: 31, #queue-req: 253, 
[1,1]<stdout>:[2025-10-11 22:15:59 DP1 TP8] Prefill batch. #new-seq: 2, #new-token: 437, #cached-token: 6, token usage: 0.05, #running-req: 30, #queue-req: 261, 
[1,1]<stdout>:[2025-10-11 22:15:59 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 149, #cached-token: 4, token usage: 0.05, #running-req: 31, #queue-req: 260, 
[1,0]<stdout>:[2025-10-11 22:15:59 DP0 TP0] Prefill batch. #new-seq: 2, #new-token: 597, #cached-token: 6, token usage: 0.06, #running-req: 30, #queue-req: 251, 
[1,1]<stdout>:[2025-10-11 22:15:59 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 15, #cached-token: 1, token usage: 0.04, #running-req: 31, #queue-req: 259, 
[1,0]<stdout>:[2025-10-11 22:15:59 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 13, #cached-token: 1, token usage: 0.06, #running-req: 31, #queue-req: 250, 
[1,1]<stdout>:[2025-10-11 22:16:00 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 219, #cached-token: 2, token usage: 0.04, #running-req: 31, #queue-req: 258, 
[1,1]<stdout>:[2025-10-11 22:16:00 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 6, #cached-token: 3, token usage: 0.04, #running-req: 31, #queue-req: 257, 
[1,0]<stdout>:[2025-10-11 22:16:00 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 440, #cached-token: 5, token usage: 0.06, #running-req: 31, #queue-req: 249, 
[1,1]<stdout>:[2025-10-11 22:16:01 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 1771, #cached-token: 1, token usage: 0.04, #running-req: 31, #queue-req: 256, 
[1,0]<stdout>:[2025-10-11 22:16:01 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 185, #cached-token: 2, token usage: 0.06, #running-req: 31, #queue-req: 248, 
[1,0]<stdout>:[2025-10-11 22:16:01 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 402, #cached-token: 1, token usage: 0.06, #running-req: 31, #queue-req: 247, 
[1,0]<stdout>:[2025-10-11 22:16:01 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 543, #cached-token: 2, token usage: 0.06, #running-req: 31, #queue-req: 246, 
[1,1]<stdout>:[2025-10-11 22:16:01 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 18, #cached-token: 1, token usage: 0.05, #running-req: 31, #queue-req: 255, 
[1,0]<stdout>:[2025-10-11 22:16:02 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 63, #cached-token: 2, token usage: 0.06, #running-req: 31, #queue-req: 245, 
[1,1]<stdout>:[2025-10-11 22:16:02 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 4, #cached-token: 2, token usage: 0.05, #running-req: 31, #queue-req: 254, 
[1,0]<stdout>:[2025-10-11 22:16:02 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 787, #cached-token: 6, token usage: 0.06, #running-req: 31, #queue-req: 244, 
[1,0]<stdout>:[2025-10-11 22:16:02 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 72, #cached-token: 3, token usage: 0.07, #running-req: 31, #queue-req: 243, 
[1,0]<stdout>:[2025-10-11 22:16:03 DP0 TP0] Decode batch. #running-req: 32, #token: 11325, token usage: 0.07, accept len: 1.00, cuda graph: True, gen throughput (token/s): 301.14, #queue-req: 243, 
[1,1]<stdout>:[2025-10-11 22:16:03 DP1 TP8] Decode batch. #running-req: 32, #token: 9238, token usage: 0.05, accept len: 1.00, cuda graph: True, gen throughput (token/s): 301.37, #queue-req: 254, 
[1,0]<stdout>:[2025-10-11 22:16:03 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 636, #cached-token: 1, token usage: 0.07, #running-req: 31, #queue-req: 242, 
[1,0]<stdout>:[2025-10-11 22:16:03 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 35, #cached-token: 3, token usage: 0.06, #running-req: 31, #queue-req: 241, 
[1,0]<stdout>:[2025-10-11 22:16:03 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 24, #cached-token: 2, token usage: 0.06, #running-req: 31, #queue-req: 240, 
[1,0]<stdout>:[2025-10-11 22:16:03 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 1, token usage: 0.06, #running-req: 31, #queue-req: 239, 
[1,0]<stdout>:[2025-10-11 22:16:04 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 2048, #cached-token: 1, token usage: 0.06, #running-req: 31, #queue-req: 238, 
[1,0]<stdout>:[2025-10-11 22:16:04 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 356, #cached-token: 0, token usage: 0.07, #running-req: 31, #queue-req: 238, 
[1,0]<stdout>:[2025-10-11 22:16:04 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 504, token usage: 0.08, #running-req: 31, #queue-req: 237, 
[1,1]<stdout>:[2025-10-11 22:16:05 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 248, #cached-token: 3, token usage: 0.06, #running-req: 31, #queue-req: 253, 
[1,1]<stdout>:[2025-10-11 22:16:05 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 166, #cached-token: 2, token usage: 0.06, #running-req: 31, #queue-req: 252, 
[1,0]<stdout>:[2025-10-11 22:16:05 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 406, #cached-token: 3, token usage: 0.07, #running-req: 31, #queue-req: 236, 
[1,1]<stdout>:[2025-10-11 22:16:05 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 335, #cached-token: 5, token usage: 0.06, #running-req: 31, #queue-req: 251, 
[1,0]<stdout>:[2025-10-11 22:16:06 DP0 TP0] Prefill batch. #new-seq: 2, #new-token: 368, #cached-token: 5, token usage: 0.07, #running-req: 30, #queue-req: 234, 
[1,0]<stdout>:[2025-10-11 22:16:06 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 9, #cached-token: 3, token usage: 0.07, #running-req: 31, #queue-req: 233, 
[1,0]<stdout>:[2025-10-11 22:16:06 DP0 TP0] Decode batch. #running-req: 32, #token: 12826, token usage: 0.08, accept len: 1.00, cuda graph: True, gen throughput (token/s): 351.25, #queue-req: 233, 
[1,1]<stdout>:[2025-10-11 22:16:06 DP1 TP8] Decode batch. #running-req: 31, #token: 10178, token usage: 0.06, accept len: 1.00, cuda graph: True, gen throughput (token/s): 352.89, #queue-req: 251, 
[1,1]<stdout>:[2025-10-11 22:16:06 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 49, #cached-token: 2, token usage: 0.06, #running-req: 31, #queue-req: 250, 
[1,0]<stdout>:[2025-10-11 22:16:06 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 355, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 232, 
[1,1]<stdout>:[2025-10-11 22:16:06 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 1051, #cached-token: 1, token usage: 0.06, #running-req: 31, #queue-req: 249, 
[1,1]<stdout>:[2025-10-11 22:16:07 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 4, #cached-token: 2, token usage: 0.06, #running-req: 31, #queue-req: 248, 
[1,1]<stdout>:[2025-10-11 22:16:07 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 377, #cached-token: 2, token usage: 0.06, #running-req: 31, #queue-req: 247, 
[1,0]<stdout>:[2025-10-11 22:16:07 DP0 TP0] Prefill batch. #new-seq: 3, #new-token: 376, #cached-token: 15, token usage: 0.08, #running-req: 29, #queue-req: 229, 
[1,1]<stdout>:[2025-10-11 22:16:08 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 172, #cached-token: 1, token usage: 0.06, #running-req: 31, #queue-req: 246, 
[1,1]<stdout>:[2025-10-11 22:16:08 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 6, #cached-token: 3, token usage: 0.07, #running-req: 31, #queue-req: 245, 
[1,0]<stdout>:[2025-10-11 22:16:09 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 21, #cached-token: 2, token usage: 0.08, #running-req: 31, #queue-req: 228, 
[1,1]<stdout>:[2025-10-11 22:16:09 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 54, #cached-token: 2, token usage: 0.06, #running-req: 31, #queue-req: 244, 
[1,1]<stdout>:[2025-10-11 22:16:09 DP1 TP8] Decode batch. #running-req: 32, #token: 10489, token usage: 0.06, accept len: 1.00, cuda graph: True, gen throughput (token/s): 397.12, #queue-req: 244, 
[1,0]<stdout>:[2025-10-11 22:16:09 DP0 TP0] Decode batch. #running-req: 32, #token: 13444, token usage: 0.08, accept len: 1.00, cuda graph: True, gen throughput (token/s): 397.36, #queue-req: 228, 
[1,0]<stdout>:[2025-10-11 22:16:09 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 1958, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 227, 
[1,0]<stdout>:[2025-10-11 22:16:10 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 4, #cached-token: 3, token usage: 0.09, #running-req: 31, #queue-req: 226, 
[1,1]<stdout>:[2025-10-11 22:16:10 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 151, #cached-token: 2, token usage: 0.06, #running-req: 31, #queue-req: 243, 
[1,1]<stdout>:[2025-10-11 22:16:11 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 361, #cached-token: 1, token usage: 0.06, #running-req: 31, #queue-req: 242, 
[1,0]<stdout>:[2025-10-11 22:16:11 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 241, #cached-token: 3, token usage: 0.08, #running-req: 31, #queue-req: 225, 
[1,1]<stdout>:[2025-10-11 22:16:11 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 3, #cached-token: 1, token usage: 0.06, #running-req: 31, #queue-req: 241, 
[1,1]<stdout>:[2025-10-11 22:16:11 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 50, #cached-token: 2, token usage: 0.06, #running-req: 31, #queue-req: 240, 
[1,0]<stdout>:[2025-10-11 22:16:11 DP0 TP0] Prefill batch. #new-seq: 2, #new-token: 116, #cached-token: 4, token usage: 0.08, #running-req: 30, #queue-req: 223, 
[1,1]<stdout>:[2025-10-11 22:16:12 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 720, #cached-token: 3, token usage: 0.07, #running-req: 31, #queue-req: 239, 
[1,1]<stdout>:[2025-10-11 22:16:12 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 92, #cached-token: 5, token usage: 0.07, #running-req: 31, #queue-req: 238, 
[1,0]<stdout>:[2025-10-11 22:16:12 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 14, #cached-token: 2, token usage: 0.07, #running-req: 31, #queue-req: 222, 
[1,0]<stdout>:[2025-10-11 22:16:12 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 782, #cached-token: 3, token usage: 0.07, #running-req: 31, #queue-req: 221, 
[1,0]<stdout>:[2025-10-11 22:16:13 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 68, #cached-token: 1, token usage: 0.07, #running-req: 31, #queue-req: 220, 
[1,1]<stdout>:[2025-10-11 22:16:13 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 30, #cached-token: 3, token usage: 0.07, #running-req: 31, #queue-req: 237, 
[1,1]<stdout>:[2025-10-11 22:16:13 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 12, #cached-token: 3, token usage: 0.07, #running-req: 31, #queue-req: 236, 
[1,1]<stdout>:[2025-10-11 22:16:13 DP1 TP8] Decode batch. #running-req: 32, #token: 11309, token usage: 0.07, accept len: 1.00, cuda graph: True, gen throughput (token/s): 348.85, #queue-req: 236, 
[1,0]<stdout>:[2025-10-11 22:16:13 DP0 TP0] Decode batch. #running-req: 32, #token: 12649, token usage: 0.07, accept len: 1.00, cuda graph: True, gen throughput (token/s): 348.86, #queue-req: 220, 
[1,0]<stdout>:[2025-10-11 22:16:13 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 5, #cached-token: 3, token usage: 0.07, #running-req: 31, #queue-req: 219, 
[1,0]<stdout>:[2025-10-11 22:16:13 DP0 TP0] Prefill batch. #new-seq: 2, #new-token: 212, #cached-token: 2, token usage: 0.07, #running-req: 30, #queue-req: 217, 
[1,0]<stdout>:[2025-10-11 22:16:14 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 38, #cached-token: 3, token usage: 0.07, #running-req: 31, #queue-req: 216, 
[1,0]<stdout>:[2025-10-11 22:16:15 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 402, token usage: 0.08, #running-req: 31, #queue-req: 215, 
[1,0]<stdout>:[2025-10-11 22:16:15 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 8, #cached-token: 5, token usage: 0.07, #running-req: 31, #queue-req: 214, 
[1,0]<stdout>:[2025-10-11 22:16:15 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 295, #cached-token: 6, token usage: 0.07, #running-req: 31, #queue-req: 213, 
[1,0]<stdout>:[2025-10-11 22:16:15 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 20, #cached-token: 1, token usage: 0.07, #running-req: 31, #queue-req: 212, 
[1,0]<stdout>:[2025-10-11 22:16:16 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 17, #cached-token: 1, token usage: 0.07, #running-req: 31, #queue-req: 211, 
[1,0]<stdout>:[2025-10-11 22:16:16 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 626, #cached-token: 4, token usage: 0.07, #running-req: 31, #queue-req: 210, 
[1,1]<stdout>:[2025-10-11 22:16:16 DP1 TP8] Prefill batch. #new-seq: 2, #new-token: 211, #cached-token: 2, token usage: 0.07, #running-req: 30, #queue-req: 234, 
[1,0]<stdout>:[2025-10-11 22:16:16 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 519, #cached-token: 1, token usage: 0.07, #running-req: 31, #queue-req: 209, 
[1,1]<stdout>:[2025-10-11 22:16:16 DP1 TP8] Decode batch. #running-req: 31, #token: 11269, token usage: 0.07, accept len: 1.00, cuda graph: True, gen throughput (token/s): 383.76, #queue-req: 234, 
[1,0]<stdout>:[2025-10-11 22:16:16 DP0 TP0] Decode batch. #running-req: 32, #token: 13026, token usage: 0.08, accept len: 1.00, cuda graph: True, gen throughput (token/s): 381.38, #queue-req: 209, 
[1,1]<stdout>:[2025-10-11 22:16:16 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 28, #cached-token: 3, token usage: 0.07, #running-req: 31, #queue-req: 233, 
[1,1]<stdout>:[2025-10-11 22:16:17 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 212, #cached-token: 1, token usage: 0.06, #running-req: 31, #queue-req: 232, 
[1,0]<stdout>:[2025-10-11 22:16:17 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 18, #cached-token: 1, token usage: 0.07, #running-req: 31, #queue-req: 208, 
[1,1]<stdout>:[2025-10-11 22:16:17 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 1426, #cached-token: 3, token usage: 0.06, #running-req: 31, #queue-req: 231, 
[1,0]<stdout>:[2025-10-11 22:16:18 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 452, #cached-token: 2, token usage: 0.07, #running-req: 31, #queue-req: 207, 
[1,0]<stdout>:[2025-10-11 22:16:18 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 8, #cached-token: 3, token usage: 0.08, #running-req: 31, #queue-req: 206, 
[1,0]<stdout>:[2025-10-11 22:16:18 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 1014, #cached-token: 1, token usage: 0.07, #running-req: 31, #queue-req: 205, 
[1,0]<stdout>:[2025-10-11 22:16:19 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 16, #cached-token: 2, token usage: 0.08, #running-req: 31, #queue-req: 204, 
[1,0]<stdout>:[2025-10-11 22:16:19 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 7, #cached-token: 3, token usage: 0.08, #running-req: 31, #queue-req: 203, 
[1,0]<stdout>:[2025-10-11 22:16:19 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 6, #cached-token: 3, token usage: 0.08, #running-req: 31, #queue-req: 202, 
[1,0]<stdout>:[2025-10-11 22:16:20 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 64, #cached-token: 2, token usage: 0.08, #running-req: 31, #queue-req: 201, 
[1,1]<stdout>:[2025-10-11 22:16:20 DP1 TP8] Decode batch. #running-req: 32, #token: 13233, token usage: 0.08, accept len: 1.00, cuda graph: True, gen throughput (token/s): 369.83, #queue-req: 231, 
[1,0]<stdout>:[2025-10-11 22:16:20 DP0 TP0] Decode batch. #running-req: 32, #token: 12920, token usage: 0.08, accept len: 1.00, cuda graph: True, gen throughput (token/s): 368.12, #queue-req: 201, 
[1,0]<stdout>:[2025-10-11 22:16:20 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 427, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 200, 
[1,0]<stdout>:[2025-10-11 22:16:21 DP0 TP0] Prefill batch. #new-seq: 2, #new-token: 539, #cached-token: 827, token usage: 0.08, #running-req: 30, #queue-req: 198, 
[1,0]<stdout>:[2025-10-11 22:16:21 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 239, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 197, 
[1,0]<stdout>:[2025-10-11 22:16:21 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 441, #cached-token: 2, token usage: 0.08, #running-req: 31, #queue-req: 196, 
[1,0]<stdout>:[2025-10-11 22:16:21 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 93, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 195, 
[1,0]<stdout>:[2025-10-11 22:16:22 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 56, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 194, 
[1,0]<stdout>:[2025-10-11 22:16:22 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 283, #cached-token: 2, token usage: 0.08, #running-req: 31, #queue-req: 193, 
[1,1]<stdout>:[2025-10-11 22:16:22 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 15, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 230, 
[1,0]<stdout>:[2025-10-11 22:16:23 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 485, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 192, 
[1,1]<stdout>:[2025-10-11 22:16:23 DP1 TP8] Decode batch. #running-req: 32, #token: 14268, token usage: 0.08, accept len: 1.00, cuda graph: True, gen throughput (token/s): 396.09, #queue-req: 230, 
[1,0]<stdout>:[2025-10-11 22:16:23 DP0 TP0] Decode batch. #running-req: 32, #token: 14390, token usage: 0.08, accept len: 1.00, cuda graph: True, gen throughput (token/s): 393.59, #queue-req: 192, 
[1,1]<stdout>:[2025-10-11 22:16:23 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 518, #cached-token: 2, token usage: 0.08, #running-req: 31, #queue-req: 229, 
[1,1]<stdout>:[2025-10-11 22:16:23 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 228, 
[1,0]<stdout>:[2025-10-11 22:16:23 DP0 TP0] Prefill batch. #new-seq: 2, #new-token: 1110, #cached-token: 6, token usage: 0.08, #running-req: 30, #queue-req: 190, 
[1,1]<stdout>:[2025-10-11 22:16:24 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 13, #cached-token: 2, token usage: 0.08, #running-req: 31, #queue-req: 227, 
[1,0]<stdout>:[2025-10-11 22:16:24 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 16, #cached-token: 3, token usage: 0.08, #running-req: 31, #queue-req: 189, 
[1,1]<stdout>:[2025-10-11 22:16:24 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 12, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 226, 
[1,0]<stdout>:[2025-10-11 22:16:24 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 2, token usage: 0.08, #running-req: 31, #queue-req: 188, 
[1,1]<stdout>:[2025-10-11 22:16:24 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 9, #cached-token: 1, token usage: 0.07, #running-req: 31, #queue-req: 225, 
[1,0]<stdout>:[2025-10-11 22:16:25 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 833, #cached-token: 7, token usage: 0.08, #running-req: 31, #queue-req: 187, 
[1,1]<stdout>:[2025-10-11 22:16:25 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 164, #cached-token: 2, token usage: 0.08, #running-req: 31, #queue-req: 224, 
[1,1]<stdout>:[2025-10-11 22:16:25 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 126, #cached-token: 2, token usage: 0.08, #running-req: 31, #queue-req: 223, 
[1,0]<stdout>:[2025-10-11 22:16:25 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 592, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 186, 
[1,0]<stdout>:[2025-10-11 22:16:25 DP0 TP0] Prefill batch. #new-seq: 2, #new-token: 1255, #cached-token: 10, token usage: 0.08, #running-req: 30, #queue-req: 184, 
[1,0]<stdout>:[2025-10-11 22:16:26 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 17, #cached-token: 2, token usage: 0.09, #running-req: 31, #queue-req: 183, 
[1,0]<stdout>:[2025-10-11 22:16:26 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 23, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 182, 
[1,0]<stdout>:[2025-10-11 22:16:26 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 10, #cached-token: 3, token usage: 0.07, #running-req: 31, #queue-req: 181, 
[1,0]<stdout>:[2025-10-11 22:16:26 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 554, #cached-token: 2, token usage: 0.07, #running-req: 31, #queue-req: 180, 
[1,0]<stdout>:[2025-10-11 22:16:27 DP0 TP0] Prefill batch. #new-seq: 2, #new-token: 1022, #cached-token: 5, token usage: 0.07, #running-req: 30, #queue-req: 178, 
[1,0]<stdout>:[2025-10-11 22:16:27 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 21, #cached-token: 1, token usage: 0.07, #running-req: 31, #queue-req: 177, 
[1,1]<stdout>:[2025-10-11 22:16:27 DP1 TP8] Decode batch. #running-req: 31, #token: 13623, token usage: 0.08, accept len: 1.00, cuda graph: True, gen throughput (token/s): 291.90, #queue-req: 223, 
[1,0]<stdout>:[2025-10-11 22:16:27 DP0 TP0] Decode batch. #running-req: 30, #token: 11133, token usage: 0.07, accept len: 1.00, cuda graph: True, gen throughput (token/s): 289.81, #queue-req: 177, 
[1,1]<stdout>:[2025-10-11 22:16:27 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 1864, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 222, 
[1,0]<stdout>:[2025-10-11 22:16:27 DP0 TP0] Prefill batch. #new-seq: 2, #new-token: 520, #cached-token: 2, token usage: 0.07, #running-req: 30, #queue-req: 175, 
[1,0]<stdout>:[2025-10-11 22:16:28 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 16, #cached-token: 2, token usage: 0.06, #running-req: 31, #queue-req: 174, 
[1,0]<stdout>:[2025-10-11 22:16:28 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 277, #cached-token: 1, token usage: 0.06, #running-req: 31, #queue-req: 173, 
[1,1]<stdout>:[2025-10-11 22:16:28 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 249, #cached-token: 2, token usage: 0.09, #running-req: 31, #queue-req: 221, 
[1,0]<stdout>:[2025-10-11 22:16:28 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 355, #cached-token: 1, token usage: 0.06, #running-req: 31, #queue-req: 172, 
[1,0]<stdout>:[2025-10-11 22:16:28 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 484, #cached-token: 4, token usage: 0.05, #running-req: 31, #queue-req: 171, 
[1,1]<stdout>:[2025-10-11 22:16:28 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 74, #cached-token: 8, token usage: 0.09, #running-req: 31, #queue-req: 220, 
[1,0]<stdout>:[2025-10-11 22:16:29 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 334, #cached-token: 4, token usage: 0.05, #running-req: 31, #queue-req: 170, 
[1,1]<stdout>:[2025-10-11 22:16:29 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 46, #cached-token: 3, token usage: 0.09, #running-req: 31, #queue-req: 219, 
[1,1]<stdout>:[2025-10-11 22:16:29 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 43, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 218, 
[1,0]<stdout>:[2025-10-11 22:16:30 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 1833, #cached-token: 1, token usage: 0.05, #running-req: 31, #queue-req: 169, 
[1,0]<stdout>:[2025-10-11 22:16:30 DP0 TP0] Prefill batch. #new-seq: 2, #new-token: 847, #cached-token: 8, token usage: 0.06, #running-req: 30, #queue-req: 167, 
[1,0]<stdout>:[2025-10-11 22:16:30 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 444, #cached-token: 6, token usage: 0.07, #running-req: 31, #queue-req: 166, 
[1,1]<stdout>:[2025-10-11 22:16:30 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 236, #cached-token: 3, token usage: 0.08, #running-req: 31, #queue-req: 217, 
[1,1]<stdout>:[2025-10-11 22:16:31 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 19, #cached-token: 3, token usage: 0.09, #running-req: 31, #queue-req: 216, 
[1,0]<stdout>:[2025-10-11 22:16:31 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 6, #cached-token: 2, token usage: 0.06, #running-req: 31, #queue-req: 165, 
[1,1]<stdout>:[2025-10-11 22:16:31 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 294, #cached-token: 4, token usage: 0.08, #running-req: 31, #queue-req: 215, 
[1,0]<stdout>:[2025-10-11 22:16:31 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 739, #cached-token: 2, token usage: 0.06, #running-req: 31, #queue-req: 164, 
[1,0]<stdout>:[2025-10-11 22:16:31 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 232, #cached-token: 8, token usage: 0.06, #running-req: 31, #queue-req: 163, 
[1,0]<stdout>:[2025-10-11 22:16:32 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 625, #cached-token: 4, token usage: 0.06, #running-req: 31, #queue-req: 162, 
[1,0]<stdout>:[2025-10-11 22:16:32 DP0 TP0] Decode batch. #running-req: 31, #token: 9749, token usage: 0.06, accept len: 1.00, cuda graph: True, gen throughput (token/s): 280.82, #queue-req: 162, 
[1,1]<stdout>:[2025-10-11 22:16:32 DP1 TP8] Decode batch. #running-req: 31, #token: 14188, token usage: 0.08, accept len: 1.00, cuda graph: True, gen throughput (token/s): 282.12, #queue-req: 215, 
[1,0]<stdout>:[2025-10-11 22:16:32 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 28, #cached-token: 1, token usage: 0.06, #running-req: 31, #queue-req: 161, 
[1,1]<stdout>:[2025-10-11 22:16:32 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 494, #cached-token: 2, token usage: 0.08, #running-req: 31, #queue-req: 214, 
[1,0]<stdout>:[2025-10-11 22:16:32 DP0 TP0] Prefill batch. #new-seq: 2, #new-token: 804, #cached-token: 6, token usage: 0.06, #running-req: 30, #queue-req: 159, 
[1,0]<stdout>:[2025-10-11 22:16:32 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 58, #cached-token: 2, token usage: 0.06, #running-req: 31, #queue-req: 158, 
[1,1]<stdout>:[2025-10-11 22:16:32 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 4, #cached-token: 3, token usage: 0.08, #running-req: 31, #queue-req: 213, 
[1,1]<stdout>:[2025-10-11 22:16:33 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 35, #cached-token: 3, token usage: 0.08, #running-req: 31, #queue-req: 212, 
[1,1]<stdout>:[2025-10-11 22:16:33 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 268, #cached-token: 3, token usage: 0.08, #running-req: 31, #queue-req: 211, 
[1,0]<stdout>:[2025-10-11 22:16:33 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 130, #cached-token: 4, token usage: 0.05, #running-req: 31, #queue-req: 157, 
[1,1]<stdout>:[2025-10-11 22:16:33 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 768, #cached-token: 5, token usage: 0.08, #running-req: 31, #queue-req: 210, 
[1,1]<stdout>:[2025-10-11 22:16:33 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 609, #cached-token: 2, token usage: 0.08, #running-req: 31, #queue-req: 209, 
[1,1]<stdout>:[2025-10-11 22:16:34 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 134, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 208, 
[1,1]<stdout>:[2025-10-11 22:16:34 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 97, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 207, 
[1,1]<stdout>:[2025-10-11 22:16:34 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 409, #cached-token: 6, token usage: 0.08, #running-req: 31, #queue-req: 206, 
[1,0]<stdout>:[2025-10-11 22:16:35 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 1811, #cached-token: 2, token usage: 0.06, #running-req: 31, #queue-req: 156, 
[1,1]<stdout>:[2025-10-11 22:16:35 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 31, #cached-token: 2, token usage: 0.08, #running-req: 31, #queue-req: 205, 
[1,1]<stdout>:[2025-10-11 22:16:35 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 38, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 204, 
[1,0]<stdout>:[2025-10-11 22:16:36 DP0 TP0] Decode batch. #running-req: 31, #token: 11454, token usage: 0.07, accept len: 1.00, cuda graph: True, gen throughput (token/s): 352.10, #queue-req: 156, 
[1,1]<stdout>:[2025-10-11 22:16:36 DP1 TP8] Decode batch. #running-req: 30, #token: 12497, token usage: 0.07, accept len: 1.00, cuda graph: True, gen throughput (token/s): 350.48, #queue-req: 204, 
[1,0]<stdout>:[2025-10-11 22:16:36 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 58, #cached-token: 1, token usage: 0.07, #running-req: 31, #queue-req: 155, 
[1,1]<stdout>:[2025-10-11 22:16:36 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 2048, #cached-token: 4, token usage: 0.07, #running-req: 30, #queue-req: 203, 
[1,1]<stdout>:[2025-10-11 22:16:36 DP1 TP8] Prefill batch. #new-seq: 2, #new-token: 711, #cached-token: 5, token usage: 0.09, #running-req: 30, #queue-req: 202, 
[1,0]<stdout>:[2025-10-11 22:16:36 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 612, #cached-token: 1, token usage: 0.07, #running-req: 31, #queue-req: 154, 
[1,1]<stdout>:[2025-10-11 22:16:36 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 851, #cached-token: 5, token usage: 0.09, #running-req: 31, #queue-req: 201, 
[1,1]<stdout>:[2025-10-11 22:16:37 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 10, #cached-token: 2, token usage: 0.09, #running-req: 31, #queue-req: 200, 
[1,0]<stdout>:[2025-10-11 22:16:37 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 23, #cached-token: 2, token usage: 0.07, #running-req: 31, #queue-req: 153, 
[1,0]<stdout>:[2025-10-11 22:16:38 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 28, #cached-token: 2, token usage: 0.07, #running-req: 31, #queue-req: 152, 
[1,1]<stdout>:[2025-10-11 22:16:38 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 47, #cached-token: 4, token usage: 0.09, #running-req: 31, #queue-req: 199, 
[1,1]<stdout>:[2025-10-11 22:16:38 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 198, 
[1,0]<stdout>:[2025-10-11 22:16:39 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 415, #cached-token: 1, token usage: 0.07, #running-req: 31, #queue-req: 151, 
[1,0]<stdout>:[2025-10-11 22:16:39 DP0 TP0] Decode batch. #running-req: 32, #token: 12201, token usage: 0.07, accept len: 1.00, cuda graph: True, gen throughput (token/s): 378.72, #queue-req: 151, 
[1,1]<stdout>:[2025-10-11 22:16:39 DP1 TP8] Decode batch. #running-req: 32, #token: 15206, token usage: 0.09, accept len: 1.00, cuda graph: True, gen throughput (token/s): 378.71, #queue-req: 198, 
[1,1]<stdout>:[2025-10-11 22:16:39 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 409, #cached-token: 2, token usage: 0.09, #running-req: 31, #queue-req: 197, 
[1,0]<stdout>:[2025-10-11 22:16:39 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 756, #cached-token: 5, token usage: 0.07, #running-req: 31, #queue-req: 150, 
[1,0]<stdout>:[2025-10-11 22:16:39 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 242, #cached-token: 2, token usage: 0.08, #running-req: 31, #queue-req: 149, 
[1,0]<stdout>:[2025-10-11 22:16:40 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 1, token usage: 0.07, #running-req: 31, #queue-req: 148, 
[1,1]<stdout>:[2025-10-11 22:16:40 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 19, #cached-token: 2, token usage: 0.09, #running-req: 31, #queue-req: 196, 
[1,0]<stdout>:[2025-10-11 22:16:40 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 20, #cached-token: 3, token usage: 0.07, #running-req: 31, #queue-req: 147, 
[1,1]<stdout>:[2025-10-11 22:16:40 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 37, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 195, 
[1,0]<stdout>:[2025-10-11 22:16:40 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 550, #cached-token: 8, token usage: 0.07, #running-req: 31, #queue-req: 146, 
[1,0]<stdout>:[2025-10-11 22:16:41 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 18, #cached-token: 2, token usage: 0.07, #running-req: 31, #queue-req: 145, 
[1,1]<stdout>:[2025-10-11 22:16:41 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 1480, #cached-token: 5, token usage: 0.09, #running-req: 31, #queue-req: 194, 
[1,0]<stdout>:[2025-10-11 22:16:41 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 389, #cached-token: 3, token usage: 0.07, #running-req: 31, #queue-req: 144, 
[1,0]<stdout>:[2025-10-11 22:16:42 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 441, #cached-token: 1, token usage: 0.07, #running-req: 31, #queue-req: 143, 
[1,0]<stdout>:[2025-10-11 22:16:42 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 291, #cached-token: 2, token usage: 0.07, #running-req: 31, #queue-req: 142, 
[1,0]<stdout>:[2025-10-11 22:16:42 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 84, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 141, 
[1,0]<stdout>:[2025-10-11 22:16:43 DP0 TP0] Prefill batch. #new-seq: 2, #new-token: 1390, #cached-token: 8, token usage: 0.07, #running-req: 30, #queue-req: 139, 
[1,0]<stdout>:[2025-10-11 22:16:43 DP0 TP0] Decode batch. #running-req: 32, #token: 13605, token usage: 0.08, accept len: 1.00, cuda graph: True, gen throughput (token/s): 324.17, #queue-req: 139, 
[1,1]<stdout>:[2025-10-11 22:16:43 DP1 TP8] Decode batch. #running-req: 31, #token: 17107, token usage: 0.10, accept len: 1.00, cuda graph: True, gen throughput (token/s): 325.96, #queue-req: 194, 
[1,1]<stdout>:[2025-10-11 22:16:43 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 21, #cached-token: 1, token usage: 0.10, #running-req: 31, #queue-req: 193, 
[1,0]<stdout>:[2025-10-11 22:16:43 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 244, #cached-token: 2, token usage: 0.08, #running-req: 31, #queue-req: 138, 
[1,0]<stdout>:[2025-10-11 22:16:43 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 47, #cached-token: 3, token usage: 0.08, #running-req: 31, #queue-req: 137, 
[1,0]<stdout>:[2025-10-11 22:16:44 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 2, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 136, 
[1,0]<stdout>:[2025-10-11 22:16:44 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 7, #cached-token: 3, token usage: 0.08, #running-req: 31, #queue-req: 135, 
[1,0]<stdout>:[2025-10-11 22:16:44 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 2048, #cached-token: 1, token usage: 0.07, #running-req: 31, #queue-req: 134, 
[1,0]<stdout>:[2025-10-11 22:16:44 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 1438, #cached-token: 0, token usage: 0.09, #running-req: 31, #queue-req: 134, 
[1,0]<stdout>:[2025-10-11 22:16:45 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 15, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 133, 
[1,0]<stdout>:[2025-10-11 22:16:45 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 34, #cached-token: 2, token usage: 0.09, #running-req: 31, #queue-req: 132, 
[1,0]<stdout>:[2025-10-11 22:16:45 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 8, #cached-token: 2, token usage: 0.09, #running-req: 31, #queue-req: 131, 
[1,1]<stdout>:[2025-10-11 22:16:46 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 281, #cached-token: 1, token usage: 0.11, #running-req: 31, #queue-req: 192, 
[1,0]<stdout>:[2025-10-11 22:16:46 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 41, #cached-token: 4, token usage: 0.09, #running-req: 31, #queue-req: 130, 
[1,0]<stdout>:[2025-10-11 22:16:46 DP0 TP0] Decode batch. #running-req: 32, #token: 15749, token usage: 0.09, accept len: 1.00, cuda graph: True, gen throughput (token/s): 350.11, #queue-req: 130, 
[1,1]<stdout>:[2025-10-11 22:16:46 DP1 TP8] Decode batch. #running-req: 32, #token: 18345, token usage: 0.11, accept len: 1.00, cuda graph: True, gen throughput (token/s): 352.28, #queue-req: 192, 
[1,1]<stdout>:[2025-10-11 22:16:47 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 223, #cached-token: 5, token usage: 0.09, #running-req: 31, #queue-req: 191, 
[1,1]<stdout>:[2025-10-11 22:16:47 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 794, #cached-token: 7, token usage: 0.09, #running-req: 31, #queue-req: 190, 
[1,1]<stdout>:[2025-10-11 22:16:47 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 16, #cached-token: 1, token usage: 0.10, #running-req: 31, #queue-req: 189, 
[1,0]<stdout>:[2025-10-11 22:16:47 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 318, #cached-token: 5, token usage: 0.09, #running-req: 31, #queue-req: 129, 
[1,0]<stdout>:[2025-10-11 22:16:47 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 42, #cached-token: 1, token usage: 0.10, #running-req: 31, #queue-req: 128, 
[1,1]<stdout>:[2025-10-11 22:16:48 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 652, #cached-token: 5, token usage: 0.09, #running-req: 31, #queue-req: 188, 
[1,0]<stdout>:[2025-10-11 22:16:48 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 108, #cached-token: 8, token usage: 0.09, #running-req: 31, #queue-req: 127, 
[1,1]<stdout>:[2025-10-11 22:16:48 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 184, #cached-token: 2, token usage: 0.09, #running-req: 31, #queue-req: 187, 
[1,0]<stdout>:[2025-10-11 22:16:48 DP0 TP0] Prefill batch. #new-seq: 2, #new-token: 501, #cached-token: 5, token usage: 0.09, #running-req: 30, #queue-req: 125, 
[1,0]<stdout>:[2025-10-11 22:16:49 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 591, #cached-token: 2, token usage: 0.08, #running-req: 31, #queue-req: 124, 
[1,0]<stdout>:[2025-10-11 22:16:49 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 258, #cached-token: 5, token usage: 0.08, #running-req: 31, #queue-req: 123, 
[1,0]<stdout>:[2025-10-11 22:16:49 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 33, #cached-token: 3, token usage: 0.08, #running-req: 31, #queue-req: 122, 
[1,0]<stdout>:[2025-10-11 22:16:49 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 517, #cached-token: 3, token usage: 0.08, #running-req: 31, #queue-req: 121, 
[1,0]<stdout>:[2025-10-11 22:16:50 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 272, #cached-token: 5, token usage: 0.08, #running-req: 31, #queue-req: 120, 
[1,1]<stdout>:[2025-10-11 22:16:50 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 666, #cached-token: 3, token usage: 0.09, #running-req: 31, #queue-req: 186, 
[1,0]<stdout>:[2025-10-11 22:16:50 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 12, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 119, 
[1,0]<stdout>:[2025-10-11 22:16:50 DP0 TP0] Decode batch. #running-req: 29, #token: 12994, token usage: 0.08, accept len: 1.00, cuda graph: True, gen throughput (token/s): 312.84, #queue-req: 119, 
[1,1]<stdout>:[2025-10-11 22:16:50 DP1 TP8] Decode batch. #running-req: 32, #token: 16843, token usage: 0.10, accept len: 1.00, cuda graph: True, gen throughput (token/s): 314.82, #queue-req: 186, 
[1,0]<stdout>:[2025-10-11 22:16:50 DP0 TP0] Prefill batch. #new-seq: 3, #new-token: 1340, #cached-token: 11, token usage: 0.08, #running-req: 29, #queue-req: 116, 
[1,0]<stdout>:[2025-10-11 22:16:51 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 57, #cached-token: 2, token usage: 0.08, #running-req: 31, #queue-req: 115, 
[1,0]<stdout>:[2025-10-11 22:16:52 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 7, #cached-token: 2, token usage: 0.08, #running-req: 31, #queue-req: 114, 
[1,1]<stdout>:[2025-10-11 22:16:52 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 41, #cached-token: 2, token usage: 0.10, #running-req: 31, #queue-req: 185, 
[1,0]<stdout>:[2025-10-11 22:16:52 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 26, #cached-token: 5, token usage: 0.08, #running-req: 31, #queue-req: 113, 
[1,0]<stdout>:[2025-10-11 22:16:53 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 657, #cached-token: 8, token usage: 0.08, #running-req: 31, #queue-req: 112, 
[1,0]<stdout>:[2025-10-11 22:16:53 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 458, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 111, 
[1,1]<stdout>:[2025-10-11 22:16:53 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 33, #cached-token: 1, token usage: 0.10, #running-req: 31, #queue-req: 184, 
[1,0]<stdout>:[2025-10-11 22:16:53 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 37, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 110, 
[1,1]<stdout>:[2025-10-11 22:16:53 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 51, #cached-token: 2, token usage: 0.10, #running-req: 31, #queue-req: 183, 
[1,0]<stdout>:[2025-10-11 22:16:54 DP0 TP0] Decode batch. #running-req: 31, #token: 13114, token usage: 0.08, accept len: 1.00, cuda graph: True, gen throughput (token/s): 395.92, #queue-req: 110, 
[1,1]<stdout>:[2025-10-11 22:16:54 DP1 TP8] Decode batch. #running-req: 31, #token: 17075, token usage: 0.10, accept len: 1.00, cuda graph: True, gen throughput (token/s): 396.86, #queue-req: 183, 
[1,1]<stdout>:[2025-10-11 22:16:54 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 8, #cached-token: 3, token usage: 0.10, #running-req: 31, #queue-req: 182, 
[1,0]<stdout>:[2025-10-11 22:16:54 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 44, #cached-token: 2, token usage: 0.08, #running-req: 31, #queue-req: 109, 
[1,0]<stdout>:[2025-10-11 22:16:54 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 274, #cached-token: 3, token usage: 0.06, #running-req: 31, #queue-req: 108, 
[1,0]<stdout>:[2025-10-11 22:16:55 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 18, #cached-token: 7, token usage: 0.06, #running-req: 31, #queue-req: 107, 
[1,0]<stdout>:[2025-10-11 22:16:55 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 360, #cached-token: 2, token usage: 0.05, #running-req: 31, #queue-req: 106, 
[1,1]<stdout>:[2025-10-11 22:16:55 DP1 TP8] Prefill batch. #new-seq: 2, #new-token: 631, #cached-token: 5, token usage: 0.10, #running-req: 30, #queue-req: 180, 
[1,1]<stdout>:[2025-10-11 22:16:55 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 104, #cached-token: 2, token usage: 0.10, #running-req: 31, #queue-req: 179, 
[1,0]<stdout>:[2025-10-11 22:16:56 DP0 TP0] Prefill batch. #new-seq: 2, #new-token: 502, #cached-token: 8, token usage: 0.06, #running-req: 30, #queue-req: 104, 
[1,0]<stdout>:[2025-10-11 22:16:56 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 208, #cached-token: 1, token usage: 0.06, #running-req: 31, #queue-req: 103, 
[1,0]<stdout>:[2025-10-11 22:16:56 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 41, #cached-token: 2, token usage: 0.06, #running-req: 31, #queue-req: 102, 
[1,0]<stdout>:[2025-10-11 22:16:57 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 461, #cached-token: 1, token usage: 0.06, #running-req: 31, #queue-req: 101, 
[1,1]<stdout>:[2025-10-11 22:16:57 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 340, #cached-token: 1, token usage: 0.10, #running-req: 31, #queue-req: 178, 
[1,1]<stdout>:[2025-10-11 22:16:57 DP1 TP8] Prefill batch. #new-seq: 2, #new-token: 19, #cached-token: 4, token usage: 0.10, #running-req: 30, #queue-req: 176, 
[1,1]<stdout>:[2025-10-11 22:16:57 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 38, #cached-token: 1, token usage: 0.10, #running-req: 31, #queue-req: 175, 
[1,0]<stdout>:[2025-10-11 22:16:57 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 17, #cached-token: 4, token usage: 0.06, #running-req: 31, #queue-req: 100, 
[1,1]<stdout>:[2025-10-11 22:16:57 DP1 TP8] Decode batch. #running-req: 32, #token: 17249, token usage: 0.10, accept len: 1.00, cuda graph: True, gen throughput (token/s): 336.08, #queue-req: 175, 
[1,0]<stdout>:[2025-10-11 22:16:57 DP0 TP0] Decode batch. #running-req: 32, #token: 10356, token usage: 0.06, accept len: 1.00, cuda graph: True, gen throughput (token/s): 335.56, #queue-req: 100, 
[1,1]<stdout>:[2025-10-11 22:16:58 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 26, #cached-token: 3, token usage: 0.10, #running-req: 31, #queue-req: 174, 
[1,1]<stdout>:[2025-10-11 22:16:58 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 284, #cached-token: 4, token usage: 0.10, #running-req: 31, #queue-req: 173, 
[1,0]<stdout>:[2025-10-11 22:16:58 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 12, #cached-token: 5, token usage: 0.06, #running-req: 31, #queue-req: 99, 
[1,1]<stdout>:[2025-10-11 22:16:59 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 3, #cached-token: 3, token usage: 0.10, #running-req: 31, #queue-req: 172, 
[1,1]<stdout>:[2025-10-11 22:16:59 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 763, #cached-token: 6, token usage: 0.10, #running-req: 31, #queue-req: 171, 
[1,1]<stdout>:[2025-10-11 22:16:59 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 369, #cached-token: 2, token usage: 0.10, #running-req: 31, #queue-req: 170, 
[1,0]<stdout>:[2025-10-11 22:17:00 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 280, #cached-token: 1, token usage: 0.06, #running-req: 31, #queue-req: 98, 
[1,1]<stdout>:[2025-10-11 22:17:00 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 290, #cached-token: 3, token usage: 0.10, #running-req: 31, #queue-req: 169, 
[1,1]<stdout>:[2025-10-11 22:17:00 DP1 TP8] Prefill batch. #new-seq: 2, #new-token: 26, #cached-token: 3, token usage: 0.10, #running-req: 30, #queue-req: 167, 
[1,0]<stdout>:[2025-10-11 22:17:01 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 9, #cached-token: 3, token usage: 0.06, #running-req: 31, #queue-req: 97, 
[1,1]<stdout>:[2025-10-11 22:17:01 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 40, #cached-token: 3, token usage: 0.10, #running-req: 31, #queue-req: 166, 
[1,1]<stdout>:[2025-10-11 22:17:01 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 63, #cached-token: 4, token usage: 0.10, #running-req: 31, #queue-req: 165, 
[1,0]<stdout>:[2025-10-11 22:17:01 DP0 TP0] Decode batch. #running-req: 32, #token: 10907, token usage: 0.06, accept len: 1.00, cuda graph: True, gen throughput (token/s): 353.38, #queue-req: 97, 
[1,1]<stdout>:[2025-10-11 22:17:01 DP1 TP8] Decode batch. #running-req: 32, #token: 16755, token usage: 0.10, accept len: 1.00, cuda graph: True, gen throughput (token/s): 351.45, #queue-req: 165, 
[1,0]<stdout>:[2025-10-11 22:17:01 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 338, #cached-token: 3, token usage: 0.06, #running-req: 31, #queue-req: 96, 
[1,0]<stdout>:[2025-10-11 22:17:02 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 363, #cached-token: 2, token usage: 0.06, #running-req: 31, #queue-req: 95, 
[1,0]<stdout>:[2025-10-11 22:17:02 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 40, #cached-token: 2, token usage: 0.06, #running-req: 31, #queue-req: 94, 
[1,0]<stdout>:[2025-10-11 22:17:02 DP0 TP0] Prefill batch. #new-seq: 2, #new-token: 409, #cached-token: 4, token usage: 0.06, #running-req: 30, #queue-req: 92, 
[1,1]<stdout>:[2025-10-11 22:17:03 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 412, #cached-token: 3, token usage: 0.09, #running-req: 31, #queue-req: 164, 
[1,1]<stdout>:[2025-10-11 22:17:03 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 229, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 163, 
[1,0]<stdout>:[2025-10-11 22:17:03 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 243, #cached-token: 4, token usage: 0.06, #running-req: 31, #queue-req: 91, 
[1,0]<stdout>:[2025-10-11 22:17:04 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 10, #cached-token: 2, token usage: 0.06, #running-req: 31, #queue-req: 90, 
[1,1]<stdout>:[2025-10-11 22:17:04 DP1 TP8] Decode batch. #running-req: 32, #token: 16381, token usage: 0.10, accept len: 1.00, cuda graph: True, gen throughput (token/s): 421.33, #queue-req: 163, 
[1,0]<stdout>:[2025-10-11 22:17:04 DP0 TP0] Decode batch. #running-req: 32, #token: 11052, token usage: 0.06, accept len: 1.00, cuda graph: True, gen throughput (token/s): 419.64, #queue-req: 90, 
[1,0]<stdout>:[2025-10-11 22:17:04 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 292, #cached-token: 5, token usage: 0.07, #running-req: 31, #queue-req: 89, 
[1,1]<stdout>:[2025-10-11 22:17:05 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 17, #cached-token: 1, token usage: 0.10, #running-req: 31, #queue-req: 162, 
[1,0]<stdout>:[2025-10-11 22:17:05 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 34, #cached-token: 2, token usage: 0.07, #running-req: 31, #queue-req: 88, 
[1,0]<stdout>:[2025-10-11 22:17:06 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 232, #cached-token: 1, token usage: 0.07, #running-req: 31, #queue-req: 87, 
[1,1]<stdout>:[2025-10-11 22:17:06 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 74, #cached-token: 3, token usage: 0.10, #running-req: 31, #queue-req: 161, 
[1,0]<stdout>:[2025-10-11 22:17:06 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 866, #cached-token: 3, token usage: 0.07, #running-req: 31, #queue-req: 86, 
[1,0]<stdout>:[2025-10-11 22:17:06 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 49, #cached-token: 3, token usage: 0.07, #running-req: 31, #queue-req: 85, 
[1,0]<stdout>:[2025-10-11 22:17:07 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 507, #cached-token: 3, token usage: 0.07, #running-req: 31, #queue-req: 84, 
[1,0]<stdout>:[2025-10-11 22:17:07 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 330, #cached-token: 6, token usage: 0.07, #running-req: 31, #queue-req: 83, 
[1,0]<stdout>:[2025-10-11 22:17:07 DP0 TP0] Decode batch. #running-req: 32, #token: 12285, token usage: 0.07, accept len: 1.00, cuda graph: True, gen throughput (token/s): 400.46, #queue-req: 83, 
[1,1]<stdout>:[2025-10-11 22:17:07 DP1 TP8] Decode batch. #running-req: 32, #token: 17155, token usage: 0.10, accept len: 1.00, cuda graph: True, gen throughput (token/s): 402.00, #queue-req: 161, 
[1,1]<stdout>:[2025-10-11 22:17:07 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 173, #cached-token: 1, token usage: 0.10, #running-req: 31, #queue-req: 160, 
[1,0]<stdout>:[2025-10-11 22:17:08 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 28, #cached-token: 3, token usage: 0.07, #running-req: 31, #queue-req: 82, 
[1,0]<stdout>:[2025-10-11 22:17:08 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 278, #cached-token: 1, token usage: 0.07, #running-req: 31, #queue-req: 81, 
[1,1]<stdout>:[2025-10-11 22:17:09 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 438, #cached-token: 3, token usage: 0.10, #running-req: 31, #queue-req: 159, 
[1,1]<stdout>:[2025-10-11 22:17:09 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 20, #cached-token: 2, token usage: 0.10, #running-req: 31, #queue-req: 158, 
[1,0]<stdout>:[2025-10-11 22:17:09 DP0 TP0] Prefill batch. #new-seq: 2, #new-token: 20, #cached-token: 3, token usage: 0.07, #running-req: 30, #queue-req: 79, 
[1,0]<stdout>:[2025-10-11 22:17:10 DP0 TP0] Decode batch. #running-req: 31, #token: 12150, token usage: 0.07, accept len: 1.00, cuda graph: True, gen throughput (token/s): 462.13, #queue-req: 79, 
[1,1]<stdout>:[2025-10-11 22:17:10 DP1 TP8] Decode batch. #running-req: 32, #token: 16965, token usage: 0.10, accept len: 1.00, cuda graph: True, gen throughput (token/s): 462.81, #queue-req: 158, 
[1,0]<stdout>:[2025-10-11 22:17:10 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 30, #cached-token: 2, token usage: 0.07, #running-req: 31, #queue-req: 78, 
[1,1]<stdout>:[2025-10-11 22:17:10 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 576, #cached-token: 3, token usage: 0.10, #running-req: 31, #queue-req: 157, 
[1,0]<stdout>:[2025-10-11 22:17:11 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 41, #cached-token: 5, token usage: 0.07, #running-req: 31, #queue-req: 77, 
[1,0]<stdout>:[2025-10-11 22:17:11 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 1233, #cached-token: 4, token usage: 0.07, #running-req: 31, #queue-req: 76, 
[1,0]<stdout>:[2025-10-11 22:17:11 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 2048, #cached-token: 1, token usage: 0.07, #running-req: 31, #queue-req: 75, 
[1,0]<stdout>:[2025-10-11 22:17:12 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 688, #cached-token: 0, token usage: 0.09, #running-req: 31, #queue-req: 75, 
[1,0]<stdout>:[2025-10-11 22:17:12 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 518, #cached-token: 5, token usage: 0.09, #running-req: 31, #queue-req: 74, 
[1,1]<stdout>:[2025-10-11 22:17:12 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 19, #cached-token: 4, token usage: 0.10, #running-req: 31, #queue-req: 156, 
[1,0]<stdout>:[2025-10-11 22:17:12 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 159, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 73, 
[1,0]<stdout>:[2025-10-11 22:17:13 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 258, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 72, 
[1,0]<stdout>:[2025-10-11 22:17:13 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 51, #cached-token: 3, token usage: 0.09, #running-req: 31, #queue-req: 71, 
[1,0]<stdout>:[2025-10-11 22:17:13 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 687, #cached-token: 3, token usage: 0.09, #running-req: 31, #queue-req: 70, 
[1,0]<stdout>:[2025-10-11 22:17:13 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 7, #cached-token: 3, token usage: 0.09, #running-req: 31, #queue-req: 69, 
[1,1]<stdout>:[2025-10-11 22:17:14 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 15, #cached-token: 5, token usage: 0.10, #running-req: 31, #queue-req: 155, 
[1,0]<stdout>:[2025-10-11 22:17:14 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 638, #cached-token: 2, token usage: 0.09, #running-req: 31, #queue-req: 68, 
[1,1]<stdout>:[2025-10-11 22:17:14 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 349, #cached-token: 6, token usage: 0.10, #running-req: 31, #queue-req: 154, 
[1,0]<stdout>:[2025-10-11 22:17:14 DP0 TP0] Decode batch. #running-req: 31, #token: 16142, token usage: 0.09, accept len: 1.00, cuda graph: True, gen throughput (token/s): 302.52, #queue-req: 68, 
[1,1]<stdout>:[2025-10-11 22:17:14 DP1 TP8] Decode batch. #running-req: 31, #token: 17333, token usage: 0.10, accept len: 1.00, cuda graph: True, gen throughput (token/s): 303.97, #queue-req: 154, 
[1,1]<stdout>:[2025-10-11 22:17:14 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 8, #cached-token: 2, token usage: 0.10, #running-req: 31, #queue-req: 153, 
[1,0]<stdout>:[2025-10-11 22:17:14 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 17, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 67, 
[1,0]<stdout>:[2025-10-11 22:17:15 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 602, #cached-token: 1, token usage: 0.10, #running-req: 31, #queue-req: 66, 
[1,0]<stdout>:[2025-10-11 22:17:15 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 12, #cached-token: 2, token usage: 0.09, #running-req: 31, #queue-req: 65, 
[1,0]<stdout>:[2025-10-11 22:17:15 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 725, #cached-token: 4, token usage: 0.09, #running-req: 31, #queue-req: 64, 
[1,1]<stdout>:[2025-10-11 22:17:15 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 739, #cached-token: 6, token usage: 0.09, #running-req: 31, #queue-req: 152, 
[1,1]<stdout>:[2025-10-11 22:17:16 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 216, #cached-token: 2, token usage: 0.09, #running-req: 31, #queue-req: 151, 
[1,0]<stdout>:[2025-10-11 22:17:16 DP0 TP0] Prefill batch. #new-seq: 2, #new-token: 409, #cached-token: 7, token usage: 0.09, #running-req: 30, #queue-req: 62, 
[1,1]<stdout>:[2025-10-11 22:17:16 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 20, #cached-token: 3, token usage: 0.09, #running-req: 31, #queue-req: 150, 
[1,1]<stdout>:[2025-10-11 22:17:16 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 502, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 149, 
[1,0]<stdout>:[2025-10-11 22:17:16 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 183, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 61, 
[1,0]<stdout>:[2025-10-11 22:17:17 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 1227, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 60, 
[1,1]<stdout>:[2025-10-11 22:17:17 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 785, #cached-token: 3, token usage: 0.09, #running-req: 31, #queue-req: 148, 
[1,0]<stdout>:[2025-10-11 22:17:17 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 7, #cached-token: 5, token usage: 0.09, #running-req: 31, #queue-req: 59, 
[1,1]<stdout>:[2025-10-11 22:17:17 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 518, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 147, 
[1,1]<stdout>:[2025-10-11 22:17:18 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 35, #cached-token: 2, token usage: 0.09, #running-req: 31, #queue-req: 146, 
[1,0]<stdout>:[2025-10-11 22:17:18 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 37, #cached-token: 3, token usage: 0.09, #running-req: 31, #queue-req: 58, 
[1,1]<stdout>:[2025-10-11 22:17:18 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 17, #cached-token: 2, token usage: 0.09, #running-req: 31, #queue-req: 145, 
[1,0]<stdout>:[2025-10-11 22:17:19 DP0 TP0] Decode batch. #running-req: 32, #token: 15449, token usage: 0.09, accept len: 1.00, cuda graph: True, gen throughput (token/s): 293.01, #queue-req: 58, 
[1,1]<stdout>:[2025-10-11 22:17:19 DP1 TP8] Decode batch. #running-req: 31, #token: 14844, token usage: 0.09, accept len: 1.00, cuda graph: True, gen throughput (token/s): 293.00, #queue-req: 145, 
[1,1]<stdout>:[2025-10-11 22:17:19 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 6, #cached-token: 3, token usage: 0.09, #running-req: 31, #queue-req: 144, 
[1,1]<stdout>:[2025-10-11 22:17:19 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 39, #cached-token: 3, token usage: 0.08, #running-req: 31, #queue-req: 143, 
[1,0]<stdout>:[2025-10-11 22:17:19 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 414, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 57, 
[1,1]<stdout>:[2025-10-11 22:17:19 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 12, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 142, 
[1,0]<stdout>:[2025-10-11 22:17:19 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 14, #cached-token: 2, token usage: 0.09, #running-req: 31, #queue-req: 56, 
[1,0]<stdout>:[2025-10-11 22:17:20 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 11, #cached-token: 3, token usage: 0.09, #running-req: 31, #queue-req: 55, 
[1,1]<stdout>:[2025-10-11 22:17:20 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 341, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 141, 
[1,1]<stdout>:[2025-10-11 22:17:20 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 2048, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 140, 
[1,1]<stdout>:[2025-10-11 22:17:20 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 752, #cached-token: 0, token usage: 0.09, #running-req: 31, #queue-req: 140, 
[1,0]<stdout>:[2025-10-11 22:17:21 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 434, #cached-token: 2, token usage: 0.08, #running-req: 31, #queue-req: 54, 
[1,0]<stdout>:[2025-10-11 22:17:21 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 18, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 53, 
[1,0]<stdout>:[2025-10-11 22:17:21 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 9, #cached-token: 3, token usage: 0.08, #running-req: 31, #queue-req: 52, 
[1,0]<stdout>:[2025-10-11 22:17:22 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 36, #cached-token: 3, token usage: 0.08, #running-req: 31, #queue-req: 51, 
[1,1]<stdout>:[2025-10-11 22:17:22 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 595, #cached-token: 5, token usage: 0.10, #running-req: 31, #queue-req: 139, 
[1,0]<stdout>:[2025-10-11 22:17:22 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 312, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 50, 
[1,1]<stdout>:[2025-10-11 22:17:22 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 795, #cached-token: 1, token usage: 0.10, #running-req: 31, #queue-req: 138, 
[1,1]<stdout>:[2025-10-11 22:17:23 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 27, #cached-token: 2, token usage: 0.10, #running-req: 31, #queue-req: 137, 
[1,0]<stdout>:[2025-10-11 22:17:23 DP0 TP0] Decode batch. #running-req: 32, #token: 14099, token usage: 0.08, accept len: 1.00, cuda graph: True, gen throughput (token/s): 305.84, #queue-req: 50, 
[1,1]<stdout>:[2025-10-11 22:17:23 DP1 TP8] Decode batch. #running-req: 32, #token: 17691, token usage: 0.10, accept len: 1.00, cuda graph: True, gen throughput (token/s): 306.08, #queue-req: 137, 
[1,1]<stdout>:[2025-10-11 22:17:23 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 766, #cached-token: 1, token usage: 0.10, #running-req: 31, #queue-req: 136, 
[1,1]<stdout>:[2025-10-11 22:17:23 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 5, #cached-token: 2, token usage: 0.10, #running-req: 31, #queue-req: 135, 
[1,1]<stdout>:[2025-10-11 22:17:24 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 22, #cached-token: 1, token usage: 0.10, #running-req: 31, #queue-req: 134, 
[1,0]<stdout>:[2025-10-11 22:17:24 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 326, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 49, 
[1,0]<stdout>:[2025-10-11 22:17:24 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 8, #cached-token: 2, token usage: 0.08, #running-req: 31, #queue-req: 48, 
[1,1]<stdout>:[2025-10-11 22:17:24 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 64, #cached-token: 2, token usage: 0.10, #running-req: 31, #queue-req: 133, 
[1,1]<stdout>:[2025-10-11 22:17:24 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 9, #cached-token: 1, token usage: 0.10, #running-req: 31, #queue-req: 132, 
[1,0]<stdout>:[2025-10-11 22:17:25 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 18, #cached-token: 2, token usage: 0.09, #running-req: 31, #queue-req: 47, 
[1,0]<stdout>:[2025-10-11 22:17:26 DP0 TP0] Decode batch. #running-req: 32, #token: 15075, token usage: 0.09, accept len: 1.00, cuda graph: True, gen throughput (token/s): 440.59, #queue-req: 47, 
[1,1]<stdout>:[2025-10-11 22:17:26 DP1 TP8] Decode batch. #running-req: 32, #token: 16929, token usage: 0.10, accept len: 1.00, cuda graph: True, gen throughput (token/s): 439.84, #queue-req: 132, 
[1,1]<stdout>:[2025-10-11 22:17:26 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 29, #cached-token: 3, token usage: 0.10, #running-req: 31, #queue-req: 131, 
[1,0]<stdout>:[2025-10-11 22:17:26 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 364, #cached-token: 6, token usage: 0.09, #running-req: 31, #queue-req: 46, 
[1,0]<stdout>:[2025-10-11 22:17:26 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 6, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 45, 
[1,0]<stdout>:[2025-10-11 22:17:27 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 164, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 44, 
[1,1]<stdout>:[2025-10-11 22:17:27 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 145, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 130, 
[1,1]<stdout>:[2025-10-11 22:17:27 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 346, #cached-token: 2, token usage: 0.08, #running-req: 31, #queue-req: 129, 
[1,1]<stdout>:[2025-10-11 22:17:28 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 2036, #cached-token: 4, token usage: 0.08, #running-req: 31, #queue-req: 128, 
[1,1]<stdout>:[2025-10-11 22:17:28 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 2048, #cached-token: 1, token usage: 0.10, #running-req: 31, #queue-req: 127, 
[1,1]<stdout>:[2025-10-11 22:17:28 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 746, #cached-token: 0, token usage: 0.11, #running-req: 31, #queue-req: 127, 
[1,1]<stdout>:[2025-10-11 22:17:29 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 89, #cached-token: 2, token usage: 0.11, #running-req: 31, #queue-req: 126, 
[1,1]<stdout>:[2025-10-11 22:17:29 DP1 TP8] Decode batch. #running-req: 32, #token: 19378, token usage: 0.11, accept len: 1.00, cuda graph: True, gen throughput (token/s): 400.70, #queue-req: 126, 
[1,0]<stdout>:[2025-10-11 22:17:29 DP0 TP0] Decode batch. #running-req: 31, #token: 15402, token usage: 0.09, accept len: 1.00, cuda graph: True, gen throughput (token/s): 401.23, #queue-req: 44, 
[1,0]<stdout>:[2025-10-11 22:17:29 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 509, #cached-token: 4, token usage: 0.09, #running-req: 31, #queue-req: 43, 
[1,0]<stdout>:[2025-10-11 22:17:29 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 30, #cached-token: 2, token usage: 0.09, #running-req: 31, #queue-req: 42, 
[1,0]<stdout>:[2025-10-11 22:17:29 DP0 TP0] Prefill batch. #new-seq: 2, #new-token: 460, #cached-token: 4, token usage: 0.09, #running-req: 30, #queue-req: 40, 
[1,0]<stdout>:[2025-10-11 22:17:30 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 34, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 39, 
[1,1]<stdout>:[2025-10-11 22:17:30 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 6, #cached-token: 4, token usage: 0.12, #running-req: 31, #queue-req: 125, 
[1,0]<stdout>:[2025-10-11 22:17:30 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 7, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 38, 
[1,1]<stdout>:[2025-10-11 22:17:30 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 533, #cached-token: 5, token usage: 0.11, #running-req: 31, #queue-req: 124, 
[1,0]<stdout>:[2025-10-11 22:17:31 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 290, #cached-token: 2, token usage: 0.08, #running-req: 31, #queue-req: 37, 
[1,0]<stdout>:[2025-10-11 22:17:31 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 143, #cached-token: 2, token usage: 0.09, #running-req: 31, #queue-req: 36, 
[1,0]<stdout>:[2025-10-11 22:17:31 DP0 TP0] Prefill batch. #new-seq: 2, #new-token: 69, #cached-token: 4, token usage: 0.08, #running-req: 30, #queue-req: 34, 
[1,1]<stdout>:[2025-10-11 22:17:31 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 139, #cached-token: 2, token usage: 0.10, #running-req: 31, #queue-req: 123, 
[1,0]<stdout>:[2025-10-11 22:17:32 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 168, #cached-token: 2, token usage: 0.08, #running-req: 31, #queue-req: 33, 
[1,1]<stdout>:[2025-10-11 22:17:32 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 56, #cached-token: 3, token usage: 0.10, #running-req: 31, #queue-req: 122, 
[1,0]<stdout>:[2025-10-11 22:17:32 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 13, #cached-token: 5, token usage: 0.08, #running-req: 31, #queue-req: 32, 
[1,0]<stdout>:[2025-10-11 22:17:33 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 483, #cached-token: 6, token usage: 0.08, #running-req: 31, #queue-req: 31, 
[1,0]<stdout>:[2025-10-11 22:17:33 DP0 TP0] Decode batch. #running-req: 32, #token: 14376, token usage: 0.08, accept len: 1.00, cuda graph: True, gen throughput (token/s): 314.94, #queue-req: 31, 
[1,1]<stdout>:[2025-10-11 22:17:33 DP1 TP8] Decode batch. #running-req: 31, #token: 16694, token usage: 0.10, accept len: 1.00, cuda graph: True, gen throughput (token/s): 316.63, #queue-req: 122, 
[1,1]<stdout>:[2025-10-11 22:17:33 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 14, #cached-token: 3, token usage: 0.10, #running-req: 31, #queue-req: 121, 
[1,0]<stdout>:[2025-10-11 22:17:33 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 179, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 30, 
[1,1]<stdout>:[2025-10-11 22:17:33 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 11, #cached-token: 3, token usage: 0.09, #running-req: 31, #queue-req: 120, 
[1,1]<stdout>:[2025-10-11 22:17:33 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 4, #cached-token: 2, token usage: 0.09, #running-req: 31, #queue-req: 119, 
[1,0]<stdout>:[2025-10-11 22:17:34 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 823, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 29, 
[1,1]<stdout>:[2025-10-11 22:17:34 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 168, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 118, 
[1,0]<stdout>:[2025-10-11 22:17:34 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 2048, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 28, 
[1,0]<stdout>:[2025-10-11 22:17:35 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 139, #cached-token: 0, token usage: 0.10, #running-req: 31, #queue-req: 28, 
[1,0]<stdout>:[2025-10-11 22:17:35 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 8, #cached-token: 2, token usage: 0.10, #running-req: 31, #queue-req: 27, 
[1,1]<stdout>:[2025-10-11 22:17:35 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 413, #cached-token: 6, token usage: 0.09, #running-req: 31, #queue-req: 117, 
[1,0]<stdout>:[2025-10-11 22:17:35 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 285, #cached-token: 1, token usage: 0.10, #running-req: 31, #queue-req: 26, 
[1,1]<stdout>:[2025-10-11 22:17:36 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 116, 
[1,1]<stdout>:[2025-10-11 22:17:36 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 344, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 115, 
[1,0]<stdout>:[2025-10-11 22:17:36 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 61, #cached-token: 1, token usage: 0.10, #running-req: 31, #queue-req: 25, 
[1,0]<stdout>:[2025-10-11 22:17:36 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 12, #cached-token: 3, token usage: 0.10, #running-req: 31, #queue-req: 24, 
[1,1]<stdout>:[2025-10-11 22:17:37 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 627, #cached-token: 3, token usage: 0.10, #running-req: 31, #queue-req: 114, 
[1,0]<stdout>:[2025-10-11 22:17:37 DP0 TP0] Decode batch. #running-req: 32, #token: 16604, token usage: 0.10, accept len: 1.00, cuda graph: True, gen throughput (token/s): 307.58, #queue-req: 24, 
[1,1]<stdout>:[2025-10-11 22:17:37 DP1 TP8] Decode batch. #running-req: 32, #token: 17232, token usage: 0.10, accept len: 1.00, cuda graph: True, gen throughput (token/s): 307.59, #queue-req: 114, 
[1,1]<stdout>:[2025-10-11 22:17:37 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 45, #cached-token: 2, token usage: 0.10, #running-req: 31, #queue-req: 113, 
[1,1]<stdout>:[2025-10-11 22:17:37 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 131, #cached-token: 4, token usage: 0.10, #running-req: 31, #queue-req: 112, 
[1,1]<stdout>:[2025-10-11 22:17:38 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 707, #cached-token: 3, token usage: 0.10, #running-req: 31, #queue-req: 111, 
[1,0]<stdout>:[2025-10-11 22:17:38 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 279, #cached-token: 2, token usage: 0.10, #running-req: 31, #queue-req: 23, 
[1,1]<stdout>:[2025-10-11 22:17:38 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 2048, #cached-token: 1, token usage: 0.10, #running-req: 31, #queue-req: 110, 
[1,1]<stdout>:[2025-10-11 22:17:38 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 454, #cached-token: 0, token usage: 0.11, #running-req: 31, #queue-req: 110, 
[1,1]<stdout>:[2025-10-11 22:17:38 DP1 TP8] Prefill batch. #new-seq: 2, #new-token: 672, #cached-token: 5, token usage: 0.11, #running-req: 30, #queue-req: 108, 
[1,0]<stdout>:[2025-10-11 22:17:39 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 108, #cached-token: 3, token usage: 0.10, #running-req: 31, #queue-req: 22, 
[1,0]<stdout>:[2025-10-11 22:17:39 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 12, #cached-token: 1, token usage: 0.10, #running-req: 31, #queue-req: 21, 
[1,0]<stdout>:[2025-10-11 22:17:40 DP0 TP0] Prefill batch. #new-seq: 2, #new-token: 557, #cached-token: 7, token usage: 0.10, #running-req: 30, #queue-req: 19, 
[1,1]<stdout>:[2025-10-11 22:17:40 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 21, #cached-token: 2, token usage: 0.12, #running-req: 31, #queue-req: 107, 
[1,0]<stdout>:[2025-10-11 22:17:40 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 770, #cached-token: 5, token usage: 0.10, #running-req: 31, #queue-req: 18, 
[1,1]<stdout>:[2025-10-11 22:17:40 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 817, #cached-token: 5, token usage: 0.11, #running-req: 31, #queue-req: 106, 
[1,1]<stdout>:[2025-10-11 22:17:41 DP1 TP8] Prefill batch. #new-seq: 2, #new-token: 602, #cached-token: 7, token usage: 0.10, #running-req: 30, #queue-req: 104, 
[1,0]<stdout>:[2025-10-11 22:17:41 DP0 TP0] Decode batch. #running-req: 31, #token: 17927, token usage: 0.11, accept len: 1.00, cuda graph: True, gen throughput (token/s): 335.43, #queue-req: 18, 
[1,1]<stdout>:[2025-10-11 22:17:41 DP1 TP8] Decode batch. #running-req: 32, #token: 17949, token usage: 0.11, accept len: 1.00, cuda graph: True, gen throughput (token/s): 334.65, #queue-req: 104, 
[1,0]<stdout>:[2025-10-11 22:17:41 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 25, #cached-token: 1, token usage: 0.11, #running-req: 31, #queue-req: 17, 
[1,0]<stdout>:[2025-10-11 22:17:41 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 32, #cached-token: 2, token usage: 0.10, #running-req: 31, #queue-req: 16, 
[1,1]<stdout>:[2025-10-11 22:17:41 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 8, #cached-token: 1, token usage: 0.10, #running-req: 31, #queue-req: 103, 
[1,0]<stdout>:[2025-10-11 22:17:42 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 426, #cached-token: 2, token usage: 0.10, #running-req: 31, #queue-req: 15, 
[1,1]<stdout>:[2025-10-11 22:17:42 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 792, #cached-token: 5, token usage: 0.10, #running-req: 31, #queue-req: 102, 
[1,0]<stdout>:[2025-10-11 22:17:42 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 9, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 14, 
[1,1]<stdout>:[2025-10-11 22:17:42 DP1 TP8] Prefill batch. #new-seq: 2, #new-token: 108, #cached-token: 5, token usage: 0.10, #running-req: 30, #queue-req: 100, 
[1,0]<stdout>:[2025-10-11 22:17:43 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 11, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 13, 
[1,0]<stdout>:[2025-10-11 22:17:43 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 4, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 12, 
[1,0]<stdout>:[2025-10-11 22:17:43 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 239, #cached-token: 1, token usage: 0.08, #running-req: 31, #queue-req: 11, 
[1,1]<stdout>:[2025-10-11 22:17:43 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 80, #cached-token: 8, token usage: 0.10, #running-req: 31, #queue-req: 99, 
[1,0]<stdout>:[2025-10-11 22:17:43 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 125, #cached-token: 3, token usage: 0.08, #running-req: 31, #queue-req: 10, 
[1,1]<stdout>:[2025-10-11 22:17:43 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 125, #cached-token: 2, token usage: 0.10, #running-req: 31, #queue-req: 98, 
[1,0]<stdout>:[2025-10-11 22:17:44 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 16, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 9, 
[1,0]<stdout>:[2025-10-11 22:17:44 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 33, #cached-token: 3, token usage: 0.09, #running-req: 31, #queue-req: 8, 
[1,0]<stdout>:[2025-10-11 22:17:45 DP0 TP0] Decode batch. #running-req: 32, #token: 14693, token usage: 0.09, accept len: 1.00, cuda graph: True, gen throughput (token/s): 325.61, #queue-req: 8, 
[1,1]<stdout>:[2025-10-11 22:17:45 DP1 TP8] Decode batch. #running-req: 32, #token: 17465, token usage: 0.10, accept len: 1.00, cuda graph: True, gen throughput (token/s): 326.38, #queue-req: 98, 
[1,0]<stdout>:[2025-10-11 22:17:45 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 282, #cached-token: 6, token usage: 0.08, #running-req: 31, #queue-req: 7, 
[1,0]<stdout>:[2025-10-11 22:17:45 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 38, #cached-token: 3, token usage: 0.09, #running-req: 31, #queue-req: 6, 
[1,1]<stdout>:[2025-10-11 22:17:45 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 785, #cached-token: 4, token usage: 0.10, #running-req: 31, #queue-req: 97, 
[1,0]<stdout>:[2025-10-11 22:17:45 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 830, #cached-token: 4, token usage: 0.08, #running-req: 31, #queue-req: 5, 
[1,1]<stdout>:[2025-10-11 22:17:46 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 612, #cached-token: 6, token usage: 0.10, #running-req: 31, #queue-req: 96, 
[1,0]<stdout>:[2025-10-11 22:17:46 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 215, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 4, 
[1,0]<stdout>:[2025-10-11 22:17:46 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 121, #cached-token: 3, token usage: 0.09, #running-req: 31, #queue-req: 3, 
[1,1]<stdout>:[2025-10-11 22:17:46 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 435, #cached-token: 2, token usage: 0.10, #running-req: 31, #queue-req: 95, 
[1,1]<stdout>:[2025-10-11 22:17:47 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 10, #cached-token: 1, token usage: 0.10, #running-req: 31, #queue-req: 94, 
[1,1]<stdout>:[2025-10-11 22:17:47 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 529, #cached-token: 6, token usage: 0.10, #running-req: 31, #queue-req: 93, 
[1,1]<stdout>:[2025-10-11 22:17:48 DP1 TP8] Prefill batch. #new-seq: 2, #new-token: 627, #cached-token: 6, token usage: 0.10, #running-req: 30, #queue-req: 91, 
[1,1]<stdout>:[2025-10-11 22:17:48 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 620, #cached-token: 1, token usage: 0.10, #running-req: 31, #queue-req: 90, 
[1,1]<stdout>:[2025-10-11 22:17:48 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 204, #cached-token: 2, token usage: 0.10, #running-req: 31, #queue-req: 89, 
[1,1]<stdout>:[2025-10-11 22:17:48 DP1 TP8] Decode batch. #running-req: 32, #token: 17916, token usage: 0.11, accept len: 1.00, cuda graph: True, gen throughput (token/s): 351.14, #queue-req: 89, 
[1,0]<stdout>:[2025-10-11 22:17:48 DP0 TP0] Decode batch. #running-req: 32, #token: 16041, token usage: 0.09, accept len: 1.00, cuda graph: True, gen throughput (token/s): 352.22, #queue-req: 3, 
[1,0]<stdout>:[2025-10-11 22:17:48 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 345, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 2, 
[1,1]<stdout>:[2025-10-11 22:17:49 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 10, #cached-token: 5, token usage: 0.10, #running-req: 31, #queue-req: 88, 
[1,0]<stdout>:[2025-10-11 22:17:49 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 181, #cached-token: 3, token usage: 0.09, #running-req: 31, #queue-req: 1, 
[1,1]<stdout>:[2025-10-11 22:17:49 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 637, #cached-token: 4, token usage: 0.10, #running-req: 31, #queue-req: 87, 
[1,1]<stdout>:[2025-10-11 22:17:49 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 540, #cached-token: 1, token usage: 0.10, #running-req: 31, #queue-req: 86, 
[1,1]<stdout>:[2025-10-11 22:17:49 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 294, #cached-token: 3, token usage: 0.10, #running-req: 31, #queue-req: 85, 
[1,0]<stdout>:[2025-10-11 22:17:49 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 3, #cached-token: 3, token usage: 0.09, #running-req: 31, #queue-req: 0, 
[1,1]<stdout>:[2025-10-11 22:17:50 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 10, #cached-token: 4, token usage: 0.10, #running-req: 31, #queue-req: 84, 
[1,1]<stdout>:[2025-10-11 22:17:50 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 194, #cached-token: 1, token usage: 0.10, #running-req: 31, #queue-req: 83, 
[1,1]<stdout>:[2025-10-11 22:17:50 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 12, #cached-token: 2, token usage: 0.10, #running-req: 31, #queue-req: 82, 
[1,1]<stdout>:[2025-10-11 22:17:51 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 331, #cached-token: 4, token usage: 0.10, #running-req: 31, #queue-req: 81, 
[1,1]<stdout>:[2025-10-11 22:17:51 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 1, token usage: 0.11, #running-req: 31, #queue-req: 80, 
[1,1]<stdout>:[2025-10-11 22:17:51 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 381, #cached-token: 2, token usage: 0.10, #running-req: 31, #queue-req: 79, 
[1,1]<stdout>:[2025-10-11 22:17:51 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 770, #cached-token: 5, token usage: 0.08, #running-req: 31, #queue-req: 78, 
[1,1]<stdout>:[2025-10-11 22:17:52 DP1 TP8] Prefill batch. #new-seq: 2, #new-token: 737, #cached-token: 7, token usage: 0.09, #running-req: 30, #queue-req: 76, 
[1,1]<stdout>:[2025-10-11 22:17:52 DP1 TP8] Prefill batch. #new-seq: 3, #new-token: 928, #cached-token: 6, token usage: 0.09, #running-req: 29, #queue-req: 73, 
[1,1]<stdout>:[2025-10-11 22:17:52 DP1 TP8] Prefill batch. #new-seq: 2, #new-token: 1030, #cached-token: 2, token usage: 0.09, #running-req: 30, #queue-req: 71, 
[1,1]<stdout>:[2025-10-11 22:17:52 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 573, #cached-token: 7, token usage: 0.09, #running-req: 31, #queue-req: 70, 
[1,1]<stdout>:[2025-10-11 22:17:53 DP1 TP8] Decode batch. #running-req: 32, #token: 16530, token usage: 0.10, accept len: 1.00, cuda graph: True, gen throughput (token/s): 292.48, #queue-req: 70, 
[1,0]<stdout>:[2025-10-11 22:17:53 DP0 TP0] Decode batch. #running-req: 27, #token: 15610, token usage: 0.09, accept len: 1.00, cuda graph: True, gen throughput (token/s): 280.43, #queue-req: 0, 
[1,1]<stdout>:[2025-10-11 22:17:53 DP1 TP8] Prefill batch. #new-seq: 2, #new-token: 452, #cached-token: 4, token usage: 0.09, #running-req: 30, #queue-req: 68, 
[1,1]<stdout>:[2025-10-11 22:17:53 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 8, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 67, 
[1,1]<stdout>:[2025-10-11 22:17:54 DP1 TP8] Prefill batch. #new-seq: 2, #new-token: 288, #cached-token: 6, token usage: 0.09, #running-req: 30, #queue-req: 65, 
[1,1]<stdout>:[2025-10-11 22:17:54 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 685, #cached-token: 8, token usage: 0.09, #running-req: 31, #queue-req: 64, 
[1,1]<stdout>:[2025-10-11 22:17:54 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 1170, #cached-token: 4, token usage: 0.09, #running-req: 31, #queue-req: 63, 
[1,1]<stdout>:[2025-10-11 22:17:55 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 3, #cached-token: 3, token usage: 0.10, #running-req: 31, #queue-req: 62, 
[1,1]<stdout>:[2025-10-11 22:17:55 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 2048, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 61, 
[1,1]<stdout>:[2025-10-11 22:17:55 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 1543, #cached-token: 0, token usage: 0.11, #running-req: 31, #queue-req: 61, 
[1,1]<stdout>:[2025-10-11 22:17:55 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 391, #cached-token: 1, token usage: 0.12, #running-req: 31, #queue-req: 60, 
[1,1]<stdout>:[2025-10-11 22:17:56 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 236, #cached-token: 2, token usage: 0.10, #running-req: 31, #queue-req: 59, 
[1,1]<stdout>:[2025-10-11 22:17:56 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 2048, #cached-token: 2, token usage: 0.10, #running-req: 31, #queue-req: 58, 
[1,1]<stdout>:[2025-10-11 22:17:56 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 1097, #cached-token: 0, token usage: 0.11, #running-req: 31, #queue-req: 58, 
[1,0]<stdout>:[2025-10-11 22:17:56 DP0 TP0] Decode batch. #running-req: 24, #token: 15438, token usage: 0.09, accept len: 1.00, cuda graph: True, gen throughput (token/s): 274.51, #queue-req: 0, 
[1,1]<stdout>:[2025-10-11 22:17:56 DP1 TP8] Decode batch. #running-req: 32, #token: 20056, token usage: 0.12, accept len: 1.00, cuda graph: True, gen throughput (token/s): 345.96, #queue-req: 58, 
[1,1]<stdout>:[2025-10-11 22:17:57 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 1347, #cached-token: 1, token usage: 0.12, #running-req: 31, #queue-req: 57, 
[1,1]<stdout>:[2025-10-11 22:17:57 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 451, #cached-token: 4, token usage: 0.12, #running-req: 31, #queue-req: 56, 
[1,1]<stdout>:[2025-10-11 22:17:57 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 14, #cached-token: 3, token usage: 0.12, #running-req: 31, #queue-req: 55, 
[1,1]<stdout>:[2025-10-11 22:17:58 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 57, #cached-token: 1, token usage: 0.12, #running-req: 31, #queue-req: 54, 
[1,1]<stdout>:[2025-10-11 22:17:58 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 542, #cached-token: 3, token usage: 0.12, #running-req: 31, #queue-req: 53, 
[1,1]<stdout>:[2025-10-11 22:17:58 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 446, #cached-token: 1, token usage: 0.12, #running-req: 31, #queue-req: 52, 
[1,1]<stdout>:[2025-10-11 22:17:59 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 432, #cached-token: 1, token usage: 0.12, #running-req: 31, #queue-req: 51, 
[1,0]<stdout>:[2025-10-11 22:17:59 DP0 TP0] Decode batch. #running-req: 19, #token: 12757, token usage: 0.08, accept len: 1.00, cuda graph: True, gen throughput (token/s): 307.43, #queue-req: 0, 
[1,1]<stdout>:[2025-10-11 22:17:59 DP1 TP8] Decode batch. #running-req: 32, #token: 20997, token usage: 0.12, accept len: 1.00, cuda graph: True, gen throughput (token/s): 437.77, #queue-req: 51, 
[1,1]<stdout>:[2025-10-11 22:17:59 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 8, #cached-token: 2, token usage: 0.12, #running-req: 31, #queue-req: 50, 
[1,1]<stdout>:[2025-10-11 22:18:00 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 50, #cached-token: 3, token usage: 0.12, #running-req: 31, #queue-req: 49, 
[1,1]<stdout>:[2025-10-11 22:18:00 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 148, #cached-token: 4, token usage: 0.12, #running-req: 31, #queue-req: 48, 
[1,1]<stdout>:[2025-10-11 22:18:01 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 784, #cached-token: 3, token usage: 0.12, #running-req: 31, #queue-req: 47, 
[1,1]<stdout>:[2025-10-11 22:18:01 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 2048, #cached-token: 1, token usage: 0.10, #running-req: 31, #queue-req: 46, 
[1,1]<stdout>:[2025-10-11 22:18:01 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 435, #cached-token: 0, token usage: 0.11, #running-req: 31, #queue-req: 46, 
[1,1]<stdout>:[2025-10-11 22:18:02 DP1 TP8] Prefill batch. #new-seq: 2, #new-token: 165, #cached-token: 5, token usage: 0.12, #running-req: 30, #queue-req: 44, 
[1,0]<stdout>:[2025-10-11 22:18:02 DP0 TP0] Decode batch. #running-req: 17, #token: 12308, token usage: 0.07, accept len: 1.00, cuda graph: True, gen throughput (token/s): 249.00, #queue-req: 0, 
[1,1]<stdout>:[2025-10-11 22:18:02 DP1 TP8] Decode batch. #running-req: 32, #token: 19478, token usage: 0.11, accept len: 1.00, cuda graph: True, gen throughput (token/s): 437.17, #queue-req: 44, 
[1,1]<stdout>:[2025-10-11 22:18:02 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 11, #cached-token: 1, token usage: 0.11, #running-req: 31, #queue-req: 43, 
[1,1]<stdout>:[2025-10-11 22:18:02 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 1236, #cached-token: 4, token usage: 0.11, #running-req: 31, #queue-req: 42, 
[1,1]<stdout>:[2025-10-11 22:18:03 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 752, #cached-token: 3, token usage: 0.12, #running-req: 31, #queue-req: 41, 
[1,1]<stdout>:[2025-10-11 22:18:03 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 354, #cached-token: 1, token usage: 0.12, #running-req: 31, #queue-req: 40, 
[1,1]<stdout>:[2025-10-11 22:18:04 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 25, #cached-token: 3, token usage: 0.12, #running-req: 31, #queue-req: 39, 
[1,1]<stdout>:[2025-10-11 22:18:04 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 389, #cached-token: 4, token usage: 0.12, #running-req: 31, #queue-req: 38, 
[1,0]<stdout>:[2025-10-11 22:18:05 DP0 TP0] Decode batch. #running-req: 15, #token: 12083, token usage: 0.07, accept len: 1.00, cuda graph: True, gen throughput (token/s): 222.17, #queue-req: 0, 
[1,1]<stdout>:[2025-10-11 22:18:05 DP1 TP8] Decode batch. #running-req: 32, #token: 21193, token usage: 0.12, accept len: 1.00, cuda graph: True, gen throughput (token/s): 454.32, #queue-req: 38, 
[1,1]<stdout>:[2025-10-11 22:18:05 DP1 TP8] Prefill batch. #new-seq: 2, #new-token: 91, #cached-token: 4, token usage: 0.12, #running-req: 30, #queue-req: 36, 
[1,1]<stdout>:[2025-10-11 22:18:06 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 8, #cached-token: 3, token usage: 0.13, #running-req: 31, #queue-req: 35, 
[1,1]<stdout>:[2025-10-11 22:18:06 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 183, #cached-token: 5, token usage: 0.12, #running-req: 31, #queue-req: 34, 
[1,1]<stdout>:[2025-10-11 22:18:07 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 309, #cached-token: 5, token usage: 0.11, #running-req: 31, #queue-req: 33, 
[1,1]<stdout>:[2025-10-11 22:18:07 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 541, #cached-token: 4, token usage: 0.10, #running-req: 31, #queue-req: 32, 
[1,1]<stdout>:[2025-10-11 22:18:07 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 39, #cached-token: 4, token usage: 0.10, #running-req: 31, #queue-req: 31, 
[1,1]<stdout>:[2025-10-11 22:18:07 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 6, #cached-token: 3, token usage: 0.10, #running-req: 31, #queue-req: 30, 
[1,1]<stdout>:[2025-10-11 22:18:08 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 555, #cached-token: 6, token usage: 0.10, #running-req: 31, #queue-req: 29, 
[1,1]<stdout>:[2025-10-11 22:18:08 DP1 TP8] Decode batch. #running-req: 32, #token: 17182, token usage: 0.10, accept len: 1.00, cuda graph: True, gen throughput (token/s): 421.64, #queue-req: 29, 
[1,0]<stdout>:[2025-10-11 22:18:08 DP0 TP0] Decode batch. #running-req: 14, #token: 12256, token usage: 0.07, accept len: 1.00, cuda graph: True, gen throughput (token/s): 194.37, #queue-req: 0, 
[1,1]<stdout>:[2025-10-11 22:18:08 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 451, #cached-token: 3, token usage: 0.09, #running-req: 31, #queue-req: 28, 
[1,1]<stdout>:[2025-10-11 22:18:08 DP1 TP8] Prefill batch. #new-seq: 2, #new-token: 354, #cached-token: 6, token usage: 0.10, #running-req: 30, #queue-req: 26, 
[1,1]<stdout>:[2025-10-11 22:18:09 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 73, #cached-token: 1, token usage: 0.10, #running-req: 31, #queue-req: 25, 
[1,1]<stdout>:[2025-10-11 22:18:09 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 1, token usage: 0.10, #running-req: 31, #queue-req: 24, 
[1,1]<stdout>:[2025-10-11 22:18:10 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 42, #cached-token: 3, token usage: 0.10, #running-req: 31, #queue-req: 23, 
[1,1]<stdout>:[2025-10-11 22:18:10 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 12, #cached-token: 3, token usage: 0.09, #running-req: 31, #queue-req: 22, 
[1,0]<stdout>:[2025-10-11 22:18:11 DP0 TP0] Decode batch. #running-req: 13, #token: 12233, token usage: 0.07, accept len: 1.00, cuda graph: True, gen throughput (token/s): 191.20, #queue-req: 0, 
[1,1]<stdout>:[2025-10-11 22:18:11 DP1 TP8] Decode batch. #running-req: 32, #token: 16414, token usage: 0.10, accept len: 1.00, cuda graph: True, gen throughput (token/s): 460.93, #queue-req: 22, 
[1,1]<stdout>:[2025-10-11 22:18:11 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 20, #cached-token: 5, token usage: 0.10, #running-req: 31, #queue-req: 21, 
[1,1]<stdout>:[2025-10-11 22:18:12 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 1049, #cached-token: 9, token usage: 0.10, #running-req: 31, #queue-req: 20, 
[1,1]<stdout>:[2025-10-11 22:18:12 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 1885, #cached-token: 2, token usage: 0.10, #running-req: 31, #queue-req: 19, 
[1,1]<stdout>:[2025-10-11 22:18:13 DP1 TP8] Prefill batch. #new-seq: 2, #new-token: 228, #cached-token: 4, token usage: 0.10, #running-req: 30, #queue-req: 17, 
[1,0]<stdout>:[2025-10-11 22:18:13 DP0 TP0] Decode batch. #running-req: 12, #token: 11474, token usage: 0.07, accept len: 1.00, cuda graph: True, gen throughput (token/s): 204.34, #queue-req: 0, 
[1,1]<stdout>:[2025-10-11 22:18:13 DP1 TP8] Decode batch. #running-req: 32, #token: 16762, token usage: 0.10, accept len: 1.00, cuda graph: True, gen throughput (token/s): 515.91, #queue-req: 17, 
[1,1]<stdout>:[2025-10-11 22:18:13 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 11, #cached-token: 1, token usage: 0.10, #running-req: 31, #queue-req: 16, 
[1,1]<stdout>:[2025-10-11 22:18:14 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 358, #cached-token: 2, token usage: 0.10, #running-req: 31, #queue-req: 15, 
[1,1]<stdout>:[2025-10-11 22:18:14 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 329, #cached-token: 2, token usage: 0.08, #running-req: 31, #queue-req: 14, 
[1,1]<stdout>:[2025-10-11 22:18:14 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 1560, #cached-token: 2, token usage: 0.08, #running-req: 31, #queue-req: 13, 
[1,1]<stdout>:[2025-10-11 22:18:15 DP1 TP8] Prefill batch. #new-seq: 2, #new-token: 1203, #cached-token: 5, token usage: 0.09, #running-req: 30, #queue-req: 11, 
[1,1]<stdout>:[2025-10-11 22:18:15 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 865, #cached-token: 4, token usage: 0.10, #running-req: 31, #queue-req: 10, 
[1,1]<stdout>:[2025-10-11 22:18:15 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 9, #cached-token: 4, token usage: 0.10, #running-req: 31, #queue-req: 9, 
[1,1]<stdout>:[2025-10-11 22:18:16 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 8, 
[1,1]<stdout>:[2025-10-11 22:18:16 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 483, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 7, 
[1,0]<stdout>:[2025-10-11 22:18:16 DP0 TP0] Decode batch. #running-req: 11, #token: 8475, token usage: 0.05, accept len: 1.00, cuda graph: True, gen throughput (token/s): 143.96, #queue-req: 0, 
[1,1]<stdout>:[2025-10-11 22:18:16 DP1 TP8] Decode batch. #running-req: 31, #token: 16038, token usage: 0.09, accept len: 1.00, cuda graph: True, gen throughput (token/s): 399.71, #queue-req: 7, 
[1,1]<stdout>:[2025-10-11 22:18:16 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 26, #cached-token: 2, token usage: 0.09, #running-req: 31, #queue-req: 6, 
[1,1]<stdout>:[2025-10-11 22:18:17 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 16, #cached-token: 2, token usage: 0.09, #running-req: 31, #queue-req: 5, 
[1,1]<stdout>:[2025-10-11 22:18:17 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 1220, #cached-token: 2, token usage: 0.09, #running-req: 31, #queue-req: 4, 
[1,1]<stdout>:[2025-10-11 22:18:18 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 109, #cached-token: 2, token usage: 0.09, #running-req: 31, #queue-req: 3, 
[1,1]<stdout>:[2025-10-11 22:18:19 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 16, #cached-token: 3, token usage: 0.10, #running-req: 31, #queue-req: 2, 
[1,0]<stdout>:[2025-10-11 22:18:19 DP0 TP0] Decode batch. #running-req: 9, #token: 5544, token usage: 0.03, accept len: 1.00, cuda graph: True, gen throughput (token/s): 159.32, #queue-req: 0, 
[1,1]<stdout>:[2025-10-11 22:18:19 DP1 TP8] Decode batch. #running-req: 32, #token: 14888, token usage: 0.09, accept len: 1.00, cuda graph: True, gen throughput (token/s): 490.99, #queue-req: 2, 
[1,1]<stdout>:[2025-10-11 22:18:19 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 19, #cached-token: 3, token usage: 0.09, #running-req: 31, #queue-req: 1, 
[1,1]<stdout>:[2025-10-11 22:18:19 DP1 TP8] Prefill batch. #new-seq: 1, #new-token: 232, #cached-token: 1, token usage: 0.09, #running-req: 31, #queue-req: 0, 
[1,0]<stdout>:[2025-10-11 22:18:21 DP0 TP0] Decode batch. #running-req: 8, #token: 5398, token usage: 0.03, accept len: 1.00, cuda graph: True, gen throughput (token/s): 147.52, #queue-req: 0, 
[1,1]<stdout>:[2025-10-11 22:18:21 DP1 TP8] Decode batch. #running-req: 27, #token: 11101, token usage: 0.07, accept len: 1.00, cuda graph: True, gen throughput (token/s): 531.83, #queue-req: 0, 
[1,0]<stdout>:[2025-10-11 22:18:23 DP0 TP0] Decode batch. #running-req: 7, #token: 5129, token usage: 0.03, accept len: 1.00, cuda graph: True, gen throughput (token/s): 161.06, #queue-req: 0, 
[1,1]<stdout>:[2025-10-11 22:18:23 DP1 TP8] Decode batch. #running-req: 19, #token: 9680, token usage: 0.06, accept len: 1.00, cuda graph: True, gen throughput (token/s): 494.16, #queue-req: 0, 
[1,0]<stdout>:[2025-10-11 22:18:25 DP0 TP0] Decode batch. #running-req: 7, #token: 5409, token usage: 0.03, accept len: 1.00, cuda graph: True, gen throughput (token/s): 153.95, #queue-req: 0, 
[1,1]<stdout>:[2025-10-11 22:18:25 DP1 TP8] Decode batch. #running-req: 19, #token: 10440, token usage: 0.06, accept len: 1.00, cuda graph: True, gen throughput (token/s): 417.85, #queue-req: 0, 
[1,0]<stdout>:[2025-10-11 22:18:27 DP0 TP0] Decode batch. #running-req: 7, #token: 5689, token usage: 0.03, accept len: 1.00, cuda graph: True, gen throughput (token/s): 139.10, #queue-req: 0, 
[1,1]<stdout>:[2025-10-11 22:18:27 DP1 TP8] Decode batch. #running-req: 16, #token: 10245, token usage: 0.06, accept len: 1.00, cuda graph: True, gen throughput (token/s): 336.83, #queue-req: 0, 
[1,0]<stdout>:[2025-10-11 22:18:29 DP0 TP0] Decode batch. #running-req: 4, #token: 3795, token usage: 0.02, accept len: 1.00, cuda graph: True, gen throughput (token/s): 110.97, #queue-req: 0, 
[1,1]<stdout>:[2025-10-11 22:18:29 DP1 TP8] Decode batch. #running-req: 16, #token: 10885, token usage: 0.06, accept len: 1.00, cuda graph: True, gen throughput (token/s): 353.33, #queue-req: 0, 
[1,0]<stdout>:[2025-10-11 22:18:30 DP0 TP0] Decode batch. #running-req: 4, #token: 3955, token usage: 0.02, accept len: 1.00, cuda graph: True, gen throughput (token/s): 90.81, #queue-req: 0, 
[1,1]<stdout>:[2025-10-11 22:18:30 DP1 TP8] Decode batch. #running-req: 12, #token: 9619, token usage: 0.06, accept len: 1.00, cuda graph: True, gen throughput (token/s): 338.32, #queue-req: 0, 
[1,1]<stdout>:[2025-10-11 22:18:32 DP1 TP8] Decode batch. #running-req: 11, #token: 9753, token usage: 0.06, accept len: 1.00, cuda graph: True, gen throughput (token/s): 268.97, #queue-req: 0, 
[1,0]<stdout>:[2025-10-11 22:18:32 DP0 TP0] Decode batch. #running-req: 4, #token: 4115, token usage: 0.02, accept len: 1.00, cuda graph: True, gen throughput (token/s): 91.17, #queue-req: 0, 
[1,1]<stdout>:[2025-10-11 22:18:34 DP1 TP8] Decode batch. #running-req: 9, #token: 8285, token usage: 0.05, accept len: 1.00, cuda graph: True, gen throughput (token/s): 208.53, #queue-req: 0, 
[1,1]<stdout>:[2025-10-11 22:18:36 DP1 TP8] Decode batch. #running-req: 9, #token: 8645, token usage: 0.05, accept len: 1.00, cuda graph: True, gen throughput (token/s): 205.00, #queue-req: 0, 
[1,1]<stdout>:[2025-10-11 22:18:37 DP1 TP8] Decode batch. #running-req: 6, #token: 6970, token usage: 0.04, accept len: 1.00, cuda graph: True, gen throughput (token/s): 175.87, #queue-req: 0, 
[1,1]<stdout>:[2025-10-11 22:18:39 DP1 TP8] Decode batch. #running-req: 5, #token: 5739, token usage: 0.03, accept len: 1.00, cuda graph: True, gen throughput (token/s): 134.23, #queue-req: 0, 
[1,1]<stdout>:[2025-10-11 22:18:41 DP1 TP8] Decode batch. #running-req: 3, #token: 3401, token usage: 0.02, accept len: 1.00, cuda graph: True, gen throughput (token/s): 84.73, #queue-req: 0, 
[1,1]<stdout>:[2025-10-11 22:18:43 DP1 TP8] Decode batch. #running-req: 3, #token: 3521, token usage: 0.02, accept len: 1.00, cuda graph: True, gen throughput (token/s): 69.42, #queue-req: 0, 
[1,1]<stdout>:[2025-10-11 22:18:44 DP1 TP8] Decode batch. #running-req: 2, #token: 2831, token usage: 0.02, accept len: 1.00, cuda graph: True, gen throughput (token/s): 47.05, #queue-req: 0, 
[1,1]<stdout>:[2025-10-11 22:18:46 DP1 TP8] Decode batch. #running-req: 2, #token: 2911, token usage: 0.02, accept len: 1.00, cuda graph: True, gen throughput (token/s): 46.42, #queue-req: 0, 
[1,1]<stdout>:[2025-10-11 22:18:48 DP1 TP8] Decode batch. #running-req: 1, #token: 2265, token usage: 0.01, accept len: 1.00, cuda graph: True, gen throughput (token/s): 31.08, #queue-req: 0, 
[1,1]<stdout>:[2025-10-11 22:18:50 DP1 TP8] Decode batch. #running-req: 1, #token: 2305, token usage: 0.01, accept len: 1.00, cuda graph: True, gen throughput (token/s): 23.37, #queue-req: 0, 
[1,1]<stdout>:[2025-10-11 22:18:51 DP1 TP8] Decode batch. #running-req: 1, #token: 2345, token usage: 0.01, accept len: 1.00, cuda graph: True, gen throughput (token/s): 23.23, #queue-req: 0, 
[1,0]<stdout>:
[1,0]<stdout>:====== Offline Throughput Benchmark Result =======
[1,0]<stdout>:Backend:                                 engine    
[1,0]<stdout>:Successful requests:                     2000      
[1,0]<stdout>:Benchmark duration (s):                  564.40    
[1,0]<stdout>:Total input tokens:                      626729    
[1,0]<stdout>:Total generated tokens:                  388685    
[1,0]<stdout>:Last generation throughput (tok/s):      23.23     
[1,0]<stdout>:Request throughput (req/s):              3.54      
[1,0]<stdout>:Input token throughput (tok/s):          1110.42   
[1,0]<stdout>:Output token throughput (tok/s):         688.66    
[1,0]<stdout>:Total token throughput (tok/s):          1799.09   
[1,0]<stdout>:==================================================
[1,1]<stdout>:[2025-10-11 22:18:54 DP1 TP15] Scheduler hit an exception: Traceback (most recent call last):
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2580, in run_scheduler_process
[1,1]<stdout>:    scheduler.event_loop_normal()
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1,1]<stdout>:    return func(*args, **kwargs)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 768, in event_loop_normal
[1,1]<stdout>:    recv_reqs = self.recv_requests()
[1,1]<stdout>:                ^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 1032, in recv_requests
[1,1]<stdout>:    control_reqs = broadcast_pyobj(
[1,1]<stdout>:                   ^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/utils.py", line 1094, in broadcast_pyobj
[1,1]<stdout>:    dist.broadcast(tensor_size, src=src, group=dist_group)
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
[1,1]<stdout>:    return func(*args, **kwargs)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py", line 2830, in broadcast
[1,1]<stdout>:    work.wait()
[1,1]<stdout>:RuntimeError: [/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:535] Read error [10.104.4.79]:28079: Connection reset by peer
[1,1]<stdout>:
[1,1]<stdout>:[2025-10-11 22:18:54 DP1 TP10] Scheduler hit an exception: Traceback (most recent call last):
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2580, in run_scheduler_process
[1,1]<stdout>:    scheduler.event_loop_normal()
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1,1]<stdout>:    return func(*args, **kwargs)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 768, in event_loop_normal
[1,1]<stdout>:    recv_reqs = self.recv_requests()
[1,1]<stdout>:                ^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 1032, in recv_requests
[1,1]<stdout>:    control_reqs = broadcast_pyobj(
[1,1]<stdout>:                   ^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/utils.py", line 1094, in broadcast_pyobj
[1,1]<stdout>:    dist.broadcast(tensor_size, src=src, group=dist_group)
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
[1,1]<stdout>:    return func(*args, **kwargs)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py", line 2830, in broadcast
[1,1]<stdout>:    work.wait()
[1,1]<stdout>:RuntimeError: [/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:535] Read error [10.104.4.79]:65414: Connection reset by peer
[1,1]<stdout>:
[1,0]<stdout>:[RANK 0] Benchmark completed successfully
=>> PBS: job killed: walltime 1827 exceeded limit 1800
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
			Resource Usage on 2025-10-11 22:34:14.192339:
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	JobId: 97076.pbs111
	Project: 50000128
	Exit Status: -29
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	NCPUs: Requested(224), Used(224)
	CPU Time Used: 03:23:51
	Memory: Requested(3760gb), Used(71982636kb)
	Vmem Used: 82229022616kb
	Walltime: Requested(00:30:00), Used(00:30:35)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	Execution Nodes Used: (a2ap-dgx005:ncpus=112:ngpus=8:mem=1971322880kb)+(a2ap-dgx006:ncpus=112:ngpus=8:mem=1971322880kb)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	GPU Duration: 30.73mins
	GPU Power Consumed: 247.87W
	GPU Max GPU Memory Used: 1.24TB
	Memory Throughput Rate (Average): a2ap-dgx005:(gpu1:2%+gpu0:1%+gpu2:2%+gpu3:2%+gpu5:2%+gpu4:1%+gpu6:2%+gpu7:1%)+a2ap-dgx006:(gpu1:1%+gpu0:1%+gpu2:1%+gpu3:2%+gpu5:1%+gpu4:2%+gpu6:1%+gpu7:1%)
	Memory Throughput Rate (Max): a2ap-dgx005:(gpu1:19%+gpu0:31%+gpu2:19%+gpu3:22%+gpu5:20%+gpu4:21%+gpu6:21%+gpu7:22%)+a2ap-dgx006:(gpu1:8%+gpu0:10%+gpu2:10%+gpu3:33%+gpu5:27%+gpu4:27%+gpu6:29%+gpu7:8%)
	Memory Throughput Rate (Min): a2ap-dgx005:(gpu1:0%+gpu0:0%+gpu2:0%+gpu3:0%+gpu5:0%+gpu4:0%+gpu6:0%+gpu7:0%)+a2ap-dgx006:(gpu1:0%+gpu0:0%+gpu2:0%+gpu3:0%+gpu5:0%+gpu4:0%+gpu6:0%+gpu7:0%)
	GPU SM Utilization (Average): a2ap-dgx005:(gpu1:34%+gpu0:32%+gpu2:32%+gpu3:32%+gpu5:31%+gpu4:31%+gpu6:31%+gpu7:32%)+a2ap-dgx006:(gpu1:32%+gpu0:33%+gpu2:34%+gpu3:32%+gpu5:30%+gpu4:32%+gpu6:32%+gpu7:30%)
	GPU SM Utilization (Max): a2ap-dgx005:(gpu1:100%+gpu0:100%+gpu2:98%+gpu3:98%+gpu5:100%+gpu4:100%+gpu6:98%+gpu7:98%)+a2ap-dgx006:(gpu1:97%+gpu0:100%+gpu2:100%+gpu3:100%+gpu5:100%+gpu4:100%+gpu6:100%+gpu7:98%)
	GPU SM Utilization (Min): a2ap-dgx005:(gpu1:0%+gpu0:0%+gpu2:0%+gpu3:0%+gpu5:0%+gpu4:0%+gpu6:0%+gpu7:0%)+a2ap-dgx006:(gpu1:0%+gpu0:0%+gpu2:0%+gpu3:0%+gpu5:0%+gpu4:0%+gpu6:0%+gpu7:0%)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Warning: None
GPU application profile: Medium
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

