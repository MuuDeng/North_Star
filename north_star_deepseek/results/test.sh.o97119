========== FA3 OPTIMIZED BENCHMARK ==========
Target: <15min | Est. SU: 238.934 | Balance: 40737.496
N/A
Job ID: 97119.pbs111 | GPUs: 16 | Master: a2ap-dgx007.asp2p.nscc.sg:5000
Config: TP16+DP2 | Attention: FlashAttention-3 | NCCL: Optimized
=============================================
[02:54:15] Launching FA3-optimized benchmark...
[1,0]<stdout>:WARNING:sglang.srt.server_args:DP attention is enabled. The chunked prefill size is adjusted to 4096 to avoid MoE kernel issues. 
[1,0]<stdout>:WARNING:sglang.srt.server_args:Overlap scheduler is disabled because of using eagle speculative decoding.
[1,0]<stdout>:WARNING:sglang.srt.server_args:Mixed chunked prefill is disabled because of using eagle speculative decoding.
[1,0]<stdout>:WARNING:sglang.srt.server_args:DP attention is enabled. The chunked prefill size is adjusted to 2048 to avoid MoE kernel issues. 
[1,0]<stdout>:WARNING:sglang.srt.server_args:Overlap scheduler is disabled because of using eagle speculative decoding.
[1,0]<stdout>:WARNING:sglang.srt.server_args:DeepSeek MTP does not require setting speculative_draft_model_path.
[1,0]<stdout>:[2025-10-12 02:54:33] Using default HuggingFace chat template with detected content format: string
[1,1]<stdout>:WARNING:sglang.srt.server_args:DP attention is enabled. The chunked prefill size is adjusted to 4096 to avoid MoE kernel issues. 
[1,1]<stdout>:WARNING:sglang.srt.server_args:Overlap scheduler is disabled because of using eagle speculative decoding.
[1,1]<stdout>:WARNING:sglang.srt.server_args:Mixed chunked prefill is disabled because of using eagle speculative decoding.
[1,1]<stdout>:WARNING:sglang.srt.server_args:DP attention is enabled. The chunked prefill size is adjusted to 2048 to avoid MoE kernel issues. 
[1,1]<stdout>:WARNING:sglang.srt.server_args:Overlap scheduler is disabled because of using eagle speculative decoding.
[1,1]<stdout>:WARNING:sglang.srt.server_args:DeepSeek MTP does not require setting speculative_draft_model_path.
[1,0]<stdout>:[2025-10-12 02:55:17 DP0 TP0] MLA optimization is turned on. Use fa3 backend.
[1,0]<stdout>:[2025-10-12 02:55:17 DP0 TP0] Chunked prefix cache is turned on.
[1,0]<stdout>:[2025-10-12 02:55:17 DP0 TP0] Init torch distributed begin.
[1,1]<stdout>:[W1012 02:55:18.890616646 Utils.hpp:137] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[1,1]<stdout>:[W1012 02:55:18.890652615 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[1,0]<stdout>:[W1012 02:55:18.816156170 Utils.hpp:137] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[1,0]<stdout>:[W1012 02:55:18.816187376 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[1,1]<stdout>:[W1012 02:55:19.894738288 Utils.hpp:137] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[1,1]<stdout>:[W1012 02:55:19.894778185 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[1,0]<stdout>:[W1012 02:55:19.908466856 Utils.hpp:137] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[1,0]<stdout>:[W1012 02:55:19.908508593 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[1,0]<stdout>:[W1012 02:55:20.157263501 Utils.hpp:137] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[1,0]<stdout>:[W1012 02:55:20.157294539 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[1,1]<stdout>:[W1012 02:55:20.110023313 Utils.hpp:137] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[1,1]<stdout>:[W1012 02:55:20.110049596 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[1,0]<stdout>:[W1012 02:55:20.874320481 Utils.hpp:137] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[1,0]<stdout>:[W1012 02:55:20.874351759 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[1,0]<stdout>:[W1012 02:55:20.967286342 Utils.hpp:137] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[1,0]<stdout>:[W1012 02:55:20.967310200 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[1,0]<stdout>:[W1012 02:55:20.010286582 Utils.hpp:137] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[1,0]<stdout>:[W1012 02:55:20.010308153 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[1,0]<stdout>:[W1012 02:55:20.040361494 Utils.hpp:137] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[1,0]<stdout>:[W1012 02:55:20.040390941 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[1,1]<stdout>:[W1012 02:55:21.590107375 Utils.hpp:137] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[1,1]<stdout>:[W1012 02:55:21.590145784 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[1,1]<stdout>:[W1012 02:55:21.592311058 Utils.hpp:137] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[1,1]<stdout>:[W1012 02:55:21.592336292 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[1,1]<stdout>:[W1012 02:55:21.603547617 Utils.hpp:137] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[1,1]<stdout>:[W1012 02:55:21.603569154 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[1,1]<stdout>:[W1012 02:55:21.682089498 Utils.hpp:137] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[1,1]<stdout>:[W1012 02:55:21.682113166 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[1,1]<stdout>:[W1012 02:55:21.707282185 Utils.hpp:137] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[1,1]<stdout>:[W1012 02:55:21.707306712 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[1,0]<stdout>:[W1012 02:55:21.212162169 Utils.hpp:137] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[1,0]<stdout>:[W1012 02:55:21.212188426 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[1,0]<stdout>:[Gloo] Rank 0 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,0]<stdout>:[Gloo] Rank 1 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,0]<stdout>:[Gloo] Rank 2 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,1]<stdout>:[Gloo] Rank 14 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,0]<stdout>:[Gloo] Rank 3 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,0]<stdout>:[Gloo] Rank 4 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,1]<stdout>:[Gloo] Rank 9 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,0]<stdout>:[Gloo] Rank 5 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,0]<stdout>:[Gloo] Rank 6 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,0]<stdout>:[Gloo] Rank 7 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,1]<stdout>:[Gloo] Rank 12 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,1]<stdout>:[Gloo] Rank 10 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,1]<stdout>:[Gloo] Rank 8 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,1]<stdout>:[Gloo] Rank 11 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,1]<stdout>:[Gloo] Rank 13 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,1]<stdout>:[Gloo] Rank 15 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,0]<stdout>:[Gloo] Rank 0 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,0]<stdout>:[2025-10-12 02:55:21 DP0 TP0] sglang is using nccl==2.27.3
[1,1]<stdout>:[Gloo] Rank 11 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,1]<stdout>:[Gloo] Rank 13 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,1]<stdout>:[Gloo] Rank 8 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,1]<stdout>:[Gloo] Rank 9 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,1]<stdout>:[Gloo] Rank 12 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,1]<stdout>:[Gloo] Rank 10 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,1]<stdout>:[Gloo] Rank 14 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,0]<stdout>:[Gloo] Rank 2 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,1]<stdout>:[Gloo] Rank 15 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,0]<stdout>:[Gloo] Rank 3 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,0]<stdout>:[Gloo] Rank 1 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,0]<stdout>:[Gloo] Rank 4 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,0]<stdout>:[Gloo] Rank 5 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,0]<stdout>:[Gloo] Rank 7 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,0]<stdout>:[Gloo] Rank 6 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,0]<stdout>:NCCL version 2.27.3+cuda12.9
[1,1]<stdout>:
[1,1]<stdout>:[2025-10-12 02:55:27] a2ap-dgx010:712793:712793 [0] transport/net_ib.cc:587 NCCL WARN NET/IB : Attempted to merge incompatible devices: [1]mlx5_1:1/IB and [2]mlx5_2:1/RoCE. Try selecting NICs of only one link type using NCCL_IB_HCA
[1,1]<stdout>:
[1,1]<stdout>:[2025-10-12 02:55:27] a2ap-dgx010:712793:712793 [0] transport/net_ib.cc:587 NCCL WARN NET/IB : Attempted to merge incompatible devices: [7]mlx5_7:1/IB and [8]mlx5_8:1/RoCE. Try selecting NICs of only one link type using NCCL_IB_HCA
[1,1]<stdout>:
[1,1]<stdout>:[2025-10-12 02:55:27] a2ap-dgx010:712794:712794 [1] transport/net_ib.cc:587 NCCL WARN NET/IB : Attempted to merge incompatible devices: [1]mlx5_1:1/IB and [2]mlx5_2:1/RoCE. Try selecting NICs of only one link type using NCCL_IB_HCA
[1,1]<stdout>:
[1,1]<stdout>:[2025-10-12 02:55:27] a2ap-dgx010:712794:712794 [1] transport/net_ib.cc:587 NCCL WARN NET/IB : Attempted to merge incompatible devices: [7]mlx5_7:1/IB and [8]mlx5_8:1/RoCE. Try selecting NICs of only one link type using NCCL_IB_HCA
[1,1]<stdout>:
[1,1]<stdout>:[2025-10-12 02:55:27] a2ap-dgx010:712798:712798 [5] transport/net_ib.cc:587 NCCL WARN NET/IB : Attempted to merge incompatible devices: [1]mlx5_1:1/IB and [2]mlx5_2:1/RoCE. Try selecting NICs of only one link type using NCCL_IB_HCA
[1,1]<stdout>:
[1,1]<stdout>:[2025-10-12 02:55:27] a2ap-dgx010:712798:712798 [5] transport/net_ib.cc:587 NCCL WARN NET/IB : Attempted to merge incompatible devices: [7]mlx5_7:1/IB and [8]mlx5_8:1/RoCE. Try selecting NICs of only one link type using NCCL_IB_HCA
[1,1]<stdout>:
[1,1]<stdout>:[2025-10-12 02:55:27] a2ap-dgx010:712799:712799 [6] transport/net_ib.cc:587 NCCL WARN NET/IB : Attempted to merge incompatible devices: [1]mlx5_1:1/IB and [2]mlx5_2:1/RoCE. Try selecting NICs of only one link type using NCCL_IB_HCA
[1,1]<stdout>:
[1,1]<stdout>:[2025-10-12 02:55:27] a2ap-dgx010:712799:712799 [6] transport/net_ib.cc:587 NCCL WARN NET/IB : Attempted to merge incompatible devices: [7]mlx5_7:1/IB and [8]mlx5_8:1/RoCE. Try selecting NICs of only one link type using NCCL_IB_HCA
[1,1]<stdout>:
[1,1]<stdout>:[2025-10-12 02:55:27] a2ap-dgx010:712795:712795 [2] transport/net_ib.cc:587 NCCL WARN NET/IB : Attempted to merge incompatible devices: [1]mlx5_1:1/IB and [2]mlx5_2:1/RoCE. Try selecting NICs of only one link type using NCCL_IB_HCA
[1,1]<stdout>:
[1,1]<stdout>:[2025-10-12 02:55:27] a2ap-dgx010:712795:712795 [2] transport/net_ib.cc:587 NCCL WARN NET/IB : Attempted to merge incompatible devices: [7]mlx5_7:1/IB and [8]mlx5_8:1/RoCE. Try selecting NICs of only one link type using NCCL_IB_HCA
[1,1]<stdout>:
[1,1]<stdout>:[2025-10-12 02:55:27] a2ap-dgx010:712800:712800 [7] transport/net_ib.cc:587 NCCL WARN NET/IB : Attempted to merge incompatible devices: [1]mlx5_1:1/IB and [2]mlx5_2:1/RoCE. Try selecting NICs of only one link type using NCCL_IB_HCA
[1,1]<stdout>:
[1,1]<stdout>:[2025-10-12 02:55:27] a2ap-dgx010:712800:712800 [7] transport/net_ib.cc:587 NCCL WARN NET/IB : Attempted to merge incompatible devices: [7]mlx5_7:1/IB and [8]mlx5_8:1/RoCE. Try selecting NICs of only one link type using NCCL_IB_HCA
[1,1]<stdout>:
[1,1]<stdout>:[2025-10-12 02:55:27] a2ap-dgx010:712797:712797 [4] transport/net_ib.cc:587 NCCL WARN NET/IB : Attempted to merge incompatible devices: [1]mlx5_1:1/IB and [2]mlx5_2:1/RoCE. Try selecting NICs of only one link type using NCCL_IB_HCA
[1,1]<stdout>:
[1,1]<stdout>:[2025-10-12 02:55:27] a2ap-dgx010:712797:712797 [4] transport/net_ib.cc:587 NCCL WARN NET/IB : Attempted to merge incompatible devices: [7]mlx5_7:1/IB and [8]mlx5_8:1/RoCE. Try selecting NICs of only one link type using NCCL_IB_HCA
[1,1]<stdout>:
[1,1]<stdout>:[2025-10-12 02:55:27] a2ap-dgx010:712796:712796 [3] transport/net_ib.cc:587 NCCL WARN NET/IB : Attempted to merge incompatible devices: [1]mlx5_1:1/IB and [2]mlx5_2:1/RoCE. Try selecting NICs of only one link type using NCCL_IB_HCA
[1,1]<stdout>:
[1,1]<stdout>:[2025-10-12 02:55:27] a2ap-dgx010:712796:712796 [3] transport/net_ib.cc:587 NCCL WARN NET/IB : Attempted to merge incompatible devices: [7]mlx5_7:1/IB and [8]mlx5_8:1/RoCE. Try selecting NICs of only one link type using NCCL_IB_HCA
[1,0]<stdout>:
[1,0]<stdout>:[2025-10-12 02:55:27] a2ap-dgx007:1177060:1177060 [4] transport/net_ib.cc:587 NCCL WARN NET/IB : Attempted to merge incompatible devices: [1]mlx5_1:1/IB and [2]mlx5_2:1/RoCE. Try selecting NICs of only one link type using NCCL_IB_HCA
[1,0]<stdout>:
[1,0]<stdout>:[2025-10-12 02:55:27] a2ap-dgx007:1177060:1177060 [4] transport/net_ib.cc:587 NCCL WARN NET/IB : Attempted to merge incompatible devices: [7]mlx5_7:1/IB and [8]mlx5_8:1/RoCE. Try selecting NICs of only one link type using NCCL_IB_HCA
[1,0]<stdout>:
[1,0]<stdout>:[2025-10-12 02:55:27] a2ap-dgx007:1177056:1177056 [0] transport/net_ib.cc:587 NCCL WARN NET/IB : Attempted to merge incompatible devices: [1]mlx5_1:1/IB and [2]mlx5_2:1/RoCE. Try selecting NICs of only one link type using NCCL_IB_HCA
[1,0]<stdout>:
[1,0]<stdout>:[2025-10-12 02:55:27] a2ap-dgx007:1177056:1177056 [0] transport/net_ib.cc:587 NCCL WARN NET/IB : Attempted to merge incompatible devices: [7]mlx5_7:1/IB and [8]mlx5_8:1/RoCE. Try selecting NICs of only one link type using NCCL_IB_HCA
[1,0]<stdout>:
[1,0]<stdout>:[2025-10-12 02:55:27] a2ap-dgx007:1177061:1177061 [5] transport/net_ib.cc:587 NCCL WARN NET/IB : Attempted to merge incompatible devices: [1]mlx5_1:1/IB and [2]mlx5_2:1/RoCE. Try selecting NICs of only one link type using NCCL_IB_HCA
[1,0]<stdout>:
[1,0]<stdout>:[2025-10-12 02:55:27] a2ap-dgx007:1177061:1177061 [5] transport/net_ib.cc:587 NCCL WARN NET/IB : Attempted to merge incompatible devices: [7]mlx5_7:1/IB and [8]mlx5_8:1/RoCE. Try selecting NICs of only one link type using NCCL_IB_HCA
[1,0]<stdout>:
[1,0]<stdout>:[2025-10-12 02:55:27] a2ap-dgx007:1177062:1177062 [6] transport/net_ib.cc:587 NCCL WARN NET/IB : Attempted to merge incompatible devices: [1]mlx5_1:1/IB and [2]mlx5_2:1/RoCE. Try selecting NICs of only one link type using NCCL_IB_HCA
[1,0]<stdout>:
[1,0]<stdout>:[2025-10-12 02:55:27] a2ap-dgx007:1177062:1177062 [6] transport/net_ib.cc:587 NCCL WARN NET/IB : Attempted to merge incompatible devices: [7]mlx5_7:1/IB and [8]mlx5_8:1/RoCE. Try selecting NICs of only one link type using NCCL_IB_HCA
[1,0]<stdout>:
[1,0]<stdout>:[2025-10-12 02:55:27] a2ap-dgx007:1177058:1177058 [2] transport/net_ib.cc:587 NCCL WARN NET/IB : Attempted to merge incompatible devices: [1]mlx5_1:1/IB and [2]mlx5_2:1/RoCE. Try selecting NICs of only one link type using NCCL_IB_HCA
[1,0]<stdout>:
[1,0]<stdout>:[2025-10-12 02:55:27] a2ap-dgx007:1177058:1177058 [2] transport/net_ib.cc:587 NCCL WARN NET/IB : Attempted to merge incompatible devices: [7]mlx5_7:1/IB and [8]mlx5_8:1/RoCE. Try selecting NICs of only one link type using NCCL_IB_HCA
[1,0]<stdout>:
[1,0]<stdout>:[2025-10-12 02:55:27] a2ap-dgx007:1177057:1177057 [1] transport/net_ib.cc:587 NCCL WARN NET/IB : Attempted to merge incompatible devices: [1]mlx5_1:1/IB and [2]mlx5_2:1/RoCE. Try selecting NICs of only one link type using NCCL_IB_HCA
[1,0]<stdout>:
[1,0]<stdout>:[2025-10-12 02:55:27] a2ap-dgx007:1177057:1177057 [1] transport/net_ib.cc:587 NCCL WARN NET/IB : Attempted to merge incompatible devices: [7]mlx5_7:1/IB and [8]mlx5_8:1/RoCE. Try selecting NICs of only one link type using NCCL_IB_HCA
[1,0]<stdout>:
[1,0]<stdout>:[2025-10-12 02:55:27] a2ap-dgx007:1177063:1177063 [7] transport/net_ib.cc:587 NCCL WARN NET/IB : Attempted to merge incompatible devices: [1]mlx5_1:1/IB and [2]mlx5_2:1/RoCE. Try selecting NICs of only one link type using NCCL_IB_HCA
[1,0]<stdout>:
[1,0]<stdout>:[2025-10-12 02:55:27] a2ap-dgx007:1177063:1177063 [7] transport/net_ib.cc:587 NCCL WARN NET/IB : Attempted to merge incompatible devices: [7]mlx5_7:1/IB and [8]mlx5_8:1/RoCE. Try selecting NICs of only one link type using NCCL_IB_HCA
[1,0]<stdout>:
[1,0]<stdout>:[2025-10-12 02:55:27] a2ap-dgx007:1177059:1177059 [3] transport/net_ib.cc:587 NCCL WARN NET/IB : Attempted to merge incompatible devices: [1]mlx5_1:1/IB and [2]mlx5_2:1/RoCE. Try selecting NICs of only one link type using NCCL_IB_HCA
[1,0]<stdout>:
[1,0]<stdout>:[2025-10-12 02:55:27] a2ap-dgx007:1177059:1177059 [3] transport/net_ib.cc:587 NCCL WARN NET/IB : Attempted to merge incompatible devices: [7]mlx5_7:1/IB and [8]mlx5_8:1/RoCE. Try selecting NICs of only one link type using NCCL_IB_HCA
[1,0]<stdout>:[2025-10-12 02:55:29 DP0 TP0] Custom allreduce is disabled because this process group spans across nodes.
[1,0]<stdout>:[2025-10-12 02:55:29 DP0 TP1] Custom allreduce is disabled because this process group spans across nodes.
[1,0]<stdout>:[2025-10-12 02:55:29 DP0 TP2] Custom allreduce is disabled because this process group spans across nodes.
[1,1]<stdout>:[2025-10-12 02:55:29 DP1 TP13] Custom allreduce is disabled because this process group spans across nodes.
[1,1]<stdout>:[2025-10-12 02:55:29 DP1 TP14] Custom allreduce is disabled because this process group spans across nodes.
[1,1]<stdout>:[2025-10-12 02:55:29 DP1 TP15] Custom allreduce is disabled because this process group spans across nodes.
[1,1]<stdout>:[2025-10-12 02:55:29 DP1 TP11] Custom allreduce is disabled because this process group spans across nodes.
[1,1]<stdout>:[2025-10-12 02:55:29 DP1 TP12] Custom allreduce is disabled because this process group spans across nodes.
[1,0]<stdout>:[2025-10-12 02:55:29 DP0 TP3] Custom allreduce is disabled because this process group spans across nodes.
[1,1]<stdout>:[2025-10-12 02:55:29 DP1 TP10] Custom allreduce is disabled because this process group spans across nodes.
[1,0]<stdout>:[2025-10-12 02:55:29 DP0 TP4] Custom allreduce is disabled because this process group spans across nodes.
[1,1]<stdout>:[2025-10-12 02:55:29 DP1 TP9] Custom allreduce is disabled because this process group spans across nodes.
[1,0]<stdout>:[2025-10-12 02:55:29 DP0 TP5] Custom allreduce is disabled because this process group spans across nodes.
[1,0]<stdout>:[2025-10-12 02:55:29 DP0 TP6] Custom allreduce is disabled because this process group spans across nodes.
[1,1]<stdout>:[2025-10-12 02:55:29 DP1 TP8] Custom allreduce is disabled because this process group spans across nodes.
[1,0]<stdout>:[2025-10-12 02:55:29 DP0 TP7] Custom allreduce is disabled because this process group spans across nodes.
[1,0]<stdout>:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1,0]<stdout>:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1,0]<stdout>:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1,0]<stdout>:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1,0]<stdout>:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1,0]<stdout>:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1,0]<stdout>:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1,0]<stdout>:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1,1]<stdout>:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1,1]<stdout>:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1,1]<stdout>:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1,1]<stdout>:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1,1]<stdout>:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1,1]<stdout>:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1,1]<stdout>:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1,1]<stdout>:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1,1]<stdout>:[Gloo] Rank 10 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,1]<stdout>:[Gloo] Rank 13 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,0]<stdout>:[Gloo] Rank 0 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,1]<stdout>:[Gloo] Rank 8 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,0]<stdout>:[Gloo] Rank 1 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,0]<stdout>:[Gloo] Rank 2 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,1]<stdout>:[Gloo] Rank 9 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,0]<stdout>:[Gloo] Rank 3 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,0]<stdout>:[Gloo] Rank 4 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,1]<stdout>:[Gloo] Rank 15 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,1]<stdout>:[Gloo] Rank 14 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,0]<stdout>:[2025-10-12 02:55:29 DP0 TP0] sglang is using nccl==2.27.3
[1,0]<stdout>:[Gloo] Rank 5 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,0]<stdout>:[Gloo] Rank 6 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,0]<stdout>:[Gloo] Rank 7 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,1]<stdout>:[Gloo] Rank 11 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,1]<stdout>:[Gloo] Rank 12 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,0]<stdout>:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1,0]<stdout>:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1,0]<stdout>:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1,0]<stdout>:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1,0]<stdout>:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1,1]<stdout>:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1,1]<stdout>:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1,1]<stdout>:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1,0]<stdout>:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1,0]<stdout>:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1,0]<stdout>:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1,1]<stdout>:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1,1]<stdout>:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1,1]<stdout>:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1,1]<stdout>:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1,1]<stdout>:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1,1]<stdout>:[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[1,0]<stdout>:[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[1,0]<stdout>:[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[1,0]<stdout>:[Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[1,0]<stdout>:[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[1,0]<stdout>:[Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[1,0]<stdout>:[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[1,0]<stdout>:[Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[1,0]<stdout>:[Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[1,1]<stdout>:[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[1,1]<stdout>:[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[1,1]<stdout>:[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[1,1]<stdout>:[Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[1,1]<stdout>:[Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[1,1]<stdout>:[Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[1,1]<stdout>:[Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[1,0]<stdout>:[2025-10-12 02:55:32 DP0 TP0] Init torch distributed ends. mem usage=1.75 GB
[1,0]<stdout>:[2025-10-12 02:55:32 DP0 TP5] Ignore import error when loading sglang.srt.models.glm4v_moe: No module named 'transformers.models.glm4v_moe'
[1,1]<stdout>:[2025-10-12 02:55:32 DP1 TP13] Ignore import error when loading sglang.srt.models.glm4v_moe: No module named 'transformers.models.glm4v_moe'
[1,1]<stdout>:[2025-10-12 02:55:32 DP1 TP10] Ignore import error when loading sglang.srt.models.glm4v_moe: No module named 'transformers.models.glm4v_moe'
[1,0]<stdout>:[2025-10-12 02:55:32 DP0 TP3] Ignore import error when loading sglang.srt.models.glm4v_moe: No module named 'transformers.models.glm4v_moe'
[1,0]<stdout>:[2025-10-12 02:55:32 DP0 TP2] Ignore import error when loading sglang.srt.models.glm4v_moe: No module named 'transformers.models.glm4v_moe'
[1,1]<stdout>:[2025-10-12 02:55:32 DP1 TP8] Ignore import error when loading sglang.srt.models.glm4v_moe: No module named 'transformers.models.glm4v_moe'
[1,1]<stdout>:[2025-10-12 02:55:32 DP1 TP14] Ignore import error when loading sglang.srt.models.glm4v_moe: No module named 'transformers.models.glm4v_moe'
[1,1]<stdout>:[2025-10-12 02:55:32 DP1 TP15] Ignore import error when loading sglang.srt.models.glm4v_moe: No module named 'transformers.models.glm4v_moe'
[1,1]<stdout>:[2025-10-12 02:55:32 DP1 TP9] Ignore import error when loading sglang.srt.models.glm4v_moe: No module named 'transformers.models.glm4v_moe'
[1,1]<stdout>:[2025-10-12 02:55:32 DP1 TP11] Ignore import error when loading sglang.srt.models.glm4v_moe: No module named 'transformers.models.glm4v_moe'
[1,1]<stdout>:[2025-10-12 02:55:32 DP1 TP12] Ignore import error when loading sglang.srt.models.glm4v_moe: No module named 'transformers.models.glm4v_moe'
[1,0]<stdout>:[2025-10-12 02:55:32 DP0 TP4] Ignore import error when loading sglang.srt.models.glm4v_moe: No module named 'transformers.models.glm4v_moe'
[1,0]<stdout>:[2025-10-12 02:55:32 DP0 TP6] Ignore import error when loading sglang.srt.models.glm4v_moe: No module named 'transformers.models.glm4v_moe'
[1,0]<stdout>:[2025-10-12 02:55:32 DP0 TP1] Ignore import error when loading sglang.srt.models.glm4v_moe: No module named 'transformers.models.glm4v_moe'
[1,0]<stdout>:[2025-10-12 02:55:33 DP0 TP0] Ignore import error when loading sglang.srt.models.glm4v_moe: No module named 'transformers.models.glm4v_moe'
[1,0]<stdout>:[2025-10-12 02:55:33 DP0 TP7] Ignore import error when loading sglang.srt.models.glm4v_moe: No module named 'transformers.models.glm4v_moe'
[1,0]<stdout>:[2025-10-12 02:55:34 DP0 TP0] Load weight begin. avail mem=76.79 GB
[1,0]<stdout>:[2025-10-12 02:55:34 DP0 TP0] Detected fp8 checkpoint.
[1,0]<stdout>:[2025-10-12 02:55:53 DP0 TP0] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=32.73 GB, mem usage=44.05 GB.
[1,1]<stdout>:[2025-10-12 02:55:56 DP1 TP14] KV Cache is allocated. #tokens: 153719, KV size: 10.06 GB
[1,1]<stdout>:[2025-10-12 02:55:56 DP1 TP12] KV Cache is allocated. #tokens: 153719, KV size: 10.06 GB
[1,1]<stdout>:[2025-10-12 02:55:56 DP1 TP10] KV Cache is allocated. #tokens: 153719, KV size: 10.06 GB
[1,0]<stdout>:[2025-10-12 02:55:56 DP0 TP7] KV Cache is allocated. #tokens: 153719, KV size: 10.06 GB
[1,1]<stdout>:[2025-10-12 02:55:56 DP1 TP15] KV Cache is allocated. #tokens: 153719, KV size: 10.06 GB
[1,1]<stdout>:[2025-10-12 02:55:56 DP1 TP13] KV Cache is allocated. #tokens: 153719, KV size: 10.06 GB
[1,1]<stdout>:[2025-10-12 02:55:56 DP1 TP8] KV Cache is allocated. #tokens: 153719, KV size: 10.06 GB
[1,0]<stdout>:[2025-10-12 02:55:56 DP0 TP5] KV Cache is allocated. #tokens: 153719, KV size: 10.06 GB
[1,0]<stdout>:[2025-10-12 02:55:56 DP0 TP3] KV Cache is allocated. #tokens: 153719, KV size: 10.06 GB
[1,1]<stdout>:[2025-10-12 02:55:56 DP1 TP9] KV Cache is allocated. #tokens: 153719, KV size: 10.06 GB
[1,0]<stdout>:[2025-10-12 02:55:56 DP0 TP2] KV Cache is allocated. #tokens: 153719, KV size: 10.06 GB
[1,0]<stdout>:[2025-10-12 02:55:56 DP0 TP6] KV Cache is allocated. #tokens: 153719, KV size: 10.06 GB
[1,0]<stdout>:[2025-10-12 02:55:56 DP0 TP4] KV Cache is allocated. #tokens: 153719, KV size: 10.06 GB
[1,1]<stdout>:[2025-10-12 02:55:56 DP1 TP11] KV Cache is allocated. #tokens: 153719, KV size: 10.06 GB
[1,0]<stdout>:[2025-10-12 02:55:57 DP0 TP0] KV Cache is allocated. #tokens: 153719, KV size: 10.06 GB
[1,0]<stdout>:[2025-10-12 02:55:57 DP0 TP0] Memory pool end. avail mem=22.25 GB
[1,0]<stdout>:[2025-10-12 02:55:57 DP0 TP1] KV Cache is allocated. #tokens: 153719, KV size: 10.06 GB
[1,0]<stdout>:[2025-10-12 02:55:57 DP0 TP0] Capture cuda graph begin. This can take up to several minutes. avail mem=22.18 GB
[1,0]<stdout>:[2025-10-12 02:55:58 DP0 TP0] Capture cuda graph bs [16, 32]
[1,0]<stdout>:  0% 0/2 [00:00<?, ?it/s][1,0]<stdout>:Capturing batches (bs=32 avail_mem=21.84 GB):   0% 0/2 [00:00<?, ?it/s][1,0]<stdout>:[2025-10-12 02:56:04 DP0 TP4] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=2112, K=7168, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-12 02:56:04 DP0 TP0] Entering DeepGEMM JIT Pre-Compile session. It may takes a long time (typically 10-20 mins) if you have not run `sglang.compile_deep_gemm`. It is recommended to run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to reduce the overhead if you have not run it before. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[1,0]<stdout>:[2025-10-12 02:56:04 DP0 TP0] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=2112, K=7168, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-12 02:56:05 DP1 TP12] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=2112, K=7168, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-12 02:56:05 DP1 TP8] Entering DeepGEMM JIT Pre-Compile session. It may takes a long time (typically 10-20 mins) if you have not run `sglang.compile_deep_gemm`. It is recommended to run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to reduce the overhead if you have not run it before. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[1,1]<stdout>:[2025-10-12 02:56:05 DP1 TP8] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=2112, K=7168, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-12 02:56:05 DP1 TP9] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=2112, K=7168, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-12 02:56:05 DP1 TP15] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=2112, K=7168, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-12 02:56:05 DP1 TP14] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=2112, K=7168, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-12 02:56:05 DP0 TP7] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=2112, K=7168, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-12 02:56:05 DP0 TP1] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=2112, K=7168, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-12 02:56:05 DP0 TP6] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=2112, K=7168, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-12 02:56:05 DP1 TP13] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=2112, K=7168, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-12 02:56:05 DP1 TP11] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=2112, K=7168, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-12 02:56:05 DP0 TP3] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=2112, K=7168, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-12 02:56:05 DP0 TP5] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=2112, K=7168, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-12 02:56:05 DP1 TP10] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=2112, K=7168, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-12 02:56:05 DP0 TP2] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=2112, K=7168, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:  0% 0/33 [00:00<?, ?it/s][1,0]<stdout>:100% 33/33 [00:00<00:00, 4647.04it/s]
[1,0]<stdout>:
[1,0]<stdout>:  0% 0/33 [00:00<?, ?it/s][A[1,0]<stdout>:100% 33/33 [00:00<00:00, 5305.38it/s]
[1,1]<stdout>:  0% 0/33 [00:00<?, ?it/s][1,1]<stdout>:100% 33/33 [00:00<00:00, 4325.51it/s]
[1,1]<stdout>:  0% 0/33 [00:00<?, ?it/s][1,1]<stdout>:100% 33/33 [00:00<00:00, 5127.51it/s]
[1,0]<stdout>:  0% 0/33 [00:00<?, ?it/s][1,0]<stdout>:100% 33/33 [00:00<00:00, 5139.70it/s]
[1,1]<stdout>:  0% 0/33 [00:00<?, ?it/s][1,1]<stdout>:  0% 0/33 [00:00<?, ?it/s][1,1]<stdout>:100% 33/33 [00:00<00:00, 5231.98it/s]
[1,1]<stdout>:100% 33/33 [00:00<00:00, 5201.70it/s]
[1,1]<stdout>:  0% 0/33 [00:00<?, ?it/s][1,0]<stdout>:  0% 0/33 [00:00<?, ?it/s][1,0]<stdout>:  0% 0/33 [00:00<?, ?it/s][1,1]<stdout>:100% 33/33 [00:00<00:00, 5475.38it/s]
[1,0]<stdout>:100% 33/33 [00:00<00:00, 5248.05it/s]
[1,1]<stdout>:  0% 0/33 [00:00<?, ?it/s][1,0]<stdout>:100% 33/33 [00:00<00:00, 5431.12it/s]
[1,1]<stdout>:100% 33/33 [00:00<00:00, 5258.82it/s]
[1,1]<stdout>:  0% 0/33 [00:00<?, ?it/s][1,1]<stdout>:100% 33/33 [00:00<00:00, 5606.68it/s]
[1,1]<stdout>:  0% 0/33 [00:00<?, ?it/s][1,0]<stdout>:  0% 0/33 [00:00<?, ?it/s][1,1]<stdout>:100% 33/33 [00:00<00:00, 4918.34it/s]
[1,0]<stdout>:100% 33/33 [00:00<00:00, 5136.26it/s]
[1,0]<stdout>:  0% 0/33 [00:00<?, ?it/s][1,0]<stdout>:100% 33/33 [00:00<00:00, 4777.28it/s]
[1,0]<stdout>:  0% 0/33 [00:00<?, ?it/s][1,0]<stdout>:100% 33/33 [00:00<00:00, 5002.24it/s]
[1,1]<stdout>:[2025-10-12 02:56:08 DP1 TP12] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=3072, K=1536, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-12 02:56:08 DP0 TP0] Entering DeepGEMM JIT Pre-Compile session. It may takes a long time (typically 10-20 mins) if you have not run `sglang.compile_deep_gemm`. It is recommended to run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to reduce the overhead if you have not run it before. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[1,0]<stdout>:[2025-10-12 02:56:08 DP0 TP0] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=3072, K=1536, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-12 02:56:08 DP0 TP6] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=3072, K=1536, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-12 02:56:08 DP1 TP15] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=3072, K=1536, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-12 02:56:08 DP1 TP9] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=3072, K=1536, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-12 02:56:08 DP1 TP14] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=3072, K=1536, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-12 02:56:08 DP0 TP1] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=3072, K=1536, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-12 02:56:08 DP1 TP11] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=3072, K=1536, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-12 02:56:08 DP0 TP4] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=3072, K=1536, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-12 02:56:08 DP1 TP13] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=3072, K=1536, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-12 02:56:08 DP1 TP8] Entering DeepGEMM JIT Pre-Compile session. It may takes a long time (typically 10-20 mins) if you have not run `sglang.compile_deep_gemm`. It is recommended to run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to reduce the overhead if you have not run it before. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[1,1]<stdout>:[2025-10-12 02:56:08 DP1 TP8] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=3072, K=1536, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-12 02:56:08 DP0 TP5] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=3072, K=1536, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-12 02:56:08 DP0 TP7] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=3072, K=1536, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-12 02:56:08 DP0 TP3] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=3072, K=1536, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-12 02:56:08 DP1 TP10] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=3072, K=1536, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-12 02:56:08 DP0 TP2] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=3072, K=1536, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:  0% 0/29 [00:00<?, ?it/s][1,1]<stdout>:100% 29/29 [00:00<00:00, 12186.64it/s]
[1,1]<stdout>:  0% 0/29 [00:00<?, ?it/s][1,1]<stdout>:100% 29/29 [00:00<00:00, 13162.52it/s]
[1,1]<stdout>:  0% 0/29 [00:00<?, ?it/s][1,1]<stdout>:100% 29/29 [00:00<00:00, 10435.38it/s]
[1,0]<stdout>:  0% 0/29 [00:00<?, ?it/s][1,0]<stdout>:100% 29/29 [00:00<00:00, 12316.20it/s]
[1,0]<stdout>:
[1,0]<stdout>:  0% 0/29 [00:00<?, ?it/s][1,0]<stdout>:[A[1,0]<stdout>:100% 29/29 [00:00<00:00, 13398.86it/s]
[1,1]<stdout>:  0% 0/29 [00:00<?, ?it/s][1,1]<stdout>:100% 29/29 [00:00<00:00, 13513.48it/s]
[1,0]<stdout>:  0% 0/29 [00:00<?, ?it/s][1,0]<stdout>:100% 29/29 [00:00<00:00, 14917.20it/s]
[1,0]<stdout>:  0% 0/29 [00:00<?, ?it/s][1,1]<stdout>:  0% 0/29 [00:00<?, ?it/s][1,0]<stdout>:100% 29/29 [00:00<00:00, 16513.01it/s]
[1,1]<stdout>:100% 29/29 [00:00<00:00, 11857.56it/s]
[1,0]<stdout>:  0% 0/29 [00:00<?, ?it/s][1,1]<stdout>:  0% 0/29 [00:00<?, ?it/s][1,0]<stdout>:100% 29/29 [00:00<00:00, 12979.92it/s]
[1,1]<stdout>:100% 29/29 [00:00<00:00, 12515.16it/s]
[1,1]<stdout>:  0% 0/29 [00:00<?, ?it/s][1,1]<stdout>:100% 29/29 [00:00<00:00, 13822.14it/s]
[1,0]<stdout>:  0% 0/29 [00:00<?, ?it/s][1,0]<stdout>:100% 29/29 [00:00<00:00, 12792.89it/s]
[1,0]<stdout>:  0% 0/29 [00:00<?, ?it/s][1,0]<stdout>:100% 29/29 [00:00<00:00, 13412.15it/s]
[1,1]<stdout>:  0% 0/29 [00:00<?, ?it/s][1,1]<stdout>:100% 29/29 [00:00<00:00, 14964.91it/s]
[1,0]<stdout>:  0% 0/29 [00:00<?, ?it/s][1,0]<stdout>:100% 29/29 [00:00<00:00, 13476.05it/s]
[1,1]<stdout>:AUTOTUNE bmm(16x64x128, 16x128x512)
[1,1]<stdout>:strides: [192, 3072, 1], [65536, 1, 128]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  triton_bmm_13 0.0075 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_7 0.0076 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,1]<stdout>:  triton_bmm_14 0.0076 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,1]<stdout>:  triton_bmm_11 0.0077 ms 97.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_1 0.0077 ms 97.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,1]<stdout>:  triton_bmm_3 0.0078 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_bmm_9 0.0079 ms 95.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  bmm 0.0079 ms 94.8% 
[1,1]<stdout>:  triton_bmm_16 0.0079 ms 94.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_bmm_10 0.0080 ms 94.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.4063 seconds and 1.3705 seconds precompiling for 18 choices
[1,1]<stdout>:AUTOTUNE bmm(16x64x128, 16x128x512)
[1,1]<stdout>:strides: [192, 3072, 1], [65536, 1, 128]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  triton_bmm_7 0.0077 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,1]<stdout>:  triton_bmm_13 0.0077 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_11 0.0078 ms 98.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  bmm 0.0080 ms 96.4% 
[1,1]<stdout>:  triton_bmm_3 0.0080 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_bmm_1 0.0080 ms 96.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,1]<stdout>:  triton_bmm_14 0.0081 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,1]<stdout>:  triton_bmm_9 0.0081 ms 94.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_10 0.0083 ms 92.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,1]<stdout>:  triton_bmm_16 0.0083 ms 92.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.4292 seconds and 1.4169 seconds precompiling for 18 choices
[1,1]<stdout>:AUTOTUNE bmm(16x64x128, 16x128x512)
[1,1]<stdout>:strides: [192, 3072, 1], [65536, 1, 128]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  triton_bmm_7 0.0076 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,1]<stdout>:  triton_bmm_13 0.0077 ms 99.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_11 0.0078 ms 98.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  bmm 0.0080 ms 96.0% 
[1,1]<stdout>:  triton_bmm_2 0.0080 ms 95.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_bmm_3 0.0080 ms 95.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_bmm_9 0.0080 ms 95.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_14 0.0080 ms 95.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,1]<stdout>:  triton_bmm_1 0.0081 ms 94.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,1]<stdout>:  triton_bmm_10 0.0082 ms 93.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.3676 seconds and 1.4941 seconds precompiling for 18 choices
[1,0]<stdout>:AUTOTUNE bmm(16x64x128, 16x128x512)
[1,0]<stdout>:strides: [192, 3072, 1], [65536, 1, 128]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  triton_bmm_7 0.0075 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,0]<stdout>:  triton_bmm_13 0.0076 ms 98.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_bmm_11 0.0077 ms 96.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_bmm_1 0.0078 ms 95.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,0]<stdout>:  bmm 0.0079 ms 94.7% 
[1,0]<stdout>:  triton_bmm_14 0.0079 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,0]<stdout>:  triton_bmm_3 0.0079 ms 94.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_bmm_9 0.0080 ms 94.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_bmm_10 0.0081 ms 92.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,0]<stdout>:  triton_bmm_16 0.0082 ms 91.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.3674 seconds and 1.5865 seconds precompiling for 18 choices
[1,1]<stdout>:AUTOTUNE bmm(16x64x128, 16x128x512)
[1,1]<stdout>:strides: [192, 3072, 1], [65536, 1, 128]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  triton_bmm_7 0.0076 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,1]<stdout>:  triton_bmm_13 0.0077 ms 98.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_14 0.0077 ms 97.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,1]<stdout>:  triton_bmm_11 0.0078 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_1 0.0079 ms 95.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,1]<stdout>:  triton_bmm_3 0.0079 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  bmm 0.0081 ms 93.7% 
[1,1]<stdout>:  triton_bmm_9 0.0081 ms 93.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_2 0.0082 ms 91.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_bmm_10 0.0082 ms 91.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.3851 seconds and 1.4743 seconds precompiling for 18 choices
[1,1]<stdout>:AUTOTUNE bmm(16x64x128, 16x128x512)
[1,1]<stdout>:strides: [192, 3072, 1], [65536, 1, 128]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  triton_bmm_7 0.0074 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,1]<stdout>:  triton_bmm_13 0.0075 ms 98.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_11 0.0076 ms 96.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_14 0.0077 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,1]<stdout>:  triton_bmm_1 0.0078 ms 95.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,1]<stdout>:  triton_bmm_3 0.0078 ms 94.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  bmm 0.0079 ms 93.5% 
[1,1]<stdout>:  triton_bmm_9 0.0079 ms 93.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_16 0.0080 ms 92.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_bmm_10 0.0081 ms 91.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.3852 seconds and 1.4134 seconds precompiling for 18 choices
[1,1]<stdout>:AUTOTUNE bmm(16x64x128, 16x128x512)
[1,1]<stdout>:strides: [192, 3072, 1], [65536, 1, 128]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  triton_bmm_7 0.0074 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,1]<stdout>:  triton_bmm_13 0.0076 ms 97.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_11 0.0077 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_14 0.0077 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,1]<stdout>:  triton_bmm_1 0.0078 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,1]<stdout>:  bmm 0.0079 ms 93.9% 
[1,1]<stdout>:  triton_bmm_9 0.0079 ms 93.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_3 0.0079 ms 93.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_bmm_16 0.0080 ms 92.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_bmm_10 0.0082 ms 90.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.3858 seconds and 1.3455 seconds precompiling for 18 choices
[1,1]<stdout>:AUTOTUNE bmm(16x64x128, 16x128x512)
[1,1]<stdout>:strides: [192, 3072, 1], [65536, 1, 128]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  triton_bmm_7 0.0075 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,1]<stdout>:  triton_bmm_13 0.0076 ms 98.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_11 0.0078 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_1 0.0078 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,1]<stdout>:  bmm 0.0079 ms 95.1% 
[1,1]<stdout>:  triton_bmm_3 0.0079 ms 94.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_bmm_14 0.0080 ms 93.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,1]<stdout>:  triton_bmm_9 0.0081 ms 92.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_2 0.0081 ms 92.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_bmm_10 0.0083 ms 90.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.3857 seconds and 1.4834 seconds precompiling for 18 choices
[1,1]<stdout>:AUTOTUNE bmm(16x64x128, 16x128x512)
[1,1]<stdout>:strides: [192, 3072, 1], [65536, 1, 128]
[1,1]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,1]<stdout>:  triton_bmm_7 0.0075 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,1]<stdout>:  triton_bmm_13 0.0076 ms 98.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_11 0.0076 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_1 0.0078 ms 95.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,1]<stdout>:  bmm 0.0078 ms 95.1% 
[1,1]<stdout>:  triton_bmm_14 0.0079 ms 94.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,1]<stdout>:  triton_bmm_3 0.0080 ms 93.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:  triton_bmm_9 0.0080 ms 93.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,1]<stdout>:  triton_bmm_10 0.0081 ms 92.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,1]<stdout>:  triton_bmm_16 0.0082 ms 91.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,1]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.4045 seconds and 1.5122 seconds precompiling for 18 choices
[1,0]<stdout>:AUTOTUNE bmm(16x64x128, 16x128x512)
[1,0]<stdout>:strides: [192, 3072, 1], [65536, 1, 128]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  triton_bmm_7 0.0076 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,0]<stdout>:  triton_bmm_13 0.0077 ms 98.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_bmm_11 0.0078 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_bmm_3 0.0079 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  bmm 0.0079 ms 95.2% 
[1,0]<stdout>:  triton_bmm_1 0.0080 ms 94.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,0]<stdout>:  triton_bmm_9 0.0081 ms 93.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_bmm_14 0.0081 ms 93.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,0]<stdout>:  triton_bmm_10 0.0083 ms 91.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,0]<stdout>:  triton_bmm_16 0.0083 ms 91.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.3538 seconds and 1.6934 seconds precompiling for 18 choices
[1,0]<stdout>:AUTOTUNE bmm(16x64x128, 16x128x512)
[1,0]<stdout>:strides: [192, 3072, 1], [65536, 1, 128]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  triton_bmm_7 0.0074 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,0]<stdout>:  triton_bmm_13 0.0075 ms 98.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_bmm_11 0.0077 ms 95.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_bmm_14 0.0077 ms 95.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,0]<stdout>:  triton_bmm_1 0.0078 ms 94.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,0]<stdout>:  triton_bmm_9 0.0078 ms 94.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  bmm 0.0078 ms 93.9% 
[1,0]<stdout>:  triton_bmm_3 0.0078 ms 93.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_bmm_10 0.0080 ms 92.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,0]<stdout>:  triton_bmm_16 0.0080 ms 92.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.3785 seconds and 1.7123 seconds precompiling for 18 choices
[1,0]<stdout>:AUTOTUNE bmm(16x64x128, 16x128x512)
[1,0]<stdout>:strides: [192, 3072, 1], [65536, 1, 128]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  triton_bmm_7 0.0075 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,0]<stdout>:  triton_bmm_13 0.0077 ms 97.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_bmm_1 0.0079 ms 95.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,0]<stdout>:  triton_bmm_9 0.0079 ms 95.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_bmm_11 0.0079 ms 95.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_bmm_14 0.0079 ms 95.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,0]<stdout>:  bmm 0.0080 ms 94.4% 
[1,0]<stdout>:  triton_bmm_3 0.0080 ms 94.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_bmm_10 0.0081 ms 92.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,0]<stdout>:  triton_bmm_2 0.0082 ms 92.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.3635 seconds and 1.6621 seconds precompiling for 18 choices
[1,0]<stdout>:AUTOTUNE bmm(16x64x128, 16x128x512)
[1,0]<stdout>:strides: [192, 3072, 1], [65536, 1, 128]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  triton_bmm_7 0.0074 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,0]<stdout>:  triton_bmm_13 0.0077 ms 96.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_bmm_11 0.0077 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_bmm_1 0.0078 ms 95.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,0]<stdout>:  triton_bmm_3 0.0078 ms 95.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_bmm_14 0.0078 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,0]<stdout>:  bmm 0.0079 ms 93.5% 
[1,0]<stdout>:  triton_bmm_9 0.0080 ms 93.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_bmm_10 0.0081 ms 92.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,0]<stdout>:  triton_bmm_16 0.0081 ms 92.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.3519 seconds and 1.3815 seconds precompiling for 18 choices
[1,0]<stdout>:AUTOTUNE bmm(16x64x128, 16x128x512)
[1,0]<stdout>:strides: [192, 3072, 1], [65536, 1, 128]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  triton_bmm_7 0.0074 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,0]<stdout>:  triton_bmm_13 0.0076 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_bmm_11 0.0076 ms 96.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  bmm 0.0077 ms 95.0% 
[1,0]<stdout>:  triton_bmm_1 0.0077 ms 95.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,0]<stdout>:  triton_bmm_3 0.0078 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_bmm_14 0.0079 ms 93.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,0]<stdout>:  triton_bmm_9 0.0080 ms 92.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_bmm_16 0.0082 ms 90.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_bmm_10 0.0082 ms 89.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.3627 seconds and 1.6336 seconds precompiling for 18 choices
[1,0]<stdout>:AUTOTUNE bmm(16x64x128, 16x128x512)
[1,0]<stdout>:strides: [192, 3072, 1], [65536, 1, 128]
[1,0]<stdout>:dtypes: torch.bfloat16, torch.bfloat16
[1,0]<stdout>:  triton_bmm_7 0.0073 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
[1,0]<stdout>:  triton_bmm_13 0.0076 ms 96.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_bmm_11 0.0078 ms 93.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_bmm_14 0.0078 ms 93.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,0]<stdout>:  triton_bmm_1 0.0079 ms 93.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
[1,0]<stdout>:  bmm 0.0079 ms 92.3% 
[1,0]<stdout>:  triton_bmm_3 0.0079 ms 92.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
[1,0]<stdout>:  triton_bmm_9 0.0080 ms 91.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
[1,0]<stdout>:  triton_bmm_10 0.0082 ms 89.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
[1,0]<stdout>:  triton_bmm_12 0.0082 ms 89.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
[1,0]<stdout>:SingleProcess AUTOTUNE benchmarking takes 0.3576 seconds and 1.6238 seconds precompiling for 18 choices
[1,1]<stdout>:[rank8]:W1012 02:56:14.679000 712793 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/_inductor/codecache.py:1026] [11/2_1] fx graph cache unable to load compiled graph
[1,1]<stdout>:[rank8]:W1012 02:56:14.679000 712793 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/_inductor/codecache.py:1026] [11/2_1] Traceback (most recent call last):
[1,1]<stdout>:[rank8]:W1012 02:56:14.679000 712793 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/_inductor/codecache.py:1026] [11/2_1]   File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_inductor/codecache.py", line 1022, in iterate_over_candidates
[1,1]<stdout>:[rank8]:W1012 02:56:14.679000 712793 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/_inductor/codecache.py:1026] [11/2_1]     with open(os.path.join(subdir, path), "rb") as f:
[1,1]<stdout>:[rank8]:W1012 02:56:14.679000 712793 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/_inductor/codecache.py:1026] [11/2_1]          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:[rank8]:W1012 02:56:14.679000 712793 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/_inductor/codecache.py:1026] [11/2_1] FileNotFoundError: [Errno 2] No such file or directory: '/home/users/industry/ai-hpc/apacsc34/scratch/.torchinductor_cache/fxgraph/ya/fyap5aivcukco2iazb3g6ftfqm2kvkysfr6eadxnnq6snrhvirqp/.1177056.140737350502208.tmp'
[1,0]<stdout>:[2025-10-12 02:56:14 DP0 TP4] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=7168, K=2048, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-12 02:56:14 DP0 TP2] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=7168, K=2048, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-12 02:56:14 DP0 TP7] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=7168, K=2048, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-12 02:56:14 DP1 TP12] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=7168, K=2048, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-12 02:56:14 DP0 TP3] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=7168, K=2048, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-12 02:56:14 DP0 TP1] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=7168, K=2048, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-12 02:56:14 DP1 TP11] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=7168, K=2048, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-12 02:56:14 DP1 TP10] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=7168, K=2048, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-12 02:56:14 DP1 TP9] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=7168, K=2048, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-12 02:56:14 DP1 TP15] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=7168, K=2048, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-12 02:56:14 DP0 TP6] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=7168, K=2048, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-12 02:56:14 DP0 TP5] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=7168, K=2048, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-12 02:56:14 DP1 TP14] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=7168, K=2048, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-12 02:56:14 DP1 TP13] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=7168, K=2048, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-12 02:56:14 DP0 TP0] Entering DeepGEMM JIT Pre-Compile session. It may takes a long time (typically 10-20 mins) if you have not run `sglang.compile_deep_gemm`. It is recommended to run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to reduce the overhead if you have not run it before. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[1,0]<stdout>:[2025-10-12 02:56:14 DP0 TP0] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=7168, K=2048, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-12 02:56:14 DP1 TP8] Entering DeepGEMM JIT Pre-Compile session. It may takes a long time (typically 10-20 mins) if you have not run `sglang.compile_deep_gemm`. It is recommended to run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to reduce the overhead if you have not run it before. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[1,1]<stdout>:[2025-10-12 02:56:14 DP1 TP8] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=7168, K=2048, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:  0% 0/16 [00:00<?, ?it/s][1,0]<stdout>:  0% 0/16 [00:00<?, ?it/s][1,0]<stdout>:100% 16/16 [00:00<00:00, 8872.14it/s]
[1,0]<stdout>:100% 16/16 [00:00<00:00, 14187.92it/s]
[1,1]<stdout>:  0% 0/16 [00:00<?, ?it/s][1,1]<stdout>:100% 16/16 [00:00<00:00, 13331.12it/s]
[1,0]<stdout>:  0% 0/16 [00:00<?, ?it/s][1,0]<stdout>:100% 16/16 [00:00<00:00, 12356.63it/s]
[1,0]<stdout>:  0% 0/16 [00:00<?, ?it/s][1,0]<stdout>:100% 16/16 [00:00<00:00, 19048.78it/s]
[1,0]<stdout>:  0% 0/16 [00:00<?, ?it/s][1,0]<stdout>:100% 16/16 [00:00<00:00, 17739.59it/s]
[1,0]<stdout>:  0% 0/16 [00:00<?, ?it/s][1,0]<stdout>:100% 16/16 [00:00<00:00, 18589.71it/s]
[1,1]<stdout>:  0% 0/16 [00:00<?, ?it/s][1,1]<stdout>:100% 16/16 [00:00<00:00, 13071.46it/s]
[1,0]<stdout>:  0% 0/16 [00:00<?, ?it/s][1,0]<stdout>:
[1,0]<stdout>:  0% 0/16 [00:00<?, ?it/s][A[1,0]<stdout>:100% 16/16 [00:00<00:00, 13535.47it/s]
[1,0]<stdout>:100% 16/16 [00:00<00:00, 16908.25it/s]
[1,1]<stdout>:  0% 0/16 [00:00<?, ?it/s][1,1]<stdout>:  0% 0/16 [00:00<?, ?it/s][1,1]<stdout>:100% 16/16 [00:00<00:00, 14976.31it/s]
[1,1]<stdout>:100% 16/16 [00:00<00:00, 17890.93it/s]
[1,1]<stdout>:  0% 0/16 [00:00<?, ?it/s][1,1]<stdout>:100% 16/16 [00:00<00:00, 16933.85it/s]
[1,1]<stdout>:  0% 0/16 [00:00<?, ?it/s][1,1]<stdout>:100% 16/16 [00:00<00:00, 18098.40it/s]
[1,1]<stdout>:  0% 0/16 [00:00<?, ?it/s][1,1]<stdout>:100% 16/16 [00:00<00:00, 15307.68it/s]
[1,1]<stdout>:  0% 0/16 [00:00<?, ?it/s][1,1]<stdout>:100% 16/16 [00:00<00:00, 12829.07it/s]
[1,1]<stdout>:NCCL version 2.27.3+cuda12.9
[1,0]<stdout>:[rank3]:W1012 02:56:24.536000 1177059 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/_inductor/codecache.py:1026] [11/3_1] fx graph cache unable to load compiled graph
[1,0]<stdout>:[rank3]:W1012 02:56:24.536000 1177059 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/_inductor/codecache.py:1026] [11/3_1] Traceback (most recent call last):
[1,0]<stdout>:[rank3]:W1012 02:56:24.536000 1177059 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/_inductor/codecache.py:1026] [11/3_1]   File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_inductor/codecache.py", line 1022, in iterate_over_candidates
[1,0]<stdout>:[rank3]:W1012 02:56:24.536000 1177059 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/_inductor/codecache.py:1026] [11/3_1]     with open(os.path.join(subdir, path), "rb") as f:
[1,0]<stdout>:[rank3]:W1012 02:56:24.536000 1177059 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/_inductor/codecache.py:1026] [11/3_1]          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:[rank3]:W1012 02:56:24.536000 1177059 /scratch/users/industry/ai-hpc/apacsc34/tanathep/py312/lib/python3.12/site-packages/torch/_inductor/codecache.py:1026] [11/3_1] FileNotFoundError: [Errno 2] No such file or directory: '/home/users/industry/ai-hpc/apacsc34/scratch/.torchinductor_cache/fxgraph/ts/fts5usyg2upgdefvsgurq3oxxg3uw6sk7ludmtll7sb3uiqtbnaa/.712796.140737350502208.tmp'
[1,0]<stdout>:[2025-10-12 02:56:24 DP0 TP6] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=2304, K=7168, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-12 02:56:24 DP1 TP14] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=2304, K=7168, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-12 02:56:24 DP0 TP7] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=2304, K=7168, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-12 02:56:24 DP1 TP15] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=2304, K=7168, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-12 02:56:24 DP1 TP11] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=2304, K=7168, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-12 02:56:24 DP1 TP10] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=2304, K=7168, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-12 02:56:24 DP1 TP8] Entering DeepGEMM JIT Pre-Compile session. It may takes a long time (typically 10-20 mins) if you have not run `sglang.compile_deep_gemm`. It is recommended to run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to reduce the overhead if you have not run it before. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[1,1]<stdout>:[2025-10-12 02:56:24 DP1 TP8] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=2304, K=7168, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-12 02:56:24 DP0 TP2] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=2304, K=7168, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-12 02:56:24 DP0 TP1] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=2304, K=7168, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-12 02:56:24 DP0 TP0] Entering DeepGEMM JIT Pre-Compile session. It may takes a long time (typically 10-20 mins) if you have not run `sglang.compile_deep_gemm`. It is recommended to run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to reduce the overhead if you have not run it before. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[1,0]<stdout>:[2025-10-12 02:56:24 DP0 TP0] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=2304, K=7168, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-12 02:56:24 DP1 TP9] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=2304, K=7168, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-12 02:56:24 DP0 TP3] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=2304, K=7168, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-12 02:56:24 DP0 TP5] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=2304, K=7168, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-12 02:56:24 DP1 TP13] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=2304, K=7168, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-12 02:56:24 DP1 TP12] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=2304, K=7168, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-12 02:56:24 DP0 TP4] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=2304, K=7168, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:  0% 0/32 [00:00<?, ?it/s][1,0]<stdout>:100% 32/32 [00:00<00:00, 14402.59it/s]
[1,0]<stdout>:  0% 0/32 [00:00<?, ?it/s][1,0]<stdout>:100% 32/32 [00:00<00:00, 12549.58it/s]
[1,1]<stdout>:  0% 0/32 [00:00<?, ?it/s][1,1]<stdout>:  0% 0/32 [00:00<?, ?it/s][1,1]<stdout>:100% 32/32 [00:00<00:00, 11809.74it/s]
[1,1]<stdout>:100% 32/32 [00:00<00:00, 13093.14it/s]
[1,1]<stdout>:  0% 0/32 [00:00<?, ?it/s][1,1]<stdout>:100% 32/32 [00:00<00:00, 13003.07it/s]
[1,1]<stdout>:  0% 0/32 [00:00<?, ?it/s][1,1]<stdout>:100% 32/32 [00:00<00:00, 14533.59it/s]
[1,1]<stdout>:  0% 0/32 [00:00<?, ?it/s][1,1]<stdout>:100% 32/32 [00:00<00:00, 13209.11it/s]
[1,1]<stdout>:  0% 0/32 [00:00<?, ?it/s][1,1]<stdout>:100% 32/32 [00:00<00:00, 13170.22it/s]
[1,0]<stdout>:
[1,0]<stdout>:  0% 0/32 [00:00<?, ?it/s][A[1,0]<stdout>:100% 32/32 [00:00<00:00, 13704.08it/s]
[1,0]<stdout>:  0% 0/32 [00:00<?, ?it/s][1,0]<stdout>:100% 32/32 [00:00<00:00, 12882.02it/s]
[1,0]<stdout>:  0% 0/32 [00:00<?, ?it/s][1,0]<stdout>:  0% 0/32 [00:00<?, ?it/s][1,0]<stdout>:100% 32/32 [00:00<00:00, 12210.49it/s]
[1,1]<stdout>:  0% 0/32 [00:00<?, ?it/s][1,0]<stdout>:100% 32/32 [00:00<00:00, 12362.32it/s]
[1,1]<stdout>:100% 32/32 [00:00<00:00, 13736.33it/s]
[1,0]<stdout>:  0% 0/32 [00:00<?, ?it/s][1,0]<stdout>:100% 32/32 [00:00<00:00, 13228.63it/s]
[1,1]<stdout>:  0% 0/32 [00:00<?, ?it/s][1,1]<stdout>:100% 32/32 [00:00<00:00, 13262.62it/s]
[1,0]<stdout>:  0% 0/32 [00:00<?, ?it/s][1,0]<stdout>:100% 32/32 [00:00<00:00, 11123.63it/s]
[1,1]<stdout>:[2025-10-12 02:56:26 DP1 TP10] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=7168, K=1152, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-12 02:56:26 DP1 TP11] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=7168, K=1152, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-12 02:56:26 DP0 TP7] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=7168, K=1152, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-12 02:56:26 DP0 TP3] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=7168, K=1152, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-12 02:56:26 DP1 TP15] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=7168, K=1152, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-12 02:56:26 DP1 TP13] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=7168, K=1152, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-12 02:56:26 DP1 TP9] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=7168, K=1152, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-12 02:56:26 DP0 TP5] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=7168, K=1152, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-12 02:56:26 DP1 TP12] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=7168, K=1152, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-12 02:56:26 DP0 TP2] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=7168, K=1152, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-12 02:56:26 DP0 TP6] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=7168, K=1152, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-12 02:56:26 DP1 TP14] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=7168, K=1152, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-12 02:56:26 DP0 TP1] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=7168, K=1152, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-12 02:56:26 DP0 TP4] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=7168, K=1152, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-12 02:56:26 DP1 TP8] Entering DeepGEMM JIT Pre-Compile session. It may takes a long time (typically 10-20 mins) if you have not run `sglang.compile_deep_gemm`. It is recommended to run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to reduce the overhead if you have not run it before. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[1,1]<stdout>:[2025-10-12 02:56:26 DP1 TP8] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=7168, K=1152, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-12 02:56:26 DP0 TP0] Entering DeepGEMM JIT Pre-Compile session. It may takes a long time (typically 10-20 mins) if you have not run `sglang.compile_deep_gemm`. It is recommended to run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to reduce the overhead if you have not run it before. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[1,0]<stdout>:[2025-10-12 02:56:26 DP0 TP0] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=7168, K=1152, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:  0% 0/16 [00:00<?, ?it/s][1,0]<stdout>:100% 16/16 [00:00<00:00, 12427.57it/s]
[1,1]<stdout>:  0% 0/16 [00:00<?, ?it/s][1,1]<stdout>:100% 16/16 [00:00<00:00, 11171.78it/s]
[1,1]<stdout>:  0% 0/16 [00:00<?, ?it/s][1,1]<stdout>:100% 16/16 [00:00<00:00, 12128.84it/s]
[1,1]<stdout>:  0% 0/16 [00:00<?, ?it/s][1,1]<stdout>:100% 16/16 [00:00<00:00, 12409.18it/s]
[1,0]<stdout>:  0% 0/16 [00:00<?, ?it/s][1,0]<stdout>:100% 16/16 [00:00<00:00, 16810.84it/s]
[1,1]<stdout>:  0% 0/16 [00:00<?, ?it/s][1,1]<stdout>:100% 16/16 [00:00<00:00, 13381.63it/s][1,1]<stdout>:
[1,1]<stdout>:  0% 0/16 [00:00<?, ?it/s][1,1]<stdout>:100% 16/16 [00:00<00:00, 11499.12it/s][1,1]<stdout>:
[1,0]<stdout>:  0% 0/16 [00:00<?, ?it/s][1,1]<stdout>:  0% 0/16 [00:00<?, ?it/s][1,0]<stdout>:100% 16/16 [00:00<00:00, 12139.81it/s]
[1,1]<stdout>:100% 16/16 [00:00<00:00, 15993.53it/s]
[1,0]<stdout>:  0% 0/16 [00:00<?, ?it/s][1,0]<stdout>:100% 16/16 [00:00<00:00, 16151.35it/s]
[1,0]<stdout>:  0% 0/16 [00:00<?, ?it/s][1,0]<stdout>:100% 16/16 [00:00<00:00, 18909.23it/s][1,0]<stdout>:
[1,0]<stdout>:  0% 0/16 [00:00<?, ?it/s][1,0]<stdout>:100% 16/16 [00:00<00:00, 16031.74it/s]
[1,1]<stdout>:  0% 0/16 [00:00<?, ?it/s][1,1]<stdout>:100% 16/16 [00:00<00:00, 15238.16it/s][1,1]<stdout>:
[1,1]<stdout>:  0% 0/16 [00:00<?, ?it/s][1,1]<stdout>:100% 16/16 [00:00<00:00, 13637.24it/s]
[1,0]<stdout>:  0% 0/16 [00:00<?, ?it/s][1,0]<stdout>:100% 16/16 [00:00<00:00, 12448.31it/s]
[1,0]<stdout>:
[1,0]<stdout>:  0% 0/16 [00:00<?, ?it/s][1,0]<stdout>:[A[1,0]<stdout>:100% 16/16 [00:00<00:00, 13908.57it/s]
[1,1]<stdout>:[2025-10-12 02:56:35 DP1 TP14] Scheduler hit an exception: Traceback (most recent call last):
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 380, in __init__
[1,1]<stdout>:    self.capture()
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 485, in capture
[1,1]<stdout>:    ) = self.capture_one_batch_size(bs, forward)
[1,1]<stdout>:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 649, in capture_one_batch_size
[1,1]<stdout>:    run_once()
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 637, in run_once
[1,1]<stdout>:    logits_output_or_pp_proxy_tensors = forward(
[1,1]<stdout>:                                        ^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 736, in compile_wrapper
[1,1]<stdout>:    return fn(*args, **kwargs)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/external_utils.py", line 70, in inner
[1,1]<stdout>:    return fn(*args, **kwargs)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1,1]<stdout>:    return func(*args, **kwargs)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1,1]<stdout>:    return func(*args, **kwargs)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/models/deepseek_v2.py", line 2221, in forward
[1,1]<stdout>:    hidden_states = self.model(input_ids, positions, forward_batch, input_embeds)
[1,1]<stdout>:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
[1,1]<stdout>:    return self._call_impl(*args, **kwargs)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
[1,1]<stdout>:    return forward_call(*args, **kwargs)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/models/deepseek_v2.py", line 2109, in forward
[1,1]<stdout>:    enable_tbo=True,
[1,1]<stdout>:                     
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/two_batch_overlap.py", line 800, in model_forward_maybe_tbo
[1,1]<stdout>:    return _model_forward_tbo(
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 1495, in __call__
[1,1]<stdout>:    return self._torchdynamo_orig_callable(
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 1272, in __call__
[1,1]<stdout>:    result = self._inner_convert(
[1,1]<stdout>:             ^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 629, in __call__
[1,1]<stdout>:    return _compile(
[1,1]<stdout>:           ^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 1164, in _compile
[1,1]<stdout>:    raise InternalTorchDynamoError(
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 1111, in[1,1]<stdout>: _compile
[1,1]<stdout>:    guarded_code = compile_inner(code, one_graph, hooks, transform)
[1,1]<stdout>:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_utils_internal.py", line 97, in wrapper_function
[1,1]<stdout>:    return function(*args, **kwargs)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 793, in compile_inner
[1,1]<stdout>:    return _compile_inner(code, one_graph, hooks, transform)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 832, in _compile_inner
[1,1]<stdout>:    out_code = transform_code_object(code, transform)
[1,1]<stdout>:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/bytecode_transformation.py", line 1424, in transform_code_object
[1,1]<stdout>:    transformations(instructions, code_options)
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 267, in _fn
[1,1]<stdout>:    return fn(*args, **kwargs)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 753, in transform
[1,1]<stdout>:    tracer.run()
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py", line 3497, in run
[1,1]<stdout>:    super().run()
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py", line 1363, in run
[1,1]<stdout>:    while self.step():
[1,1]<stdout>:          ^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py", line 1257, in step
[1,1]<stdout>:    return self.step_graph_break(inst)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py", line 1342, in step_graph_break
[1,1]<stdout>:    self.output.compile_subgraph(
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 1399, in compile_subgraph
[1,1]<stdout>:    self.codegen_suffix(tx, stack_values_flat, pass1)
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 1477, in codegen_suffix
[1,1]<stdout>:    self.side_effects.codegen_update_mutated(cg)
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/side_effects.py", line 1065, in codegen_update_mutated
[1,1]<stdout>:    cg(value)
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/codegen.py", line 235, in __call__
[1,1]<stdout>:    raise IncorrectUsage(
[1,1]<stdout>:torch._dynamo.exc.InternalTorchDynamoError: IncorrectUsage: NYI: Returning a @contextmanager object from a torch.compile function
[1,1]<stdout>:
[1,1]<stdout>:from user code:
[1,1]<stdout>:   File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/two_batch_overlap.py", line 831, in _model_forward_tbo
[1,1]<stdout>:    with context:
[1,1]<stdout>:
[1,1]<stdout>:Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1,1]<stdout>:
[1,1]<stdout>:
[1,1]<stdout>:During handling of the above exception, another exception occurred:
[1,1]<stdout>:
[1,1]<stdout>:Traceback (most recent call last):
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2555, in run_scheduler_process
[1,1]<stdout>:    scheduler = Scheduler(
[1,1]<stdout>:                ^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 315, in __init__
[1,1]<stdout>:    self.tp_worke[1,1]<stdout>:r = TpWorkerClass(
[1,1]<stdout>:                     ^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 84, in __init__
[1,1]<stdout>:    self.model_runner = ModelRunner(
[1,1]<stdout>:                        ^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 242, in __init__
[1,1]<stdout>:    self.initialize(min_per_gpu_memory)
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 342, in initialize
[1,1]<stdout>:    self.init_cuda_graphs()
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 1611, in init_cuda_graphs
[1,1]<stdout>:    self.cuda_graph_runner = CudaGraphRunner(self)
[1,1]<stdout>:                             ^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 382, in __init__
[1,1]<stdout>:    raise Exception(
[1,1]<stdout>:Exception: Capture cuda graph failed: IncorrectUsage: NYI: Returning a @contextmanager object from a torch.compile function
[1,1]<stdout>:
[1,1]<stdout>:from user code:
[1,1]<stdout>:   File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/two_batch_overlap.py", line 831, in _model_forward_tbo
[1,1]<stdout>:    with context:
[1,1]<stdout>:
[1,1]<stdout>:Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1,1]<stdout>:
[1,1]<stdout>:Possible solutions:
[1,1]<stdout>:1. set --mem-fraction-static to a smaller value (e.g., 0.8 or 0.7)
[1,1]<stdout>:2. set --cuda-graph-max-bs to a smaller value (e.g., 16)
[1,1]<stdout>:3. disable torch compile by not using --enable-torch-compile
[1,1]<stdout>:4. disable CUDA graph by --disable-cuda-graph. (Not recommended. Huge performance loss)
[1,1]<stdout>:Open an issue on GitHub https://github.com/sgl-project/sglang/issues/new/choose 
[1,1]<stdout>:
[1,1]<stdout>:
[1,0]<stdout>:[2025-10-12 02:56:35 DP0 TP6] Scheduler hit an exception: Traceback (most recent call last):
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 380, in __init__
[1,0]<stdout>:    self.capture()
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 485, in capture
[1,0]<stdout>:    ) = self.capture_one_batch_size(bs, forward)
[1,0]<stdout>:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 649, in capture_one_batch_size
[1,0]<stdout>:    run_once()
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 637, in run_once
[1,0]<stdout>:    logits_output_or_pp_proxy_tensors = forward(
[1,0]<stdout>:                                        ^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 736, in compile_wrapper
[1,0]<stdout>:    return fn(*args, **kwargs)
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/external_utils.py", line 70, in inner
[1,0]<stdout>:    return fn(*args, **kwargs)
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1,0]<stdout>:    return func(*args, **kwargs)
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1,0]<stdout>:    return func(*args, **kwargs)
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/models/deepseek_v2.py", line 2221, in forward
[1,0]<stdout>:    hidden_states = self.model(input_ids, positions, forward_batch, input_embeds)
[1,0]<stdout>:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
[1,0]<stdout>:    return self._call_impl(*args, **kwargs)
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
[1,0]<stdout>:    return forward_call(*args, **kwargs)
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/models/deepseek_v2.py", line 2109, in forward
[1,0]<stdout>:    enable_tbo=True,
[1,0]<stdout>:                     
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/two_batch_overlap.py", line 800, in model_forward_maybe_tbo
[1,0]<stdout>:    return _model_forward_tbo(
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 1495, in __call__
[1,0]<stdout>:    return self._torchdynamo_orig_callable(
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 1272, in __call__
[1,0]<stdout>:    result = self._inner_convert(
[1,0]<stdout>:             ^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 629, in __call__
[1,0]<stdout>:    return _compile(
[1,0]<stdout>:           ^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 1164, in _compile
[1,0]<stdout>:    raise InternalTorchDynamoError(
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 1111, in [1,0]<stdout>:_compile
[1,0]<stdout>:    guarded_code = compile_inner(code, one_graph, hooks, transform)
[1,0]<stdout>:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_utils_internal.py", line 97, in wrapper_function
[1,0]<stdout>:    return function(*args, **kwargs)
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 793, in compile_inner
[1,0]<stdout>:    return _compile_inner(code, one_graph, hooks, transform)
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 832, in _compile_inner
[1,0]<stdout>:    out_code = transform_code_object(code, transform)
[1,0]<stdout>:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/bytecode_transformation.py", line 1424, in transform_code_object
[1,0]<stdout>:    transformations(instructions, code_options)
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 267, in _fn
[1,0]<stdout>:    return fn(*args, **kwargs)
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 753, in transform
[1,0]<stdout>:    tracer.run()
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py", line 3497, in run
[1,0]<stdout>:    super().run()
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py", line 1363, in run
[1,0]<stdout>:    while self.step():
[1,0]<stdout>:          ^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py", line 1257, in step
[1,0]<stdout>:    return self.step_graph_break(inst)
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py", line 1342, in step_graph_break
[1,0]<stdout>:    self.output.compile_subgraph(
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 1399, in compile_subgraph
[1,0]<stdout>:    self.codegen_suffix(tx, stack_values_flat, pass1)
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 1477, in codegen_suffix
[1,0]<stdout>:    self.side_effects.codegen_update_mutated(cg)
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/side_effects.py", line 1065, in codegen_update_mutated
[1,0]<stdout>:    cg(value)
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/codegen.py", line 235, in __call__
[1,0]<stdout>:    raise IncorrectUsage(
[1,0]<stdout>:torch._dynamo.exc.InternalTorchDynamoError: IncorrectUsage: NYI: Returning a @contextmanager object from a torch.compile function
[1,0]<stdout>:
[1,0]<stdout>:from user code:
[1,0]<stdout>:   File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/two_batch_overlap.py", line 831, in _model_forward_tbo
[1,0]<stdout>:    with context:
[1,0]<stdout>:
[1,0]<stdout>:Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1,0]<stdout>:
[1,0]<stdout>:
[1,0]<stdout>:During handling of the above exception, another exception occurred:
[1,0]<stdout>:
[1,0]<stdout>:Traceback (most recent call last):
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2555, in run_scheduler_process
[1,0]<stdout>:    scheduler = Scheduler(
[1,0]<stdout>:                ^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 315, in __init__
[1,0]<stdout>:    self.tp_worker[1,0]<stdout>: = TpWorkerClass(
[1,0]<stdout>:                     ^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 84, in __init__
[1,0]<stdout>:    self.model_runner = ModelRunner(
[1,0]<stdout>:                        ^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 242, in __init__
[1,0]<stdout>:    self.initialize(min_per_gpu_memory)
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 342, in initialize
[1,0]<stdout>:    self.init_cuda_graphs()
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 1611, in init_cuda_graphs
[1,0]<stdout>:    self.cuda_graph_runner = CudaGraphRunner(self)
[1,0]<stdout>:                             ^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 382, in __init__
[1,0]<stdout>:    raise Exception(
[1,0]<stdout>:Exception: Capture cuda graph failed: IncorrectUsage: NYI: Returning a @contextmanager object from a torch.compile function
[1,0]<stdout>:
[1,0]<stdout>:from user code:
[1,0]<stdout>:   File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/two_batch_overlap.py", line 831, in _model_forward_tbo
[1,0]<stdout>:    with context:
[1,0]<stdout>:
[1,0]<stdout>:Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1,0]<stdout>:
[1,0]<stdout>:Possible solutions:
[1,0]<stdout>:1. set --mem-fraction-static to a smaller value (e.g., 0.8 or 0.7)
[1,0]<stdout>:2. set --cuda-graph-max-bs to a smaller value (e.g., 16)
[1,0]<stdout>:3. disable torch compile by not using --enable-torch-compile
[1,0]<stdout>:4. disable CUDA graph by --disable-cuda-graph. (Not recommended. Huge performance loss)
[1,0]<stdout>:Open an issue on GitHub https://github.com/sgl-project/sglang/issues/new/choose 
[1,0]<stdout>:
[1,0]<stdout>:
[1,1]<stdout>:[2025-10-12 02:56:35 DP1 TP15] Scheduler hit an exception: Traceback (most recent call last):
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 380, in __init__
[1,1]<stdout>:    self.capture()
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 485, in capture
[1,1]<stdout>:    ) = self.capture_one_batch_size(bs, forward)
[1,1]<stdout>:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 649, in capture_one_batch_size
[1,1]<stdout>:    run_once()
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 637, in run_once
[1,1]<stdout>:    logits_output_or_pp_proxy_tensors = forward(
[1,1]<stdout>:                                        ^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 736, in compile_wrapper
[1,1]<stdout>:    return fn(*args, **kwargs)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/external_utils.py", line 70, in inner
[1,1]<stdout>:    return fn(*args, **kwargs)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1,1]<stdout>:    return func(*args, **kwargs)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1,1]<stdout>:    return func(*args, **kwargs)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/models/deepseek_v2.py", line 2221, in forward
[1,1]<stdout>:    hidden_states = self.model(input_ids, positions, forward_batch, input_embeds)
[1,1]<stdout>:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
[1,1]<stdout>:    return self._call_impl(*args, **kwargs)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
[1,1]<stdout>:    return forward_call(*args, **kwargs)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/models/deepseek_v2.py", line 2109, in forward
[1,1]<stdout>:    enable_tbo=True,
[1,1]<stdout>:                     
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/two_batch_overlap.py", line 800, in model_forward_maybe_tbo
[1,1]<stdout>:    return _model_forward_tbo(
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 1495, in __call__
[1,1]<stdout>:    return self._torchdynamo_orig_callable(
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 1272, in __call__
[1,1]<stdout>:    result = self._inner_convert(
[1,1]<stdout>:             ^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 629, in __call__
[1,1]<stdout>:    return _compile(
[1,1]<stdout>:           ^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 1164, in _compile
[1,1]<stdout>:    raise InternalTorchDynamoError(
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 1111, in[1,1]<stdout>: _compile
[1,1]<stdout>:    guarded_code = compile_inner(code, one_graph, hooks, transform)
[1,1]<stdout>:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_utils_internal.py", line 97, in wrapper_function
[1,1]<stdout>:    return function(*args, **kwargs)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 793, in compile_inner
[1,1]<stdout>:    return _compile_inner(code, one_graph, hooks, transform)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 832, in _compile_inner
[1,1]<stdout>:    out_code = transform_code_object(code, transform)
[1,1]<stdout>:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/bytecode_transformation.py", line 1424, in transform_code_object
[1,1]<stdout>:    transformations(instructions, code_options)
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 267, in _fn
[1,1]<stdout>:    return fn(*args, **kwargs)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 753, in transform
[1,1]<stdout>:    tracer.run()
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py", line 3497, in run
[1,1]<stdout>:    super().run()
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py", line 1363, in run
[1,1]<stdout>:    while self.step():
[1,1]<stdout>:          ^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py", line 1257, in step
[1,1]<stdout>:    return self.step_graph_break(inst)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py", line 1342, in step_graph_break
[1,1]<stdout>:    self.output.compile_subgraph(
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 1399, in compile_subgraph
[1,1]<stdout>:    self.codegen_suffix(tx, stack_values_flat, pass1)
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 1477, in codegen_suffix
[1,1]<stdout>:    self.side_effects.codegen_update_mutated(cg)
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/side_effects.py", line 1065, in codegen_update_mutated
[1,1]<stdout>:    cg(value)
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/codegen.py", line 235, in __call__
[1,1]<stdout>:    raise IncorrectUsage(
[1,1]<stdout>:torch._dynamo.exc.InternalTorchDynamoError: IncorrectUsage: NYI: Returning a @contextmanager object from a torch.compile function
[1,1]<stdout>:
[1,1]<stdout>:from user code:
[1,1]<stdout>:   File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/two_batch_overlap.py", line 831, in _model_forward_tbo
[1,1]<stdout>:    with context:
[1,1]<stdout>:
[1,1]<stdout>:Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1,1]<stdout>:
[1,1]<stdout>:
[1,1]<stdout>:During handling of the above exception, another exception occurred:
[1,1]<stdout>:
[1,1]<stdout>:Traceback (most recent call last):
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2555, in run_scheduler_process
[1,1]<stdout>:    scheduler = Scheduler(
[1,1]<stdout>:                ^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 315, in __init__
[1,1]<stdout>:    self.tp_worke[1,1]<stdout>:r = TpWorkerClass(
[1,1]<stdout>:                     ^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 84, in __init__
[1,1]<stdout>:    self.model_runner = ModelRunner(
[1,1]<stdout>:                        ^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 242, in __init__
[1,1]<stdout>:    self.initialize(min_per_gpu_memory)
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 342, in initialize
[1,1]<stdout>:    self.init_cuda_graphs()
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 1611, in init_cuda_graphs
[1,1]<stdout>:    self.cuda_graph_runner = CudaGraphRunner(self)
[1,1]<stdout>:                             ^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 382, in __init__
[1,1]<stdout>:    raise Exception(
[1,1]<stdout>:Exception: Capture cuda graph failed: IncorrectUsage: NYI: Returning a @contextmanager object from a torch.compile function
[1,1]<stdout>:
[1,1]<stdout>:from user code:
[1,1]<stdout>:   File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/two_batch_overlap.py", line 831, in _model_forward_tbo
[1,1]<stdout>:    with context:
[1,1]<stdout>:
[1,1]<stdout>:Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1,1]<stdout>:
[1,1]<stdout>:Possible solutions:
[1,1]<stdout>:1. set --mem-fraction-static to a smaller value (e.g., 0.8 or 0.7)
[1,1]<stdout>:2. set --cuda-graph-max-bs to a smaller value (e.g., 16)
[1,1]<stdout>:3. disable torch compile by not using --enable-torch-compile
[1,1]<stdout>:4. disable CUDA graph by --disable-cuda-graph. (Not recommended. Huge performance loss)
[1,1]<stdout>:Open an issue on GitHub https://github.com/sgl-project/sglang/issues/new/choose 
[1,1]<stdout>:
[1,1]<stdout>:
[1,1]<stdout>:[2025-10-12 02:56:35 DP1 TP11] Scheduler hit an exception: Traceback (most recent call last):
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 380, in __init__
[1,1]<stdout>:    self.capture()
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 485, in capture
[1,1]<stdout>:    ) = self.capture_one_batch_size(bs, forward)
[1,1]<stdout>:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 649, in capture_one_batch_size
[1,1]<stdout>:    run_once()
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 637, in run_once
[1,1]<stdout>:    logits_output_or_pp_proxy_tensors = forward(
[1,1]<stdout>:                                        ^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 736, in compile_wrapper
[1,1]<stdout>:    return fn(*args, **kwargs)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/external_utils.py", line 70, in inner
[1,1]<stdout>:    return fn(*args, **kwargs)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1,1]<stdout>:    return func(*args, **kwargs)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1,1]<stdout>:    return func(*args, **kwargs)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/models/deepseek_v2.py", line 2221, in forward
[1,1]<stdout>:    hidden_states = self.model(input_ids, positions, forward_batch, input_embeds)
[1,1]<stdout>:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
[1,1]<stdout>:    return self._call_impl(*args, **kwargs)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
[1,1]<stdout>:    return forward_call(*args, **kwargs)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/models/deepseek_v2.py", line 2109, in forward
[1,1]<stdout>:    enable_tbo=True,
[1,1]<stdout>:                     
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/two_batch_overlap.py", line 800, in model_forward_maybe_tbo
[1,1]<stdout>:    return _model_forward_tbo(
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 1495, in __call__
[1,1]<stdout>:    return self._torchdynamo_orig_callable(
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 1272, in __call__
[1,1]<stdout>:    result = self._inner_convert(
[1,1]<stdout>:             ^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 629, in __call__
[1,1]<stdout>:    return _compile(
[1,1]<stdout>:           ^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 1164, in _compile
[1,1]<stdout>:    raise InternalTorchDynamoError(
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 1111, in [1,1]<stdout>:_compile
[1,1]<stdout>:    guarded_code = compile_inner(code, one_graph, hooks, transform)
[1,1]<stdout>:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_utils_internal.py", line 97, in wrapper_function
[1,1]<stdout>:    return function(*args, **kwargs)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 793, in compile_inner
[1,1]<stdout>:    return _compile_inner(code, one_graph, hooks, transform)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 832, in _compile_inner
[1,1]<stdout>:    out_code = transform_code_object(code, transform)
[1,1]<stdout>:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/bytecode_transformation.py", line 1424, in transform_code_object
[1,1]<stdout>:    transformations(instructions, code_options)
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 267, in _fn
[1,1]<stdout>:    return fn(*args, **kwargs)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 753, in transform
[1,1]<stdout>:    tracer.run()
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py", line 3497, in run
[1,1]<stdout>:    super().run()
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py", line 1363, in run
[1,1]<stdout>:    while self.step():
[1,1]<stdout>:          ^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py", line 1257, in step
[1,1]<stdout>:    return self.step_graph_break(inst)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py", line 1342, in step_graph_break
[1,1]<stdout>:    self.output.compile_subgraph(
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 1399, in compile_subgraph
[1,1]<stdout>:    self.codegen_suffix(tx, stack_values_flat, pass1)
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 1477, in codegen_suffix
[1,1]<stdout>:    self.side_effects.codegen_update_mutated(cg)
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/side_effects.py", line 1065, in codegen_update_mutated
[1,1]<stdout>:    cg(value)
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/codegen.py", line 235, in __call__
[1,1]<stdout>:    raise IncorrectUsage(
[1,1]<stdout>:torch._dynamo.exc.InternalTorchDynamoError: IncorrectUsage: NYI: Returning a @contextmanager object from a torch.compile function
[1,1]<stdout>:
[1,1]<stdout>:from user code:
[1,1]<stdout>:   File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/two_batch_overlap.py", line 831, in _model_forward_tbo
[1,1]<stdout>:    with context:
[1,1]<stdout>:
[1,1]<stdout>:Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1,1]<stdout>:
[1,1]<stdout>:
[1,1]<stdout>:During handling of the above exception, another exception occurred:
[1,1]<stdout>:
[1,1]<stdout>:Traceback (most recent call last):
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2555, in run_scheduler_process
[1,1]<stdout>:    scheduler = Scheduler(
[1,1]<stdout>:                ^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 315, in __init__
[1,1]<stdout>:    self.tp_worker[1,1]<stdout>: = TpWorkerClass(
[1,1]<stdout>:                     ^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 84, in __init__
[1,1]<stdout>:    self.model_runner = ModelRunner(
[1,1]<stdout>:                        ^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 242, in __init__
[1,1]<stdout>:    self.initialize(min_per_gpu_memory)
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 342, in initialize
[1,1]<stdout>:    self.init_cuda_graphs()
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 1611, in init_cuda_graphs
[1,1]<stdout>:    self.cuda_graph_runner = CudaGraphRunner(self)
[1,1]<stdout>:                             ^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 382, in __init__
[1,1]<stdout>:    raise Exception(
[1,1]<stdout>:Exception: Capture cuda graph failed: IncorrectUsage: NYI: Returning a @contextmanager object from a torch.compile function
[1,1]<stdout>:
[1,1]<stdout>:from user code:
[1,1]<stdout>:   File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/two_batch_overlap.py", line 831, in _model_forward_tbo
[1,1]<stdout>:    with context:
[1,1]<stdout>:
[1,1]<stdout>:Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1,1]<stdout>:
[1,1]<stdout>:Possible solutions:
[1,1]<stdout>:1. set --mem-fraction-static to a smaller value (e.g., 0.8 or 0.7)
[1,1]<stdout>:2. set --cuda-graph-max-bs to a smaller value (e.g., 16)
[1,1]<stdout>:3. disable torch compile by not using --enable-torch-compile
[1,1]<stdout>:4. disable CUDA graph by --disable-cuda-graph. (Not recommended. Huge performance loss)
[1,1]<stdout>:Open an issue on GitHub https://github.com/sgl-project/sglang/issues/new/choose 
[1,1]<stdout>:
[1,1]<stdout>:
[1,0]<stdout>:[2025-10-12 02:56:35 DP0 TP3] Scheduler hit an exception: Traceback (most recent call last):
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 380, in __init__
[1,0]<stdout>:    self.capture()
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 485, in capture
[1,0]<stdout>:    ) = self.capture_one_batch_size(bs, forward)
[1,0]<stdout>:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 649, in capture_one_batch_size
[1,0]<stdout>:    run_once()
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 637, in run_once
[1,0]<stdout>:    logits_output_or_pp_proxy_tensors = forward(
[1,0]<stdout>:                                        ^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 736, in compile_wrapper
[1,0]<stdout>:    return fn(*args, **kwargs)
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/external_utils.py", line 70, in inner
[1,0]<stdout>:    return fn(*args, **kwargs)
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1,0]<stdout>:    return func(*args, **kwargs)
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1,0]<stdout>:    return func(*args, **kwargs)
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/models/deepseek_v2.py", line 2221, in forward
[1,0]<stdout>:    hidden_states = self.model(input_ids, positions, forward_batch, input_embeds)
[1,0]<stdout>:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
[1,0]<stdout>:    return self._call_impl(*args, **kwargs)
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
[1,0]<stdout>:    return forward_call(*args, **kwargs)
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/models/deepseek_v2.py", line 2109, in forward
[1,0]<stdout>:    enable_tbo=True,
[1,0]<stdout>:                     
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/two_batch_overlap.py", line 800, in model_forward_maybe_tbo
[1,0]<stdout>:    return _model_forward_tbo(
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 1495, in __call__
[1,0]<stdout>:    return self._torchdynamo_orig_callable(
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 1272, in __call__
[1,0]<stdout>:    result = self._inner_convert(
[1,0]<stdout>:             ^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 629, in __call__
[1,0]<stdout>:    return _compile(
[1,0]<stdout>:           ^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 1164, in _compile
[1,0]<stdout>:    raise InternalTorchDynamoError(
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 1111, in _[1,0]<stdout>:compile
[1,0]<stdout>:    guarded_code = compile_inner(code, one_graph, hooks, transform)
[1,0]<stdout>:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_utils_internal.py", line 97, in wrapper_function
[1,0]<stdout>:    return function(*args, **kwargs)
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 793, in compile_inner
[1,0]<stdout>:    return _compile_inner(code, one_graph, hooks, transform)
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 832, in _compile_inner
[1,0]<stdout>:    out_code = transform_code_object(code, transform)
[1,0]<stdout>:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/bytecode_transformation.py", line 1424, in transform_code_object
[1,0]<stdout>:    transformations(instructions, code_options)
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 267, in _fn
[1,0]<stdout>:    return fn(*args, **kwargs)
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 753, in transform
[1,0]<stdout>:    tracer.run()
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py", line 3497, in run
[1,0]<stdout>:    super().run()
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py", line 1363, in run
[1,0]<stdout>:    while self.step():
[1,0]<stdout>:          ^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py", line 1257, in step
[1,0]<stdout>:    return self.step_graph_break(inst)
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py", line 1342, in step_graph_break
[1,0]<stdout>:    self.output.compile_subgraph(
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 1399, in compile_subgraph
[1,0]<stdout>:    self.codegen_suffix(tx, stack_values_flat, pass1)
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 1477, in codegen_suffix
[1,0]<stdout>:    self.side_effects.codegen_update_mutated(cg)
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/side_effects.py", line 1065, in codegen_update_mutated
[1,0]<stdout>:    cg(value)
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/codegen.py", line 235, in __call__
[1,0]<stdout>:    raise IncorrectUsage(
[1,0]<stdout>:torch._dynamo.exc.InternalTorchDynamoError: IncorrectUsage: NYI: Returning a @contextmanager object from a torch.compile function
[1,0]<stdout>:
[1,0]<stdout>:from user code:
[1,0]<stdout>:   File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/two_batch_overlap.py", line 831, in _model_forward_tbo
[1,0]<stdout>:    with context:
[1,0]<stdout>:
[1,0]<stdout>:Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1,0]<stdout>:
[1,0]<stdout>:
[1,0]<stdout>:During handling of the above exception, another exception occurred:
[1,0]<stdout>:
[1,0]<stdout>:Traceback (most recent call last):
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2555, in run_scheduler_process
[1,0]<stdout>:    scheduler = Scheduler(
[1,0]<stdout>:                ^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 315, in __init__
[1,0]<stdout>:    self.tp_worker [1,0]<stdout>:= TpWorkerClass(
[1,0]<stdout>:                     ^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 84, in __init__
[1,0]<stdout>:    self.model_runner = ModelRunner(
[1,0]<stdout>:                        ^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 242, in __init__
[1,0]<stdout>:    self.initialize(min_per_gpu_memory)
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 342, in initialize
[1,0]<stdout>:    self.init_cuda_graphs()
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 1611, in init_cuda_graphs
[1,0]<stdout>:    self.cuda_graph_runner = CudaGraphRunner(self)
[1,0]<stdout>:                             ^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 382, in __init__
[1,0]<stdout>:    raise Exception(
[1,0]<stdout>:Exception: Capture cuda graph failed: IncorrectUsage: NYI: Returning a @contextmanager object from a torch.compile function
[1,0]<stdout>:
[1,0]<stdout>:from user code:
[1,0]<stdout>:   File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/two_batch_overlap.py", line 831, in _model_forward_tbo
[1,0]<stdout>:    with context:
[1,0]<stdout>:
[1,0]<stdout>:Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1,0]<stdout>:
[1,0]<stdout>:Possible solutions:
[1,0]<stdout>:1. set --mem-fraction-static to a smaller value (e.g., 0.8 or 0.7)
[1,0]<stdout>:2. set --cuda-graph-max-bs to a smaller value (e.g., 16)
[1,0]<stdout>:3. disable torch compile by not using --enable-torch-compile
[1,0]<stdout>:4. disable CUDA graph by --disable-cuda-graph. (Not recommended. Huge performance loss)
[1,0]<stdout>:Open an issue on GitHub https://github.com/sgl-project/sglang/issues/new/choose 
[1,0]<stdout>:
[1,0]<stdout>:
[1,1]<stdout>:[2025-10-12 02:56:35 DP1 TP9] Scheduler hit an exception: Traceback (most recent call last):
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 380, in __init__
[1,1]<stdout>:    self.capture()
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 485, in capture
[1,1]<stdout>:    ) = self.capture_one_batch_size(bs, forward)
[1,1]<stdout>:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 649, in capture_one_batch_size
[1,1]<stdout>:    run_once()
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 637, in run_once
[1,1]<stdout>:    logits_output_or_pp_proxy_tensors = forward(
[1,1]<stdout>:                                        ^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 736, in compile_wrapper
[1,1]<stdout>:    return fn(*args, **kwargs)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/external_utils.py", line 70, in inner
[1,1]<stdout>:    return fn(*args, **kwargs)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1,1]<stdout>:    return func(*args, **kwargs)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1,1]<stdout>:    return func(*args, **kwargs)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/models/deepseek_v2.py", line 2221, in forward
[1,1]<stdout>:    hidden_states = self.model(input_ids, positions, forward_batch, input_embeds)
[1,1]<stdout>:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
[1,1]<stdout>:    return self._call_impl(*args, **kwargs)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
[1,1]<stdout>:    return forward_call(*args, **kwargs)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/models/deepseek_v2.py", line 2109, in forward
[1,1]<stdout>:    enable_tbo=True,
[1,1]<stdout>:                     
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/two_batch_overlap.py", line 800, in model_forward_maybe_tbo
[1,1]<stdout>:    return _model_forward_tbo(
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 1495, in __call__
[1,1]<stdout>:    return self._torchdynamo_orig_callable(
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 1272, in __call__
[1,1]<stdout>:    result = self._inner_convert(
[1,1]<stdout>:             ^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 629, in __call__
[1,1]<stdout>:    return _compile(
[1,1]<stdout>:           ^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 1164, in _compile
[1,1]<stdout>:    raise InternalTorchDynamoError(
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 1111, in _[1,1]<stdout>:compile
[1,1]<stdout>:    guarded_code = compile_inner(code, one_graph, hooks, transform)
[1,1]<stdout>:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_utils_internal.py", line 97, in wrapper_function
[1,1]<stdout>:    return function(*args, **kwargs)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 793, in compile_inner
[1,1]<stdout>:    return _compile_inner(code, one_graph, hooks, transform)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 832, in _compile_inner
[1,1]<stdout>:    out_code = transform_code_object(code, transform)
[1,1]<stdout>:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/bytecode_transformation.py", line 1424, in transform_code_object
[1,1]<stdout>:    transformations(instructions, code_options)
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 267, in _fn
[1,1]<stdout>:    return fn(*args, **kwargs)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 753, in transform
[1,1]<stdout>:    tracer.run()
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py", line 3497, in run
[1,1]<stdout>:    super().run()
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py", line 1363, in run
[1,1]<stdout>:    while self.step():
[1,1]<stdout>:          ^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py", line 1257, in step
[1,1]<stdout>:    return self.step_graph_break(inst)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py", line 1342, in step_graph_break
[1,1]<stdout>:    self.output.compile_subgraph(
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 1399, in compile_subgraph
[1,1]<stdout>:    self.codegen_suffix(tx, stack_values_flat, pass1)
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 1477, in codegen_suffix
[1,1]<stdout>:    self.side_effects.codegen_update_mutated(cg)
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/side_effects.py", line 1065, in codegen_update_mutated
[1,1]<stdout>:    cg(value)
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/codegen.py", line 235, in __call__
[1,1]<stdout>:    raise IncorrectUsage(
[1,1]<stdout>:torch._dynamo.exc.InternalTorchDynamoError: IncorrectUsage: NYI: Returning a @contextmanager object from a torch.compile function
[1,1]<stdout>:
[1,1]<stdout>:from user code:
[1,1]<stdout>:   File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/two_batch_overlap.py", line 831, in _model_forward_tbo
[1,1]<stdout>:    with context:
[1,1]<stdout>:
[1,1]<stdout>:Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1,1]<stdout>:
[1,1]<stdout>:
[1,1]<stdout>:During handling of the above exception, another exception occurred:
[1,1]<stdout>:
[1,1]<stdout>:Traceback (most recent call last):
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2555, in run_scheduler_process
[1,1]<stdout>:    scheduler = Scheduler(
[1,1]<stdout>:                ^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 315, in __init__
[1,1]<stdout>:    self.tp_worker [1,1]<stdout>:= TpWorkerClass(
[1,1]<stdout>:                     ^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 84, in __init__
[1,1]<stdout>:    self.model_runner = ModelRunner(
[1,1]<stdout>:                        ^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 242, in __init__
[1,1]<stdout>:    self.initialize(min_per_gpu_memory)
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 342, in initialize
[1,1]<stdout>:    self.init_cuda_graphs()
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 1611, in init_cuda_graphs
[1,1]<stdout>:    self.cuda_graph_runner = CudaGraphRunner(self)
[1,1]<stdout>:                             ^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 382, in __init__
[1,1]<stdout>:    raise Exception(
[1,1]<stdout>:Exception: Capture cuda graph failed: IncorrectUsage: NYI: Returning a @contextmanager object from a torch.compile function
[1,1]<stdout>:
[1,1]<stdout>:from user code:
[1,1]<stdout>:   File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/two_batch_overlap.py", line 831, in _model_forward_tbo
[1,1]<stdout>:    with context:
[1,1]<stdout>:
[1,1]<stdout>:Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1,1]<stdout>:
[1,1]<stdout>:Possible solutions:
[1,1]<stdout>:1. set --mem-fraction-static to a smaller value (e.g., 0.8 or 0.7)
[1,1]<stdout>:2. set --cuda-graph-max-bs to a smaller value (e.g., 16)
[1,1]<stdout>:3. disable torch compile by not using --enable-torch-compile
[1,1]<stdout>:4. disable CUDA graph by --disable-cuda-graph. (Not recommended. Huge performance loss)
[1,1]<stdout>:Open an issue on GitHub https://github.com/sgl-project/sglang/issues/new/choose 
[1,1]<stdout>:
[1,1]<stdout>:
[1,1]<stdout>:[2025-10-12 02:56:35 DP1 TP8] Scheduler hit an exception: Traceback (most recent call last):
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 380, in __init__
[1,1]<stdout>:    self.capture()
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 485, in capture
[1,1]<stdout>:    ) = self.capture_one_batch_size(bs, forward)
[1,1]<stdout>:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 649, in capture_one_batch_size
[1,1]<stdout>:    run_once()
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 637, in run_once
[1,1]<stdout>:    logits_output_or_pp_proxy_tensors = forward(
[1,1]<stdout>:                                        ^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 736, in compile_wrapper
[1,1]<stdout>:    return fn(*args, **kwargs)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/external_utils.py", line 70, in inner
[1,1]<stdout>:    return fn(*args, **kwargs)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1,1]<stdout>:    return func(*args, **kwargs)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1,1]<stdout>:    return func(*args, **kwargs)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/models/deepseek_v2.py", line 2221, in forward
[1,1]<stdout>:    hidden_states = self.model(input_ids, positions, forward_batch, input_embeds)
[1,1]<stdout>:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
[1,1]<stdout>:    return self._call_impl(*args, **kwargs)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
[1,1]<stdout>:    return forward_call(*args, **kwargs)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/models/deepseek_v2.py", line 2109, in forward
[1,1]<stdout>:    enable_tbo=True,
[1,1]<stdout>:                     
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/two_batch_overlap.py", line 800, in model_forward_maybe_tbo
[1,1]<stdout>:    return _model_forward_tbo(
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 1495, in __call__
[1,1]<stdout>:    return self._torchdynamo_orig_callable(
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 1272, in __call__
[1,1]<stdout>:    result = self._inner_convert(
[1,1]<stdout>:             ^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 629, in __call__
[1,1]<stdout>:    return _compile(
[1,1]<stdout>:           ^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 1164, in _compile
[1,1]<stdout>:    raise InternalTorchDynamoError(
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 1111, in _[1,1]<stdout>:compile
[1,1]<stdout>:    guarded_code = compile_inner(code, one_graph, hooks, transform)
[1,1]<stdout>:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_utils_internal.py", line 97, in wrapper_function
[1,1]<stdout>:    return function(*args, **kwargs)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 793, in compile_inner
[1,1]<stdout>:    return _compile_inner(code, one_graph, hooks, transform)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 832, in _compile_inner
[1,1]<stdout>:    out_code = transform_code_object(code, transform)
[1,1]<stdout>:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/bytecode_transformation.py", line 1424, in transform_code_object
[1,1]<stdout>:    transformations(instructions, code_options)
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 267, in _fn
[1,1]<stdout>:    return fn(*args, **kwargs)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 753, in transform
[1,1]<stdout>:    tracer.run()
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py", line 3497, in run
[1,1]<stdout>:    super().run()
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py", line 1363, in run
[1,1]<stdout>:    while self.step():
[1,1]<stdout>:          ^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py", line 1257, in step
[1,1]<stdout>:    return self.step_graph_break(inst)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py", line 1342, in step_graph_break
[1,1]<stdout>:    self.output.compile_subgraph(
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 1399, in compile_subgraph
[1,1]<stdout>:    self.codegen_suffix(tx, stack_values_flat, pass1)
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 1477, in codegen_suffix
[1,1]<stdout>:    self.side_effects.codegen_update_mutated(cg)
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/side_effects.py", line 1065, in codegen_update_mutated
[1,1]<stdout>:    cg(value)
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/codegen.py", line 235, in __call__
[1,1]<stdout>:    raise IncorrectUsage(
[1,1]<stdout>:torch._dynamo.exc.InternalTorchDynamoError: IncorrectUsage: NYI: Returning a @contextmanager object from a torch.compile function
[1,1]<stdout>:
[1,1]<stdout>:from user code:
[1,1]<stdout>:   File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/two_batch_overlap.py", line 831, in _model_forward_tbo
[1,1]<stdout>:    with context:
[1,1]<stdout>:
[1,1]<stdout>:Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1,1]<stdout>:
[1,1]<stdout>:
[1,1]<stdout>:During handling of the above exception, another exception occurred:
[1,1]<stdout>:
[1,1]<stdout>:Traceback (most recent call last):
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2555, in run_scheduler_process
[1,1]<stdout>:    scheduler = Scheduler(
[1,1]<stdout>:                ^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 315, in __init__
[1,1]<stdout>:    self.tp_worker [1,1]<stdout>:= TpWorkerClass(
[1,1]<stdout>:                     ^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 84, in __init__
[1,1]<stdout>:    self.model_runner = ModelRunner(
[1,1]<stdout>:                        ^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 242, in __init__
[1,1]<stdout>:    self.initialize(min_per_gpu_memory)
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 342, in initialize
[1,1]<stdout>:    self.init_cuda_graphs()
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 1611, in init_cuda_graphs
[1,1]<stdout>:    self.cuda_graph_runner = CudaGraphRunner(self)
[1,1]<stdout>:                             ^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 382, in __init__
[1,1]<stdout>:    raise Exception(
[1,1]<stdout>:Exception: Capture cuda graph failed: IncorrectUsage: NYI: Returning a @contextmanager object from a torch.compile function
[1,1]<stdout>:
[1,1]<stdout>:from user code:
[1,1]<stdout>:   File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/two_batch_overlap.py", line 831, in _model_forward_tbo
[1,1]<stdout>:    with context:
[1,1]<stdout>:
[1,1]<stdout>:Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1,1]<stdout>:
[1,1]<stdout>:Possible solutions:
[1,1]<stdout>:1. set --mem-fraction-static to a smaller value (e.g., 0.8 or 0.7)
[1,1]<stdout>:2. set --cuda-graph-max-bs to a smaller value (e.g., 16)
[1,1]<stdout>:3. disable torch compile by not using --enable-torch-compile
[1,1]<stdout>:4. disable CUDA graph by --disable-cuda-graph. (Not recommended. Huge performance loss)
[1,1]<stdout>:Open an issue on GitHub https://github.com/sgl-project/sglang/issues/new/choose 
[1,1]<stdout>:
[1,1]<stdout>:
[1,0]<stdout>:[2025-10-12 02:56:35 DP0 TP1] Scheduler hit an exception: Traceback (most recent call last):
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 380, in __init__
[1,0]<stdout>:    self.capture()
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 485, in capture
[1,0]<stdout>:    ) = self.capture_one_batch_size(bs, forward)
[1,0]<stdout>:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 649, in capture_one_batch_size
[1,0]<stdout>:    run_once()
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 637, in run_once
[1,0]<stdout>:    logits_output_or_pp_proxy_tensors = forward(
[1,0]<stdout>:                                        ^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 736, in compile_wrapper
[1,0]<stdout>:    return fn(*args, **kwargs)
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/external_utils.py", line 70, in inner
[1,0]<stdout>:    return fn(*args, **kwargs)
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1,0]<stdout>:    return func(*args, **kwargs)
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1,0]<stdout>:    return func(*args, **kwargs)
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/models/deepseek_v2.py", line 2221, in forward
[1,0]<stdout>:    hidden_states = self.model(input_ids, positions, forward_batch, input_embeds)
[1,0]<stdout>:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
[1,0]<stdout>:    return self._call_impl(*args, **kwargs)
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
[1,0]<stdout>:    return forward_call(*args, **kwargs)
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/models/deepseek_v2.py", line 2109, in forward
[1,0]<stdout>:    enable_tbo=True,
[1,0]<stdout>:                     
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/two_batch_overlap.py", line 800, in model_forward_maybe_tbo
[1,0]<stdout>:    return _model_forward_tbo(
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 1495, in __call__
[1,0]<stdout>:    return self._torchdynamo_orig_callable(
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 1272, in __call__
[1,0]<stdout>:    result = self._inner_convert(
[1,0]<stdout>:             ^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 629, in __call__
[1,0]<stdout>:    return _compile(
[1,0]<stdout>:           ^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 1164, in _compile
[1,0]<stdout>:    raise InternalTorchDynamoError(
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 1111, in _[1,0]<stdout>:compile
[1,0]<stdout>:    guarded_code = compile_inner(code, one_graph, hooks, transform)
[1,0]<stdout>:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_utils_internal.py", line 97, in wrapper_function
[1,0]<stdout>:    return function(*args, **kwargs)
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 793, in compile_inner
[1,0]<stdout>:    return _compile_inner(code, one_graph, hooks, transform)
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 832, in _compile_inner
[1,0]<stdout>:    out_code = transform_code_object(code, transform)
[1,0]<stdout>:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/bytecode_transformation.py", line 1424, in transform_code_object
[1,0]<stdout>:    transformations(instructions, code_options)
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 267, in _fn
[1,0]<stdout>:    return fn(*args, **kwargs)
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 753, in transform
[1,0]<stdout>:    tracer.run()
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py", line 3497, in run
[1,0]<stdout>:    super().run()
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py", line 1363, in run
[1,0]<stdout>:    while self.step():
[1,0]<stdout>:          ^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py", line 1257, in step
[1,0]<stdout>:    return self.step_graph_break(inst)
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py", line 1342, in step_graph_break
[1,0]<stdout>:    self.output.compile_subgraph(
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 1399, in compile_subgraph
[1,0]<stdout>:    self.codegen_suffix(tx, stack_values_flat, pass1)
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 1477, in codegen_suffix
[1,0]<stdout>:    self.side_effects.codegen_update_mutated(cg)
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/side_effects.py", line 1065, in codegen_update_mutated
[1,0]<stdout>:    cg(value)
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/codegen.py", line 235, in __call__
[1,0]<stdout>:    raise IncorrectUsage(
[1,0]<stdout>:torch._dynamo.exc.InternalTorchDynamoError: IncorrectUsage: NYI: Returning a @contextmanager object from a torch.compile function
[1,0]<stdout>:
[1,0]<stdout>:from user code:
[1,0]<stdout>:   File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/two_batch_overlap.py", line 831, in _model_forward_tbo
[1,0]<stdout>:    with context:
[1,0]<stdout>:
[1,0]<stdout>:Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1,0]<stdout>:
[1,0]<stdout>:
[1,0]<stdout>:During handling of the above exception, another exception occurred:
[1,0]<stdout>:
[1,0]<stdout>:Traceback (most recent call last):
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2555, in run_scheduler_process
[1,0]<stdout>:    scheduler = Scheduler(
[1,0]<stdout>:                ^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 315, in __init__
[1,0]<stdout>:    self.tp_worker [1,0]<stdout>:= TpWorkerClass(
[1,0]<stdout>:                     ^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 84, in __init__
[1,0]<stdout>:    self.model_runner = ModelRunner(
[1,0]<stdout>:                        ^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 242, in __init__
[1,0]<stdout>:    self.initialize(min_per_gpu_memory)
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 342, in initialize
[1,0]<stdout>:    self.init_cuda_graphs()
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 1611, in init_cuda_graphs
[1,0]<stdout>:    self.cuda_graph_runner = CudaGraphRunner(self)
[1,0]<stdout>:                             ^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 382, in __init__
[1,0]<stdout>:    raise Exception(
[1,0]<stdout>:Exception: Capture cuda graph failed: IncorrectUsage: NYI: Returning a @contextmanager object from a torch.compile function
[1,0]<stdout>:
[1,0]<stdout>:from user code:
[1,0]<stdout>:   File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/two_batch_overlap.py", line 831, in _model_forward_tbo
[1,0]<stdout>:    with context:
[1,0]<stdout>:
[1,0]<stdout>:Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1,0]<stdout>:
[1,0]<stdout>:Possible solutions:
[1,0]<stdout>:1. set --mem-fraction-static to a smaller value (e.g., 0.8 or 0.7)
[1,0]<stdout>:2. set --cuda-graph-max-bs to a smaller value (e.g., 16)
[1,0]<stdout>:3. disable torch compile by not using --enable-torch-compile
[1,0]<stdout>:4. disable CUDA graph by --disable-cuda-graph. (Not recommended. Huge performance loss)
[1,0]<stdout>:Open an issue on GitHub https://github.com/sgl-project/sglang/issues/new/choose 
[1,0]<stdout>:
[1,0]<stdout>:
[1,1]<stdout>:[2025-10-12 02:56:35 DP1 TP12] Scheduler hit an exception: Traceback (most recent call last):
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 380, in __init__
[1,1]<stdout>:    self.capture()
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 485, in capture
[1,1]<stdout>:    ) = self.capture_one_batch_size(bs, forward)
[1,1]<stdout>:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 649, in capture_one_batch_size
[1,1]<stdout>:    run_once()
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 637, in run_once
[1,1]<stdout>:    logits_output_or_pp_proxy_tensors = forward(
[1,1]<stdout>:                                        ^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 736, in compile_wrapper
[1,1]<stdout>:    return fn(*args, **kwargs)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/external_utils.py", line 70, in inner
[1,1]<stdout>:    return fn(*args, **kwargs)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1,1]<stdout>:    return func(*args, **kwargs)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1,1]<stdout>:    return func(*args, **kwargs)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/models/deepseek_v2.py", line 2221, in forward
[1,1]<stdout>:    hidden_states = self.model(input_ids, positions, forward_batch, input_embeds)
[1,1]<stdout>:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
[1,1]<stdout>:    return self._call_impl(*args, **kwargs)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
[1,1]<stdout>:    return forward_call(*args, **kwargs)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/models/deepseek_v2.py", line 2109, in forward
[1,1]<stdout>:    enable_tbo=True,
[1,1]<stdout>:                     
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/two_batch_overlap.py", line 800, in model_forward_maybe_tbo
[1,1]<stdout>:    return _model_forward_tbo(
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 1495, in __call__
[1,1]<stdout>:    return self._torchdynamo_orig_callable(
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 1272, in __call__
[1,1]<stdout>:    result = self._inner_convert(
[1,1]<stdout>:             ^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 629, in __call__
[1,1]<stdout>:    return _compile(
[1,1]<stdout>:           ^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 1164, in _compile
[1,1]<stdout>:    raise InternalTorchDynamoError(
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 1111, in[1,1]<stdout>: _compile
[1,1]<stdout>:    guarded_code = compile_inner(code, one_graph, hooks, transform)
[1,1]<stdout>:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_utils_internal.py", line 97, in wrapper_function
[1,1]<stdout>:    return function(*args, **kwargs)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 793, in compile_inner
[1,1]<stdout>:    return _compile_inner(code, one_graph, hooks, transform)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 832, in _compile_inner
[1,1]<stdout>:    out_code = transform_code_object(code, transform)
[1,1]<stdout>:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/bytecode_transformation.py", line 1424, in transform_code_object
[1,1]<stdout>:    transformations(instructions, code_options)
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 267, in _fn
[1,1]<stdout>:    return fn(*args, **kwargs)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 753, in transform
[1,1]<stdout>:    tracer.run()
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py", line 3497, in run
[1,1]<stdout>:    super().run()
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py", line 1363, in run
[1,1]<stdout>:    while self.step():
[1,1]<stdout>:          ^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py", line 1257, in step
[1,1]<stdout>:    return self.step_graph_break(inst)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py", line 1342, in step_graph_break
[1,1]<stdout>:    self.output.compile_subgraph(
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 1399, in compile_subgraph
[1,1]<stdout>:    self.codegen_suffix(tx, stack_values_flat, pass1)
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 1477, in codegen_suffix
[1,1]<stdout>:    self.side_effects.codegen_update_mutated(cg)
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/side_effects.py", line 1065, in codegen_update_mutated
[1,1]<stdout>:    cg(value)
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/codegen.py", line 235, in __call__
[1,1]<stdout>:    raise IncorrectUsage(
[1,1]<stdout>:torch._dynamo.exc.InternalTorchDynamoError: IncorrectUsage: NYI: Returning a @contextmanager object from a torch.compile function
[1,1]<stdout>:
[1,1]<stdout>:from user code:
[1,1]<stdout>:   File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/two_batch_overlap.py", line 831, in _model_forward_tbo
[1,1]<stdout>:    with context:
[1,1]<stdout>:
[1,1]<stdout>:Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1,1]<stdout>:
[1,1]<stdout>:
[1,1]<stdout>:During handling of the above exception, another exception occurred:
[1,1]<stdout>:
[1,1]<stdout>:Traceback (most recent call last):
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2555, in run_scheduler_process
[1,1]<stdout>:    scheduler = Scheduler(
[1,1]<stdout>:                ^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 315, in __init__
[1,1]<stdout>:    self.tp_worke[1,1]<stdout>:r = TpWorkerClass(
[1,1]<stdout>:                     ^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 84, in __init__
[1,1]<stdout>:    self.model_runner = ModelRunner(
[1,1]<stdout>:                        ^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 242, in __init__
[1,1]<stdout>:    self.initialize(min_per_gpu_memory)
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 342, in initialize
[1,1]<stdout>:    self.init_cuda_graphs()
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 1611, in init_cuda_graphs
[1,1]<stdout>:    self.cuda_graph_runner = CudaGraphRunner(self)
[1,1]<stdout>:                             ^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 382, in __init__
[1,1]<stdout>:    raise Exception(
[1,1]<stdout>:Exception: Capture cuda graph failed: IncorrectUsage: NYI: Returning a @contextmanager object from a torch.compile function
[1,1]<stdout>:
[1,1]<stdout>:from user code:
[1,1]<stdout>:   File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/two_batch_overlap.py", line 831, in _model_forward_tbo
[1,1]<stdout>:    with context:
[1,1]<stdout>:
[1,1]<stdout>:Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1,1]<stdout>:
[1,1]<stdout>:Possible solutions:
[1,1]<stdout>:1. set --mem-fraction-static to a smaller value (e.g., 0.8 or 0.7)
[1,1]<stdout>:2. set --cuda-graph-max-bs to a smaller value (e.g., 16)
[1,1]<stdout>:3. disable torch compile by not using --enable-torch-compile
[1,1]<stdout>:4. disable CUDA graph by --disable-cuda-graph. (Not recommended. Huge performance loss)
[1,1]<stdout>:Open an issue on GitHub https://github.com/sgl-project/sglang/issues/new/choose 
[1,1]<stdout>:
[1,1]<stdout>:
[1,1]<stdout>:[2025-10-12 02:56:35 DP1 TP13] Scheduler hit an exception: Traceback (most recent call last):
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 380, in __init__
[1,1]<stdout>:    self.capture()
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 485, in capture
[1,1]<stdout>:    ) = self.capture_one_batch_size(bs, forward)
[1,1]<stdout>:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 649, in capture_one_batch_size
[1,1]<stdout>:    run_once()
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 637, in run_once
[1,1]<stdout>:    logits_output_or_pp_proxy_tensors = forward(
[1,1]<stdout>:                                        ^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 736, in compile_wrapper
[1,1]<stdout>:    return fn(*args, **kwargs)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/external_utils.py", line 70, in inner
[1,1]<stdout>:    return fn(*args, **kwargs)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1,1]<stdout>:    return func(*args, **kwargs)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1,1]<stdout>:    return func(*args, **kwargs)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/models/deepseek_v2.py", line 2221, in forward
[1,1]<stdout>:    hidden_states = self.model(input_ids, positions, forward_batch, input_embeds)
[1,1]<stdout>:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
[1,1]<stdout>:    return self._call_impl(*args, **kwargs)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
[1,1]<stdout>:    return forward_call(*args, **kwargs)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/models/deepseek_v2.py", line 2109, in forward
[1,1]<stdout>:    enable_tbo=True,
[1,1]<stdout>:                     
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/two_batch_overlap.py", line 800, in model_forward_maybe_tbo
[1,1]<stdout>:    return _model_forward_tbo(
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 1495, in __call__
[1,1]<stdout>:    return self._torchdynamo_orig_callable(
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 1272, in __call__
[1,1]<stdout>:    result = self._inner_convert(
[1,1]<stdout>:             ^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 629, in __call__
[1,1]<stdout>:    return _compile(
[1,1]<stdout>:           ^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 1164, in _compile
[1,1]<stdout>:    raise InternalTorchDynamoError(
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 1111, in[1,1]<stdout>: _compile
[1,1]<stdout>:    guarded_code = compile_inner(code, one_graph, hooks, transform)
[1,1]<stdout>:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_utils_internal.py", line 97, in wrapper_function
[1,1]<stdout>:    return function(*args, **kwargs)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 793, in compile_inner
[1,1]<stdout>:    return _compile_inner(code, one_graph, hooks, transform)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 832, in _compile_inner
[1,1]<stdout>:    out_code = transform_code_object(code, transform)
[1,1]<stdout>:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/bytecode_transformation.py", line 1424, in transform_code_object
[1,1]<stdout>:    transformations(instructions, code_options)
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 267, in _fn
[1,1]<stdout>:    return fn(*args, **kwargs)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 753, in transform
[1,1]<stdout>:    tracer.run()
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py", line 3497, in run
[1,1]<stdout>:    super().run()
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py", line 1363, in run
[1,1]<stdout>:    while self.step():
[1,1]<stdout>:          ^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py", line 1257, in step
[1,1]<stdout>:    return self.step_graph_break(inst)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py", line 1342, in step_graph_break
[1,1]<stdout>:    self.output.compile_subgraph(
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 1399, in compile_subgraph
[1,1]<stdout>:    self.codegen_suffix(tx, stack_values_flat, pass1)
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 1477, in codegen_suffix
[1,1]<stdout>:    self.side_effects.codegen_update_mutated(cg)
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/side_effects.py", line 1065, in codegen_update_mutated
[1,1]<stdout>:    cg(value)
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/codegen.py", line 235, in __call__
[1,1]<stdout>:    raise IncorrectUsage(
[1,1]<stdout>:torch._dynamo.exc.InternalTorchDynamoError: IncorrectUsage: NYI: Returning a @contextmanager object from a torch.compile function
[1,1]<stdout>:
[1,1]<stdout>:from user code:
[1,1]<stdout>:   File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/two_batch_overlap.py", line 831, in _model_forward_tbo
[1,1]<stdout>:    with context:
[1,1]<stdout>:
[1,1]<stdout>:Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1,1]<stdout>:
[1,1]<stdout>:
[1,1]<stdout>:During handling of the above exception, another exception occurred:
[1,1]<stdout>:
[1,1]<stdout>:Traceback (most recent call last):
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2555, in run_scheduler_process
[1,1]<stdout>:    scheduler = Scheduler(
[1,1]<stdout>:                ^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 315, in __init__
[1,1]<stdout>:    self.tp_worke[1,1]<stdout>:r = TpWorkerClass(
[1,1]<stdout>:                     ^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 84, in __init__
[1,1]<stdout>:    self.model_runner = ModelRunner(
[1,1]<stdout>:                        ^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 242, in __init__
[1,1]<stdout>:    self.initialize(min_per_gpu_memory)
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 342, in initialize
[1,1]<stdout>:    self.init_cuda_graphs()
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 1611, in init_cuda_graphs
[1,1]<stdout>:    self.cuda_graph_runner = CudaGraphRunner(self)
[1,1]<stdout>:                             ^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 382, in __init__
[1,1]<stdout>:    raise Exception(
[1,1]<stdout>:Exception: Capture cuda graph failed: IncorrectUsage: NYI: Returning a @contextmanager object from a torch.compile function
[1,1]<stdout>:
[1,1]<stdout>:from user code:
[1,1]<stdout>:   File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/two_batch_overlap.py", line 831, in _model_forward_tbo
[1,1]<stdout>:    with context:
[1,1]<stdout>:
[1,1]<stdout>:Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1,1]<stdout>:
[1,1]<stdout>:Possible solutions:
[1,1]<stdout>:1. set --mem-fraction-static to a smaller value (e.g., 0.8 or 0.7)
[1,1]<stdout>:2. set --cuda-graph-max-bs to a smaller value (e.g., 16)
[1,1]<stdout>:3. disable torch compile by not using --enable-torch-compile
[1,1]<stdout>:4. disable CUDA graph by --disable-cuda-graph. (Not recommended. Huge performance loss)
[1,1]<stdout>:Open an issue on GitHub https://github.com/sgl-project/sglang/issues/new/choose 
[1,1]<stdout>:
[1,1]<stdout>:
[1,0]<stdout>:[2025-10-12 02:56:35 DP0 TP4] Scheduler hit an exception: Traceback (most recent call last):
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 380, in __init__
[1,0]<stdout>:    self.capture()
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 485, in capture
[1,0]<stdout>:    ) = self.capture_one_batch_size(bs, forward)
[1,0]<stdout>:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 649, in capture_one_batch_size
[1,0]<stdout>:    run_once()
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 637, in run_once
[1,0]<stdout>:    logits_output_or_pp_proxy_tensors = forward(
[1,0]<stdout>:                                        ^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 736, in compile_wrapper
[1,0]<stdout>:    return fn(*args, **kwargs)
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/external_utils.py", line 70, in inner
[1,0]<stdout>:    return fn(*args, **kwargs)
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1,0]<stdout>:    return func(*args, **kwargs)
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1,0]<stdout>:    return func(*args, **kwargs)
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/models/deepseek_v2.py", line 2221, in forward
[1,0]<stdout>:    hidden_states = self.model(input_ids, positions, forward_batch, input_embeds)
[1,0]<stdout>:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
[1,0]<stdout>:    return self._call_impl(*args, **kwargs)
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
[1,0]<stdout>:    return forward_call(*args, **kwargs)
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/models/deepseek_v2.py", line 2109, in forward
[1,0]<stdout>:    enable_tbo=True,
[1,0]<stdout>:                     
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/two_batch_overlap.py", line 800, in model_forward_maybe_tbo
[1,0]<stdout>:    return _model_forward_tbo(
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 1495, in __call__
[1,0]<stdout>:    return self._torchdynamo_orig_callable(
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 1272, in __call__
[1,0]<stdout>:    result = self._inner_convert(
[1,0]<stdout>:             ^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 629, in __call__
[1,0]<stdout>:    return _compile(
[1,0]<stdout>:           ^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 1164, in _compile
[1,0]<stdout>:    raise InternalTorchDynamoError(
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 1111, in _[1,0]<stdout>:compile
[1,0]<stdout>:    guarded_code = compile_inner(code, one_graph, hooks, transform)
[1,0]<stdout>:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_utils_internal.py", line 97, in wrapper_function
[1,0]<stdout>:    return function(*args, **kwargs)
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 793, in compile_inner
[1,0]<stdout>:    return _compile_inner(code, one_graph, hooks, transform)
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 832, in _compile_inner
[1,0]<stdout>:    out_code = transform_code_object(code, transform)
[1,0]<stdout>:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/bytecode_transformation.py", line 1424, in transform_code_object
[1,0]<stdout>:    transformations(instructions, code_options)
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 267, in _fn
[1,0]<stdout>:    return fn(*args, **kwargs)
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 753, in transform
[1,0]<stdout>:    tracer.run()
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py", line 3497, in run
[1,0]<stdout>:    super().run()
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py", line 1363, in run
[1,0]<stdout>:    while self.step():
[1,0]<stdout>:          ^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py", line 1257, in step
[1,0]<stdout>:    return self.step_graph_break(inst)
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py", line 1342, in step_graph_break
[1,0]<stdout>:    self.output.compile_subgraph(
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 1399, in compile_subgraph
[1,0]<stdout>:    self.codegen_suffix(tx, stack_values_flat, pass1)
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 1477, in codegen_suffix
[1,0]<stdout>:    self.side_effects.codegen_update_mutated(cg)
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/side_effects.py", line 1065, in codegen_update_mutated
[1,0]<stdout>:    cg(value)
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/codegen.py", line 235, in __call__
[1,0]<stdout>:    raise IncorrectUsage(
[1,0]<stdout>:torch._dynamo.exc.InternalTorchDynamoError: IncorrectUsage: NYI: Returning a @contextmanager object from a torch.compile function
[1,0]<stdout>:
[1,0]<stdout>:from user code:
[1,0]<stdout>:   File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/two_batch_overlap.py", line 831, in _model_forward_tbo
[1,0]<stdout>:    with context:
[1,0]<stdout>:
[1,0]<stdout>:Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1,0]<stdout>:
[1,0]<stdout>:
[1,0]<stdout>:During handling of the above exception, another exception occurred:
[1,0]<stdout>:
[1,0]<stdout>:Traceback (most recent call last):
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2555, in run_scheduler_process
[1,0]<stdout>:    scheduler = Scheduler(
[1,0]<stdout>:                ^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 315, in __init__
[1,0]<stdout>:    self.tp_worker [1,0]<stdout>:= TpWorkerClass(
[1,0]<stdout>:                     ^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 84, in __init__
[1,0]<stdout>:    self.model_runner = ModelRunner(
[1,0]<stdout>:                        ^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 242, in __init__
[1,0]<stdout>:    self.initialize(min_per_gpu_memory)
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 342, in initialize
[1,0]<stdout>:    self.init_cuda_graphs()
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 1611, in init_cuda_graphs
[1,0]<stdout>:    self.cuda_graph_runner = CudaGraphRunner(self)
[1,0]<stdout>:                             ^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 382, in __init__
[1,0]<stdout>:    raise Exception(
[1,0]<stdout>:Exception: Capture cuda graph failed: IncorrectUsage: NYI: Returning a @contextmanager object from a torch.compile function
[1,0]<stdout>:
[1,0]<stdout>:from user code:
[1,0]<stdout>:   File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/two_batch_overlap.py", line 831, in _model_forward_tbo
[1,0]<stdout>:    with context:
[1,0]<stdout>:
[1,0]<stdout>:Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1,0]<stdout>:
[1,0]<stdout>:Possible solutions:
[1,0]<stdout>:1. set --mem-fraction-static to a smaller value (e.g., 0.8 or 0.7)
[1,0]<stdout>:2. set --cuda-graph-max-bs to a smaller value (e.g., 16)
[1,0]<stdout>:3. disable torch compile by not using --enable-torch-compile
[1,0]<stdout>:4. disable CUDA graph by --disable-cuda-graph. (Not recommended. Huge performance loss)
[1,0]<stdout>:Open an issue on GitHub https://github.com/sgl-project/sglang/issues/new/choose 
[1,0]<stdout>:
[1,0]<stdout>:
[1,0]<stdout>:[2025-10-12 02:56:35 DP0 TP2] Scheduler hit an exception: Traceback (most recent call last):
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 380, in __init__
[1,0]<stdout>:    self.capture()
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 485, in capture
[1,0]<stdout>:    ) = self.capture_one_batch_size(bs, forward)
[1,0]<stdout>:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 649, in capture_one_batch_size
[1,0]<stdout>:    run_once()
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 637, in run_once
[1,0]<stdout>:    logits_output_or_pp_proxy_tensors = forward(
[1,0]<stdout>:                                        ^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 736, in compile_wrapper
[1,0]<stdout>:    return fn(*args, **kwargs)
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/external_utils.py", line 70, in inner
[1,0]<stdout>:    return fn(*args, **kwargs)
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1,0]<stdout>:    return func(*args, **kwargs)
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1,0]<stdout>:    return func(*args, **kwargs)
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/models/deepseek_v2.py", line 2221, in forward
[1,0]<stdout>:    hidden_states = self.model(input_ids, positions, forward_batch, input_embeds)
[1,0]<stdout>:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
[1,0]<stdout>:    return self._call_impl(*args, **kwargs)
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
[1,0]<stdout>:    return forward_call(*args, **kwargs)
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/models/deepseek_v2.py", line 2109, in forward
[1,0]<stdout>:    enable_tbo=True,
[1,0]<stdout>:                     
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/two_batch_overlap.py", line 800, in model_forward_maybe_tbo
[1,0]<stdout>:    return _model_forward_tbo(
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 1495, in __call__
[1,0]<stdout>:    return self._torchdynamo_orig_callable(
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 1272, in __call__
[1,0]<stdout>:    result = self._inner_convert(
[1,0]<stdout>:             ^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 629, in __call__
[1,0]<stdout>:    return _compile(
[1,0]<stdout>:           ^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 1164, in _compile
[1,0]<stdout>:    raise InternalTorchDynamoError(
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 1111, in _[1,0]<stdout>:compile
[1,0]<stdout>:    guarded_code = compile_inner(code, one_graph, hooks, transform)
[1,0]<stdout>:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_utils_internal.py", line 97, in wrapper_function
[1,0]<stdout>:    return function(*args, **kwargs)
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 793, in compile_inner
[1,0]<stdout>:    return _compile_inner(code, one_graph, hooks, transform)
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 832, in _compile_inner
[1,0]<stdout>:    out_code = transform_code_object(code, transform)
[1,0]<stdout>:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/bytecode_transformation.py", line 1424, in transform_code_object
[1,0]<stdout>:    transformations(instructions, code_options)
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 267, in _fn
[1,0]<stdout>:    return fn(*args, **kwargs)
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 753, in transform
[1,0]<stdout>:    tracer.run()
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py", line 3497, in run
[1,0]<stdout>:    super().run()
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py", line 1363, in run
[1,0]<stdout>:    while self.step():
[1,0]<stdout>:          ^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py", line 1257, in step
[1,0]<stdout>:    return self.step_graph_break(inst)
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py", line 1342, in step_graph_break
[1,0]<stdout>:    self.output.compile_subgraph(
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 1399, in compile_subgraph
[1,0]<stdout>:    self.codegen_suffix(tx, stack_values_flat, pass1)
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 1477, in codegen_suffix
[1,0]<stdout>:    self.side_effects.codegen_update_mutated(cg)
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/side_effects.py", line 1065, in codegen_update_mutated
[1,0]<stdout>:    cg(value)
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/codegen.py", line 235, in __call__
[1,0]<stdout>:    raise IncorrectUsage(
[1,0]<stdout>:torch._dynamo.exc.InternalTorchDynamoError: IncorrectUsage: NYI: Returning a @contextmanager object from a torch.compile function
[1,0]<stdout>:
[1,0]<stdout>:from user code:
[1,0]<stdout>:   File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/two_batch_overlap.py", line 831, in _model_forward_tbo
[1,0]<stdout>:    with context:
[1,0]<stdout>:
[1,0]<stdout>:Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1,0]<stdout>:
[1,0]<stdout>:
[1,0]<stdout>:During handling of the above exception, another exception occurred:
[1,0]<stdout>:
[1,0]<stdout>:Traceback (most recent call last):
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2555, in run_scheduler_process
[1,0]<stdout>:    scheduler = Scheduler(
[1,0]<stdout>:                ^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 315, in __init__
[1,0]<stdout>:    self.tp_worker [1,0]<stdout>:= TpWorkerClass(
[1,0]<stdout>:                     ^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 84, in __init__
[1,0]<stdout>:    self.model_runner = ModelRunner(
[1,0]<stdout>:                        ^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 242, in __init__
[1,0]<stdout>:    self.initialize(min_per_gpu_memory)
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 342, in initialize
[1,0]<stdout>:    self.init_cuda_graphs()
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 1611, in init_cuda_graphs
[1,0]<stdout>:    self.cuda_graph_runner = CudaGraphRunner(self)
[1,0]<stdout>:                             ^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 382, in __init__
[1,0]<stdout>:    raise Exception(
[1,0]<stdout>:Exception: Capture cuda graph failed: IncorrectUsage: NYI: Returning a @contextmanager object from a torch.compile function
[1,0]<stdout>:
[1,0]<stdout>:from user code:
[1,0]<stdout>:   File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/two_batch_overlap.py", line 831, in _model_forward_tbo
[1,0]<stdout>:    with context:
[1,0]<stdout>:
[1,0]<stdout>:Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1,0]<stdout>:
[1,0]<stdout>:Possible solutions:
[1,0]<stdout>:1. set --mem-fraction-static to a smaller value (e.g., 0.8 or 0.7)
[1,0]<stdout>:2. set --cuda-graph-max-bs to a smaller value (e.g., 16)
[1,0]<stdout>:3. disable torch compile by not using --enable-torch-compile
[1,0]<stdout>:4. disable CUDA graph by --disable-cuda-graph. (Not recommended. Huge performance loss)
[1,0]<stdout>:Open an issue on GitHub https://github.com/sgl-project/sglang/issues/new/choose 
[1,0]<stdout>:
[1,0]<stdout>:
[1,1]<stdout>:[2025-10-12 02:56:36 DP1 TP10] Scheduler hit an exception: Traceback (most recent call last):
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 380, in __init__
[1,1]<stdout>:    self.capture()
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 485, in capture
[1,1]<stdout>:    ) = self.capture_one_batch_size(bs, forward)
[1,1]<stdout>:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 649, in capture_one_batch_size
[1,1]<stdout>:    run_once()
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 637, in run_once
[1,1]<stdout>:    logits_output_or_pp_proxy_tensors = forward(
[1,1]<stdout>:                                        ^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 736, in compile_wrapper
[1,1]<stdout>:    return fn(*args, **kwargs)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/external_utils.py", line 70, in inner
[1,1]<stdout>:    return fn(*args, **kwargs)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1,1]<stdout>:    return func(*args, **kwargs)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1,1]<stdout>:    return func(*args, **kwargs)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/models/deepseek_v2.py", line 2221, in forward
[1,1]<stdout>:    hidden_states = self.model(input_ids, positions, forward_batch, input_embeds)
[1,1]<stdout>:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
[1,1]<stdout>:    return self._call_impl(*args, **kwargs)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
[1,1]<stdout>:    return forward_call(*args, **kwargs)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/models/deepseek_v2.py", line 2109, in forward
[1,1]<stdout>:    enable_tbo=True,
[1,1]<stdout>:                     
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/two_batch_overlap.py", line 800, in model_forward_maybe_tbo
[1,1]<stdout>:    return _model_forward_tbo(
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 1495, in __call__
[1,1]<stdout>:    return self._torchdynamo_orig_callable(
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 1272, in __call__
[1,1]<stdout>:    result = self._inner_convert(
[1,1]<stdout>:             ^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 629, in __call__
[1,1]<stdout>:    return _compile(
[1,1]<stdout>:           ^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 1164, in _compile
[1,1]<stdout>:    raise InternalTorchDynamoError(
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 1111, in[1,1]<stdout>: _compile
[1,1]<stdout>:    guarded_code = compile_inner(code, one_graph, hooks, transform)
[1,1]<stdout>:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_utils_internal.py", line 97, in wrapper_function
[1,1]<stdout>:    return function(*args, **kwargs)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 793, in compile_inner
[1,1]<stdout>:    return _compile_inner(code, one_graph, hooks, transform)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 832, in _compile_inner
[1,1]<stdout>:    out_code = transform_code_object(code, transform)
[1,1]<stdout>:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/bytecode_transformation.py", line 1424, in transform_code_object
[1,1]<stdout>:    transformations(instructions, code_options)
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 267, in _fn
[1,1]<stdout>:    return fn(*args, **kwargs)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 753, in transform
[1,1]<stdout>:    tracer.run()
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py", line 3497, in run
[1,1]<stdout>:    super().run()
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py", line 1363, in run
[1,1]<stdout>:    while self.step():
[1,1]<stdout>:          ^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py", line 1257, in step
[1,1]<stdout>:    return self.step_graph_break(inst)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py", line 1342, in step_graph_break
[1,1]<stdout>:    self.output.compile_subgraph(
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 1399, in compile_subgraph
[1,1]<stdout>:    self.codegen_suffix(tx, stack_values_flat, pass1)
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 1477, in codegen_suffix
[1,1]<stdout>:    self.side_effects.codegen_update_mutated(cg)
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/side_effects.py", line 1065, in codegen_update_mutated
[1,1]<stdout>:    cg(value)
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/_dynamo/codegen.py", line 235, in __call__
[1,1]<stdout>:    raise IncorrectUsage(
[1,1]<stdout>:torch._dynamo.exc.InternalTorchDynamoError: IncorrectUsage: NYI: Returning a @contextmanager object from a torch.compile function
[1,1]<stdout>:
[1,1]<stdout>:from user code:
[1,1]<stdout>:   File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/two_batch_overlap.py", line 831, in _model_forward_tbo
[1,1]<stdout>:    with context:
[1,1]<stdout>:
[1,1]<stdout>:Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1,1]<stdout>:
[1,1]<stdout>:
[1,1]<stdout>:During handling of the above exception, another exception occurred:
[1,1]<stdout>:
[1,1]<stdout>:Traceback (most recent call last):
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2555, in run_scheduler_process
[1,1]<stdout>:    scheduler = Scheduler(
[1,1]<stdout>:                ^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 315, in __init__
[1,1]<stdout>:    self.tp_worke[1,1]<stdout>:r [1,1]<stdout>:= TpWorkerClass(
[1,1]<stdout>:                     ^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 84, in __init__
[1,1]<stdout>:    self.model_runner = ModelRunner(
[1,1]<stdout>:                        ^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 242, in __init__
[1,1]<stdout>:    self.initialize(min_per_gpu_memory)
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 342, in initialize
[1,1]<stdout>:    self.init_cuda_graphs()
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 1611, in init_cuda_graphs
[1,1]<stdout>:    self.cuda_graph_runner = CudaGraphRunner(self)
[1,1]<stdout>:                             ^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 382, in __init__
[1,1]<stdout>:    raise Exception(
[1,1]<stdout>:Exception: Capture cuda graph failed: IncorrectUsage: NYI: Returning a @contextmanager object from a torch.compile function
[1,1]<stdout>:
[1,1]<stdout>:from user code:
[1,1]<stdout>:   File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/two_batch_overlap.py", line 831, in _model_forward_tbo
[1,1]<stdout>:    with context:
[1,1]<stdout>:
[1,1]<stdout>:Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1,1]<stdout>:
[1,1]<stdout>:Possible solutions:
[1,1]<stdout>:1. set --mem-fraction-static to a smaller value (e.g., 0.8 or 0.7)
[1,1]<stdout>:2. set --cuda-graph-max-bs to a smaller value (e.g., 16)
[1,1]<stdout>:3. disable torch compile by not using --enable-torch-compile
[1,1]<stdout>:4. disable CUDA graph by --disable-cuda-graph. (Not recommended. Huge performance loss)
[1,1]<stdout>:Open an issue on GitHub https://github.com/sgl-project/sglang/issues/new/choose 
[1,1]<stdout>:
[1,1]<stdout>:
=>> PBS: job killed: walltime 602 exceeded limit 600
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
			Resource Usage on 2025-10-12 03:04:35.174462:
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	JobId: 97119.pbs111
	Project: 50000128
	Exit Status: -29
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	NCPUs: Requested(224), Used(224)
	CPU Time Used: 00:18:04
	Memory: Requested(3760gb), Used(34466648kb)
	Vmem Used: 76100811976kb
	Walltime: Requested(00:10:00), Used(00:10:16)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	Execution Nodes Used: (a2ap-dgx007:ncpus=112:ngpus=8:mem=1971322880kb)+(a2ap-dgx010:ncpus=112:ngpus=8:mem=1971322880kb)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	GPU Duration: 10.39mins
	GPU Power Consumed: 156.89W
	GPU Max GPU Memory Used: 1.24TB
	Memory Throughput Rate (Average): a2ap-dgx007:(gpu1:0%+gpu0:0%+gpu2:0%+gpu3:0%+gpu5:0%+gpu4:0%+gpu6:0%+gpu7:0%)+a2ap-dgx010:(gpu1:0%+gpu0:0%+gpu2:0%+gpu3:0%+gpu5:0%+gpu4:0%+gpu6:0%+gpu7:0%)
	Memory Throughput Rate (Max): a2ap-dgx007:(gpu1:7%+gpu0:29%+gpu2:1%+gpu3:17%+gpu5:2%+gpu4:42%+gpu6:2%+gpu7:8%)+a2ap-dgx010:(gpu1:2%+gpu0:2%+gpu2:2%+gpu3:2%+gpu5:4%+gpu4:2%+gpu6:0%+gpu7:0%)
	Memory Throughput Rate (Min): a2ap-dgx007:(gpu1:0%+gpu0:0%+gpu2:0%+gpu3:0%+gpu5:0%+gpu4:0%+gpu6:0%+gpu7:0%)+a2ap-dgx010:(gpu1:0%+gpu0:0%+gpu2:0%+gpu3:0%+gpu5:0%+gpu4:0%+gpu6:0%+gpu7:0%)
	GPU SM Utilization (Average): a2ap-dgx007:(gpu1:0%+gpu0:0%+gpu2:0%+gpu3:0%+gpu5:0%+gpu4:0%+gpu6:1%+gpu7:0%)+a2ap-dgx010:(gpu1:1%+gpu0:0%+gpu2:0%+gpu3:0%+gpu5:0%+gpu4:1%+gpu6:0%+gpu7:0%)
	GPU SM Utilization (Max): a2ap-dgx007:(gpu1:7%+gpu0:35%+gpu2:5%+gpu3:29%+gpu5:16%+gpu4:28%+gpu6:43%+gpu7:9%)+a2ap-dgx010:(gpu1:53%+gpu0:11%+gpu2:3%+gpu3:26%+gpu5:2%+gpu4:81%+gpu6:4%+gpu7:7%)
	GPU SM Utilization (Min): a2ap-dgx007:(gpu1:0%+gpu0:0%+gpu2:0%+gpu3:0%+gpu5:0%+gpu4:0%+gpu6:0%+gpu7:0%)+a2ap-dgx010:(gpu1:0%+gpu0:0%+gpu2:0%+gpu3:0%+gpu5:0%+gpu4:0%+gpu6:0%+gpu7:0%)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Warning: GPUs 0, 2, 3, 5, 6, 7 have a percentage of 0 utilisation.
GPU application profile: Low
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

