========== FA3 OPTIMIZED BENCHMARK ==========
Target: <15min | Est. SU: 238.934 | Balance: 40054.830
N/A
Job ID: 97120.pbs111 | GPUs: 16 | Master: a2ap-dgx007.asp2p.nscc.sg:5000
Config: TP16+DP2 | Attention: FlashAttention-3 | NCCL: Optimized
=============================================
[03:09:35] Launching FA3-optimized benchmark...
[1,0]<stdout>:WARNING:sglang.srt.server_args:DP attention is enabled. The chunked prefill size is adjusted to 4096 to avoid MoE kernel issues. 
[1,0]<stdout>:WARNING:sglang.srt.server_args:Overlap scheduler is disabled because of using eagle speculative decoding.
[1,0]<stdout>:WARNING:sglang.srt.server_args:Mixed chunked prefill is disabled because of using eagle speculative decoding.
[1,0]<stdout>:WARNING:sglang.srt.server_args:DP attention is enabled. The chunked prefill size is adjusted to 2048 to avoid MoE kernel issues. 
[1,0]<stdout>:WARNING:sglang.srt.server_args:Overlap scheduler is disabled because of using eagle speculative decoding.
[1,0]<stdout>:WARNING:sglang.srt.server_args:DeepSeek MTP does not require setting speculative_draft_model_path.
[1,0]<stdout>:[2025-10-12 03:09:54] Using default HuggingFace chat template with detected content format: string
[1,1]<stdout>:WARNING:sglang.srt.server_args:DP attention is enabled. The chunked prefill size is adjusted to 4096 to avoid MoE kernel issues. 
[1,1]<stdout>:WARNING:sglang.srt.server_args:Overlap scheduler is disabled because of using eagle speculative decoding.
[1,1]<stdout>:WARNING:sglang.srt.server_args:Mixed chunked prefill is disabled because of using eagle speculative decoding.
[1,1]<stdout>:WARNING:sglang.srt.server_args:DP attention is enabled. The chunked prefill size is adjusted to 2048 to avoid MoE kernel issues. 
[1,1]<stdout>:WARNING:sglang.srt.server_args:Overlap scheduler is disabled because of using eagle speculative decoding.
[1,1]<stdout>:WARNING:sglang.srt.server_args:DeepSeek MTP does not require setting speculative_draft_model_path.
[1,0]<stdout>:[2025-10-12 03:10:37 DP0 TP0] MLA optimization is turned on. Use fa3 backend.
[1,0]<stdout>:[2025-10-12 03:10:37 DP0 TP0] Chunked prefix cache is turned on.
[1,0]<stdout>:[2025-10-12 03:10:37 DP0 TP0] Init torch distributed begin.
[1,0]<stdout>:[W1012 03:10:39.535937415 Utils.hpp:137] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[1,0]<stdout>:[W1012 03:10:39.535973743 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[1,0]<stdout>:[W1012 03:10:40.097458184 Utils.hpp:137] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[1,0]<stdout>:[W1012 03:10:40.097489363 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[1,1]<stdout>:[W1012 03:10:40.435892580 Utils.hpp:137] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[1,1]<stdout>:[W1012 03:10:40.435926845 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[1,0]<stdout>:[W1012 03:10:41.874017641 Utils.hpp:137] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[1,0]<stdout>:[W1012 03:10:41.874047538 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[1,0]<stdout>:[W1012 03:10:41.020530428 Utils.hpp:137] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[1,0]<stdout>:[W1012 03:10:41.020567788 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[1,0]<stdout>:[W1012 03:10:41.028635644 Utils.hpp:137] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[1,0]<stdout>:[W1012 03:10:41.028660334 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[1,0]<stdout>:[W1012 03:10:42.161683263 Utils.hpp:137] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[1,0]<stdout>:[W1012 03:10:42.161706740 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[1,0]<stdout>:[W1012 03:10:42.208957779 Utils.hpp:137] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[1,0]<stdout>:[W1012 03:10:42.208977556 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[1,1]<stdout>:[W1012 03:10:42.937678977 Utils.hpp:137] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[1,1]<stdout>:[W1012 03:10:42.937719976 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[1,1]<stdout>:[W1012 03:10:44.170753477 Utils.hpp:137] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[1,1]<stdout>:[W1012 03:10:44.170782788 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[1,1]<stdout>:[W1012 03:10:44.212176164 Utils.hpp:137] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[1,1]<stdout>:[W1012 03:10:44.212200363 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[1,1]<stdout>:[W1012 03:10:44.212281502 Utils.hpp:137] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[1,1]<stdout>:[W1012 03:10:44.212302377 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[1,1]<stdout>:[W1012 03:10:44.390452575 Utils.hpp:137] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[1,1]<stdout>:[W1012 03:10:44.390476021 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[1,1]<stdout>:[W1012 03:10:44.424642628 Utils.hpp:137] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[1,1]<stdout>:[W1012 03:10:44.424666740 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[1,1]<stdout>:[W1012 03:10:44.455611107 Utils.hpp:137] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[1,1]<stdout>:[W1012 03:10:44.455633971 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[1,0]<stdout>:[W1012 03:10:44.962458002 Utils.hpp:137] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[1,0]<stdout>:[W1012 03:10:44.962490861 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[1,0]<stdout>:[Gloo] Rank 0 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,0]<stdout>:[Gloo] Rank 1 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,0]<stdout>:[Gloo] Rank 2 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,0]<stdout>:[Gloo] Rank 5 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,0]<stdout>:[Gloo] Rank 4 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,1]<stdout>:[Gloo] Rank 11 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,1]<stdout>:[Gloo] Rank 9 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,1]<stdout>:[Gloo] Rank 10 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,0]<stdout>:[Gloo] Rank 6 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,0]<stdout>:[Gloo] Rank 3 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,1]<stdout>:[Gloo] Rank 14 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,1]<stdout>:[Gloo] Rank 12 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,1]<stdout>:[Gloo] Rank 8 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,0]<stdout>:[Gloo] Rank 7 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,1]<stdout>:[Gloo] Rank 13 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,1]<stdout>:[Gloo] Rank 15 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,0]<stdout>:[Gloo] Rank 0 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,0]<stdout>:[2025-10-12 03:10:44 DP0 TP0] sglang is using nccl==2.27.3
[1,1]<stdout>:[Gloo] Rank 8 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,1]<stdout>:[Gloo] Rank 9 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,1]<stdout>:[Gloo] Rank 10 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,1]<stdout>:[Gloo] Rank 12 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,1]<stdout>:[Gloo] Rank 11 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,1]<stdout>:[Gloo] Rank 13 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,1]<stdout>:[Gloo] Rank 14 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,1]<stdout>:[Gloo] Rank 15 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,0]<stdout>:[Gloo] Rank 5 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,0]<stdout>:[Gloo] Rank 3 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,0]<stdout>:[Gloo] Rank 2 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,0]<stdout>:[Gloo] Rank 1 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,0]<stdout>:[Gloo] Rank 6 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,0]<stdout>:[Gloo] Rank 7 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,0]<stdout>:[Gloo] Rank 4 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,0]<stdout>:NCCL version 2.27.3+cuda12.9
[1,0]<stdout>:
[1,0]<stdout>:[2025-10-12 03:10:50] a2ap-dgx007:1217033:1217033 [5] transport/net_ib.cc:587 NCCL WARN NET/IB : Attempted to merge incompatible devices: [1]mlx5_1:1/IB and [2]mlx5_2:1/RoCE. Try selecting NICs of only one link type using NCCL_IB_HCA
[1,0]<stdout>:
[1,0]<stdout>:[2025-10-12 03:10:50] a2ap-dgx007:1217033:1217033 [5] transport/net_ib.cc:587 NCCL WARN NET/IB : Attempted to merge incompatible devices: [7]mlx5_7:1/IB and [8]mlx5_8:1/RoCE. Try selecting NICs of only one link type using NCCL_IB_HCA
[1,0]<stdout>:
[1,0]<stdout>:[2025-10-12 03:10:50] a2ap-dgx007:1217032:1217032 [4] transport/net_ib.cc:587 NCCL WARN NET/IB : Attempted to merge incompatible devices: [1]mlx5_1:1/IB and [2]mlx5_2:1/RoCE. Try selecting NICs of only one link type using NCCL_IB_HCA
[1,0]<stdout>:
[1,0]<stdout>:[2025-10-12 03:10:50] a2ap-dgx007:1217032:1217032 [4] transport/net_ib.cc:587 NCCL WARN NET/IB : Attempted to merge incompatible devices: [7]mlx5_7:1/IB and [8]mlx5_8:1/RoCE. Try selecting NICs of only one link type using NCCL_IB_HCA
[1,0]<stdout>:
[1,0]<stdout>:[2025-10-12 03:10:50] a2ap-dgx007:1217035:1217035 [7] transport/net_ib.cc:587 NCCL WARN NET/IB : Attempted to merge incompatible devices: [1]mlx5_1:1/IB and [2]mlx5_2:1/RoCE. Try selecting NICs of only one link type using NCCL_IB_HCA
[1,0]<stdout>:
[1,0]<stdout>:[2025-10-12 03:10:50] a2ap-dgx007:1217035:1217035 [7] transport/net_ib.cc:587 NCCL WARN NET/IB : Attempted to merge incompatible devices: [7]mlx5_7:1/IB and [8]mlx5_8:1/RoCE. Try selecting NICs of only one link type using NCCL_IB_HCA
[1,0]<stdout>:
[1,0]<stdout>:[2025-10-12 03:10:50] a2ap-dgx007:1217029:1217029 [1] transport/net_ib.cc:587 NCCL WARN NET/IB : Attempted to merge incompatible devices: [1]mlx5_1:1/IB and [2]mlx5_2:1/RoCE. Try selecting NICs of only one link type using NCCL_IB_HCA
[1,0]<stdout>:
[1,0]<stdout>:[2025-10-12 03:10:50] a2ap-dgx007:1217029:1217029 [1] transport/net_ib.cc:587 NCCL WARN NET/IB : Attempted to merge incompatible devices: [7]mlx5_7:1/IB and [8]mlx5_8:1/RoCE. Try selecting NICs of only one link type using NCCL_IB_HCA
[1,0]<stdout>:
[1,0]<stdout>:[2025-10-12 03:10:50] a2ap-dgx007:1217031:1217031 [3] transport/net_ib.cc:587 NCCL WARN NET/IB : Attempted to merge incompatible devices: [1]mlx5_1:1/IB and [2]mlx5_2:1/RoCE. Try selecting NICs of only one link type using NCCL_IB_HCA
[1,0]<stdout>:
[1,0]<stdout>:[2025-10-12 03:10:50] a2ap-dgx007:1217031:1217031 [3] transport/net_ib.cc:587 NCCL WARN NET/IB : Attempted to merge incompatible devices: [7]mlx5_7:1/IB and [8]mlx5_8:1/RoCE. Try selecting NICs of only one link type using NCCL_IB_HCA
[1,0]<stdout>:
[1,0]<stdout>:[2025-10-12 03:10:50] a2ap-dgx007:1217034:1217034 [6] transport/net_ib.cc:587 NCCL WARN NET/IB : Attempted to merge incompatible devices: [1]mlx5_1:1/IB and [2]mlx5_2:1/RoCE. Try selecting NICs of only one link type using NCCL_IB_HCA
[1,0]<stdout>:
[1,0]<stdout>:[2025-10-12 03:10:50] a2ap-dgx007:1217034:1217034 [6] transport/net_ib.cc:587 NCCL WARN NET/IB : Attempted to merge incompatible devices: [7]mlx5_7:1/IB and [8]mlx5_8:1/RoCE. Try selecting NICs of only one link type using NCCL_IB_HCA
[1,0]<stdout>:
[1,0]<stdout>:[2025-10-12 03:10:50] a2ap-dgx007:1217028:1217028 [0] transport/net_ib.cc:587 NCCL WARN NET/IB : Attempted to merge incompatible devices: [1]mlx5_1:1/IB and [2]mlx5_2:1/RoCE. Try selecting NICs of only one link type using NCCL_IB_HCA
[1,0]<stdout>:
[1,0]<stdout>:[2025-10-12 03:10:50] a2ap-dgx007:1217028:1217028 [0] transport/net_ib.cc:587 NCCL WARN NET/IB : Attempted to merge incompatible devices: [7]mlx5_7:1/IB and [8]mlx5_8:1/RoCE. Try selecting NICs of only one link type using NCCL_IB_HCA
[1,0]<stdout>:
[1,0]<stdout>:[2025-10-12 03:10:50] a2ap-dgx007:1217030:1217030 [2] transport/net_ib.cc:587 NCCL WARN NET/IB : Attempted to merge incompatible devices: [1]mlx5_1:1/IB and [2]mlx5_2:1/RoCE. Try selecting NICs of only one link type using NCCL_IB_HCA
[1,0]<stdout>:
[1,0]<stdout>:[2025-10-12 03:10:50] a2ap-dgx007:1217030:1217030 [2] transport/net_ib.cc:587 NCCL WARN NET/IB : Attempted to merge incompatible devices: [7]mlx5_7:1/IB and [8]mlx5_8:1/RoCE. Try selecting NICs of only one link type using NCCL_IB_HCA
[1,1]<stdout>:
[1,1]<stdout>:[2025-10-12 03:10:51] a2ap-dgx010:753858:753858 [4] transport/net_ib.cc:587 NCCL WARN NET/IB : Attempted to merge incompatible devices: [1]mlx5_1:1/IB and [2]mlx5_2:1/RoCE. Try selecting NICs of only one link type using NCCL_IB_HCA
[1,1]<stdout>:
[1,1]<stdout>:[2025-10-12 03:10:51] a2ap-dgx010:753858:753858 [4] transport/net_ib.cc:587 NCCL WARN NET/IB : Attempted to merge incompatible devices: [7]mlx5_7:1/IB and [8]mlx5_8:1/RoCE. Try selecting NICs of only one link type using NCCL_IB_HCA
[1,1]<stdout>:
[1,1]<stdout>:[2025-10-12 03:10:51] a2ap-dgx010:753854:753854 [0] transport/net_ib.cc:587 NCCL WARN NET/IB : Attempted to merge incompatible devices: [1]mlx5_1:1/IB and [2]mlx5_2:1/RoCE. Try selecting NICs of only one link type using NCCL_IB_HCA
[1,1]<stdout>:
[1,1]<stdout>:[2025-10-12 03:10:51] a2ap-dgx010:753854:753854 [0] transport/net_ib.cc:587 NCCL WARN NET/IB : Attempted to merge incompatible devices: [7]mlx5_7:1/IB and [8]mlx5_8:1/RoCE. Try selecting NICs of only one link type using NCCL_IB_HCA
[1,1]<stdout>:
[1,1]<stdout>:[2025-10-12 03:10:51] a2ap-dgx010:753861:753861 [7] transport/net_ib.cc:587 NCCL WARN NET/IB : Attempted to merge incompatible devices: [1]mlx5_1:1/IB and [2]mlx5_2:1/RoCE. Try selecting NICs of only one link type using NCCL_IB_HCA
[1,1]<stdout>:
[1,1]<stdout>:[2025-10-12 03:10:51] a2ap-dgx010:753861:753861 [7] transport/net_ib.cc:587 NCCL WARN NET/IB : Attempted to merge incompatible devices: [7]mlx5_7:1/IB and [8]mlx5_8:1/RoCE. Try selecting NICs of only one link type using NCCL_IB_HCA
[1,1]<stdout>:
[1,1]<stdout>:[2025-10-12 03:10:51] a2ap-dgx010:753856:753856 [2] transport/net_ib.cc:587 NCCL WARN NET/IB : Attempted to merge incompatible devices: [1]mlx5_1:1/IB and [2]mlx5_2:1/RoCE. Try selecting NICs of only one link type using NCCL_IB_HCA
[1,1]<stdout>:
[1,1]<stdout>:[2025-10-12 03:10:51] a2ap-dgx010:753856:753856 [2] transport/net_ib.cc:587 NCCL WARN NET/IB : Attempted to merge incompatible devices: [7]mlx5_7:1/IB and [8]mlx5_8:1/RoCE. Try selecting NICs of only one link type using NCCL_IB_HCA
[1,1]<stdout>:
[1,1]<stdout>:[2025-10-12 03:10:51] a2ap-dgx010:753855:753855 [1] transport/net_ib.cc:587 NCCL WARN NET/IB : Attempted to merge incompatible devices: [1]mlx5_1:1/IB and [2]mlx5_2:1/RoCE. Try selecting NICs of only one link type using NCCL_IB_HCA
[1,1]<stdout>:
[1,1]<stdout>:[2025-10-12 03:10:51] a2ap-dgx010:753855:753855 [1] transport/net_ib.cc:587 NCCL WARN NET/IB : Attempted to merge incompatible devices: [7]mlx5_7:1/IB and [8]mlx5_8:1/RoCE. Try selecting NICs of only one link type using NCCL_IB_HCA
[1,1]<stdout>:
[1,1]<stdout>:[2025-10-12 03:10:51] a2ap-dgx010:753859:753859 [5] transport/net_ib.cc:587 NCCL WARN NET/IB : Attempted to merge incompatible devices: [1]mlx5_1:1/IB and [2]mlx5_2:1/RoCE. Try selecting NICs of only one link type using NCCL_IB_HCA
[1,1]<stdout>:
[1,1]<stdout>:[2025-10-12 03:10:51] a2ap-dgx010:753859:753859 [5] transport/net_ib.cc:587 NCCL WARN NET/IB : Attempted to merge incompatible devices: [7]mlx5_7:1/IB and [8]mlx5_8:1/RoCE. Try selecting NICs of only one link type using NCCL_IB_HCA
[1,1]<stdout>:
[1,1]<stdout>:[2025-10-12 03:10:51] a2ap-dgx010:753860:753860 [6] transport/net_ib.cc:587 NCCL WARN NET/IB : Attempted to merge incompatible devices: [1]mlx5_1:1/IB and [2]mlx5_2:1/RoCE. Try selecting NICs of only one link type using NCCL_IB_HCA
[1,1]<stdout>:
[1,1]<stdout>:[2025-10-12 03:10:51] a2ap-dgx010:753860:753860 [6] transport/net_ib.cc:587 NCCL WARN NET/IB : Attempted to merge incompatible devices: [7]mlx5_7:1/IB and [8]mlx5_8:1/RoCE. Try selecting NICs of only one link type using NCCL_IB_HCA
[1,1]<stdout>:
[1,1]<stdout>:[2025-10-12 03:10:51] a2ap-dgx010:753857:753857 [3] transport/net_ib.cc:587 NCCL WARN NET/IB : Attempted to merge incompatible devices: [1]mlx5_1:1/IB and [2]mlx5_2:1/RoCE. Try selecting NICs of only one link type using NCCL_IB_HCA
[1,1]<stdout>:
[1,1]<stdout>:[2025-10-12 03:10:51] a2ap-dgx010:753857:753857 [3] transport/net_ib.cc:587 NCCL WARN NET/IB : Attempted to merge incompatible devices: [7]mlx5_7:1/IB and [8]mlx5_8:1/RoCE. Try selecting NICs of only one link type using NCCL_IB_HCA
[1,0]<stdout>:[2025-10-12 03:10:52 DP0 TP3] Custom allreduce is disabled because this process group spans across nodes.
[1,0]<stdout>:[2025-10-12 03:10:52 DP0 TP1] Custom allreduce is disabled because this process group spans across nodes.
[1,0]<stdout>:[2025-10-12 03:10:52 DP0 TP2] Custom allreduce is disabled because this process group spans across nodes.
[1,0]<stdout>:[2025-10-12 03:10:52 DP0 TP5] Custom allreduce is disabled because this process group spans across nodes.
[1,0]<stdout>:[2025-10-12 03:10:52 DP0 TP0] Custom allreduce is disabled because this process group spans across nodes.
[1,0]<stdout>:[2025-10-12 03:10:52 DP0 TP4] Custom allreduce is disabled because this process group spans across nodes.
[1,0]<stdout>:[2025-10-12 03:10:52 DP0 TP6] Custom allreduce is disabled because this process group spans across nodes.
[1,0]<stdout>:[2025-10-12 03:10:52 DP0 TP7] Custom allreduce is disabled because this process group spans across nodes.
[1,1]<stdout>:[2025-10-12 03:10:52 DP1 TP14] Custom allreduce is disabled because this process group spans across nodes.
[1,1]<stdout>:[2025-10-12 03:10:52 DP1 TP12] Custom allreduce is disabled because this process group spans across nodes.
[1,1]<stdout>:[2025-10-12 03:10:52 DP1 TP15] Custom allreduce is disabled because this process group spans across nodes.
[1,1]<stdout>:[2025-10-12 03:10:52 DP1 TP13] Custom allreduce is disabled because this process group spans across nodes.
[1,1]<stdout>:[2025-10-12 03:10:52 DP1 TP10] Custom allreduce is disabled because this process group spans across nodes.
[1,1]<stdout>:[2025-10-12 03:10:52 DP1 TP11] Custom allreduce is disabled because this process group spans across nodes.
[1,1]<stdout>:[2025-10-12 03:10:52 DP1 TP9] Custom allreduce is disabled because this process group spans across nodes.
[1,1]<stdout>:[2025-10-12 03:10:52 DP1 TP8] Custom allreduce is disabled because this process group spans across nodes.
[1,0]<stdout>:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1,0]<stdout>:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1,0]<stdout>:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1,0]<stdout>:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1,0]<stdout>:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1,0]<stdout>:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1,0]<stdout>:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1,0]<stdout>:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1,1]<stdout>:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1,1]<stdout>:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1,1]<stdout>:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1,1]<stdout>:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1,1]<stdout>:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1,1]<stdout>:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1,1]<stdout>:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1,1]<stdout>:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1,1]<stdout>:[Gloo] Rank 14 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,1]<stdout>:[Gloo] Rank 13 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,1]<stdout>:[Gloo] Rank 10 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,0]<stdout>:[Gloo] Rank 0 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,0]<stdout>:[Gloo] Rank 1 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,0]<stdout>:[Gloo] Rank 2 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,0]<stdout>:[2025-10-12 03:10:52 DP0 TP0] sglang is using nccl==2.27.3
[1,1]<stdout>:[Gloo] Rank 15 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,0]<stdout>:[Gloo] Rank 3 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,1]<stdout>:[Gloo] Rank 12 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,0]<stdout>:[Gloo] Rank 4 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,0]<stdout>:[Gloo] Rank 5 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,0]<stdout>:[Gloo] Rank 6 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,0]<stdout>:[Gloo] Rank 7 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,1]<stdout>:[Gloo] Rank 8 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,1]<stdout>:[Gloo] Rank 9 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,1]<stdout>:[Gloo] Rank 11 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
[1,0]<stdout>:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1,1]<stdout>:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1,1]<stdout>:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1,1]<stdout>:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1,1]<stdout>:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1,1]<stdout>:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1,1]<stdout>:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1,1]<stdout>:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1,1]<stdout>:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1,0]<stdout>:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1,0]<stdout>:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1,0]<stdout>:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1,0]<stdout>:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1,0]<stdout>:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1,0]<stdout>:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1,0]<stdout>:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1,1]<stdout>:[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[1,1]<stdout>:[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[1,1]<stdout>:[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[1,1]<stdout>:[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[1,1]<stdout>:[Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[1,1]<stdout>:[Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[1,1]<stdout>:[Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[1,1]<stdout>:[Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[1,0]<stdout>:[Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[1,0]<stdout>:[Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[1,0]<stdout>:[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[1,0]<stdout>:[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[1,0]<stdout>:[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[1,0]<stdout>:[Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[1,0]<stdout>:[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[1,0]<stdout>:[Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[1,0]<stdout>:[2025-10-12 03:10:55 DP0 TP0] Init torch distributed ends. mem usage=1.75 GB
[1,0]<stdout>:[2025-10-12 03:10:55 DP0 TP4] Ignore import error when loading sglang.srt.models.glm4v_moe: No module named 'transformers.models.glm4v_moe'
[1,0]<stdout>:[2025-10-12 03:10:55 DP0 TP2] Ignore import error when loading sglang.srt.models.glm4v_moe: No module named 'transformers.models.glm4v_moe'
[1,1]<stdout>:[2025-10-12 03:10:55 DP1 TP8] Ignore import error when loading sglang.srt.models.glm4v_moe: No module named 'transformers.models.glm4v_moe'
[1,1]<stdout>:[2025-10-12 03:10:55 DP1 TP14] Ignore import error when loading sglang.srt.models.glm4v_moe: No module named 'transformers.models.glm4v_moe'
[1,1]<stdout>:[2025-10-12 03:10:55 DP1 TP9] Ignore import error when loading sglang.srt.models.glm4v_moe: No module named 'transformers.models.glm4v_moe'
[1,1]<stdout>:[2025-10-12 03:10:55 DP1 TP12] Ignore import error when loading sglang.srt.models.glm4v_moe: No module named 'transformers.models.glm4v_moe'
[1,1]<stdout>:[2025-10-12 03:10:55 DP1 TP13] Ignore import error when loading sglang.srt.models.glm4v_moe: No module named 'transformers.models.glm4v_moe'
[1,1]<stdout>:[2025-10-12 03:10:55 DP1 TP15] Ignore import error when loading sglang.srt.models.glm4v_moe: No module named 'transformers.models.glm4v_moe'
[1,1]<stdout>:[2025-10-12 03:10:55 DP1 TP10] Ignore import error when loading sglang.srt.models.glm4v_moe: No module named 'transformers.models.glm4v_moe'
[1,1]<stdout>:[2025-10-12 03:10:55 DP1 TP11] Ignore import error when loading sglang.srt.models.glm4v_moe: No module named 'transformers.models.glm4v_moe'
[1,0]<stdout>:[2025-10-12 03:10:55 DP0 TP3] Ignore import error when loading sglang.srt.models.glm4v_moe: No module named 'transformers.models.glm4v_moe'
[1,0]<stdout>:[2025-10-12 03:10:55 DP0 TP0] Ignore import error when loading sglang.srt.models.glm4v_moe: No module named 'transformers.models.glm4v_moe'
[1,0]<stdout>:[2025-10-12 03:10:55 DP0 TP1] Ignore import error when loading sglang.srt.models.glm4v_moe: No module named 'transformers.models.glm4v_moe'
[1,0]<stdout>:[2025-10-12 03:10:55 DP0 TP5] Ignore import error when loading sglang.srt.models.glm4v_moe: No module named 'transformers.models.glm4v_moe'
[1,0]<stdout>:[2025-10-12 03:10:55 DP0 TP6] Ignore import error when loading sglang.srt.models.glm4v_moe: No module named 'transformers.models.glm4v_moe'
[1,0]<stdout>:[2025-10-12 03:10:55 DP0 TP7] Ignore import error when loading sglang.srt.models.glm4v_moe: No module named 'transformers.models.glm4v_moe'
[1,0]<stdout>:[2025-10-12 03:10:56 DP0 TP0] Load weight begin. avail mem=76.79 GB
[1,0]<stdout>:[2025-10-12 03:10:56 DP0 TP0] Detected fp8 checkpoint.
[1,0]<stdout>:[2025-10-12 03:11:15 DP0 TP0] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=32.73 GB, mem usage=44.05 GB.
[1,0]<stdout>:[2025-10-12 03:11:18 DP0 TP2] KV Cache is allocated. #tokens: 153719, KV size: 10.06 GB
[1,0]<stdout>:[2025-10-12 03:11:18 DP0 TP1] KV Cache is allocated. #tokens: 153719, KV size: 10.06 GB
[1,0]<stdout>:[2025-10-12 03:11:19 DP0 TP3] KV Cache is allocated. #tokens: 153719, KV size: 10.06 GB
[1,0]<stdout>:[2025-10-12 03:11:19 DP0 TP5] KV Cache is allocated. #tokens: 153719, KV size: 10.06 GB
[1,0]<stdout>:[2025-10-12 03:11:19 DP0 TP7] KV Cache is allocated. #tokens: 153719, KV size: 10.06 GB
[1,0]<stdout>:[2025-10-12 03:11:19 DP0 TP4] KV Cache is allocated. #tokens: 153719, KV size: 10.06 GB
[1,0]<stdout>:[2025-10-12 03:11:19 DP0 TP6] KV Cache is allocated. #tokens: 153719, KV size: 10.06 GB
[1,0]<stdout>:[2025-10-12 03:11:19 DP0 TP0] KV Cache is allocated. #tokens: 153719, KV size: 10.06 GB
[1,0]<stdout>:[2025-10-12 03:11:19 DP0 TP0] Memory pool end. avail mem=22.25 GB
[1,1]<stdout>:[2025-10-12 03:11:19 DP1 TP8] KV Cache is allocated. #tokens: 153719, KV size: 10.06 GB
[1,1]<stdout>:[2025-10-12 03:11:19 DP1 TP14] KV Cache is allocated. #tokens: 153719, KV size: 10.06 GB
[1,1]<stdout>:[2025-10-12 03:11:19 DP1 TP13] KV Cache is allocated. #tokens: 153719, KV size: 10.06 GB
[1,1]<stdout>:[2025-10-12 03:11:19 DP1 TP15] KV Cache is allocated. #tokens: 153719, KV size: 10.06 GB
[1,0]<stdout>:[2025-10-12 03:11:19 DP0 TP0] Capture cuda graph begin. This can take up to several minutes. avail mem=22.18 GB
[1,1]<stdout>:[2025-10-12 03:11:19 DP1 TP12] KV Cache is allocated. #tokens: 153719, KV size: 10.06 GB
[1,1]<stdout>:[2025-10-12 03:11:19 DP1 TP11] KV Cache is allocated. #tokens: 153719, KV size: 10.06 GB
[1,1]<stdout>:[2025-10-12 03:11:19 DP1 TP10] KV Cache is allocated. #tokens: 153719, KV size: 10.06 GB
[1,1]<stdout>:[2025-10-12 03:11:20 DP1 TP9] KV Cache is allocated. #tokens: 153719, KV size: 10.06 GB
[1,0]<stdout>:[2025-10-12 03:11:20 DP0 TP0] Capture cuda graph bs [16, 32]
[1,0]<stdout>:  0% 0/2 [00:00<?, ?it/s][1,0]<stdout>:Capturing batches (bs=32 avail_mem=21.84 GB):   0% 0/2 [00:00<?, ?it/s][1,0]<stdout>:[2025-10-12 03:11:22 DP0 TP2] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=2112, K=7168, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-12 03:11:22 DP0 TP0] Entering DeepGEMM JIT Pre-Compile session. It may takes a long time (typically 10-20 mins) if you have not run `sglang.compile_deep_gemm`. It is recommended to run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to reduce the overhead if you have not run it before. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[1,0]<stdout>:[2025-10-12 03:11:22 DP0 TP5] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=2112, K=7168, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-12 03:11:22 DP0 TP0] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=2112, K=7168, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-12 03:11:22 DP0 TP4] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=2112, K=7168, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-12 03:11:22 DP0 TP6] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=2112, K=7168, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-12 03:11:22 DP1 TP14] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=2112, K=7168, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-12 03:11:22 DP1 TP12] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=2112, K=7168, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-12 03:11:22 DP0 TP1] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=2112, K=7168, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-12 03:11:22 DP1 TP9] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=2112, K=7168, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-12 03:11:22 DP0 TP3] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=2112, K=7168, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-12 03:11:22 DP0 TP7] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=2112, K=7168, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-12 03:11:22 DP1 TP15] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=2112, K=7168, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-12 03:11:22 DP1 TP11] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=2112, K=7168, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-12 03:11:22 DP1 TP13] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=2112, K=7168, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-12 03:11:22 DP1 TP10] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=2112, K=7168, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-12 03:11:22 DP1 TP8] Entering DeepGEMM JIT Pre-Compile session. It may takes a long time (typically 10-20 mins) if you have not run `sglang.compile_deep_gemm`. It is recommended to run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to reduce the overhead if you have not run it before. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[1,1]<stdout>:[2025-10-12 03:11:22 DP1 TP8] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=2112, K=7168, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:  0% 0/33 [00:00<?, ?it/s][1,0]<stdout>:100% 33/33 [00:00<00:00, 5023.48it/s]
[1,1]<stdout>:  0% 0/33 [00:00<?, ?it/s][1,0]<stdout>:[2025-10-12 03:11:23 DP0 TP2] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=3072, K=1536, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:100% 33/33 [00:00<00:00, 4854.69it/s]
[1,0]<stdout>:
[1,0]<stdout>:  0% 0/33 [00:00<?, ?it/s][A[1,0]<stdout>:  0% 0/33 [00:00<?, ?it/s][1,0]<stdout>:  0% 0/33 [00:00<?, ?it/s][1,0]<stdout>:  0% 0/33 [00:00<?, ?it/s][1,0]<stdout>:100% 33/33 [00:00<00:00, 5681.01it/s]
[1,1]<stdout>:  0% 0/33 [00:00<?, ?it/s][1,0]<stdout>:100% 33/33 [00:00<00:00, 5434.32it/s]
[1,0]<stdout>:  0% 0/33 [00:00<?, ?it/s][1,1]<stdout>:  0% 0/33 [00:00<?, ?it/s][1,1]<stdout>:  0% 0/33 [00:00<?, ?it/s][1,0]<stdout>:  0% 0/33 [00:00<?, ?it/s][1,0]<stdout>:100% 33/33 [00:00<00:00, 5412.22it/s]
[1,0]<stdout>:  0% 0/33 [00:00<?, ?it/s][1,1]<stdout>:  0% 0/33 [00:00<?, ?it/s][1,0]<stdout>:100% 33/33 [00:00<00:00, 5693.63it/s]
[1,0]<stdout>:100% 33/33 [00:00<00:00, 5384.22it/s]
[1,1]<stdout>:100% 33/33 [00:00<00:00, 4894.86it/s]
[1,1]<stdout>:100% 33/33 [00:00<00:00, 5017.84it/s][1,1]<stdout>:
[1,1]<stdout>:100% 33/33 [00:00<00:00, 4790.67it/s][1,1]<stdout>:
[1,0]<stdout>:100% 33/33 [00:00<00:00, 5191.94it/s]
[1,0]<stdout>:100% 33/33 [00:00<00:00, 5656.40it/s]
[1,1]<stdout>:  0% 0/33 [00:00<?, ?it/s][1,1]<stdout>:100% 33/33 [00:00<00:00, 5137.03it/s]
[1,1]<stdout>:100% 33/33 [00:00<00:00, 5337.71it/s]
[1,0]<stdout>:[2025-10-12 03:11:23 DP0 TP0] Entering DeepGEMM JIT Pre-Compile session. It may takes a long time (typically 10-20 mins) if you have not run `sglang.compile_deep_gemm`. It is recommended to run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to reduce the overhead if you have not run it before. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[1,0]<stdout>:[2025-10-12 03:11:23 DP0 TP7] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=3072, K=1536, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-12 03:11:23 DP0 TP0] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=3072, K=1536, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-12 03:11:23 DP1 TP15] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=3072, K=1536, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-12 03:11:23 DP0 TP5] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=3072, K=1536, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-12 03:11:23 DP0 TP6] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=3072, K=1536, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-12 03:11:23 DP0 TP1] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=3072, K=1536, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-12 03:11:23 DP0 TP4] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=3072, K=1536, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-12 03:11:23 DP1 TP14] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=3072, K=1536, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-12 03:11:23 DP0 TP3] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=3072, K=1536, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-12 03:11:23 DP1 TP8] Entering DeepGEMM JIT Pre-Compile session. It may takes a long time (typically 10-20 mins) if you have not run `sglang.compile_deep_gemm`. It is recommended to run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to reduce the overhead if you have not run it before. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[1,1]<stdout>:[2025-10-12 03:11:23 DP1 TP8] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=3072, K=1536, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-12 03:11:23 DP1 TP12] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=3072, K=1536, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-12 03:11:23 DP1 TP11] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=3072, K=1536, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-12 03:11:23 DP1 TP9] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=3072, K=1536, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:  0% 0/33 [00:00<?, ?it/s][1,1]<stdout>:100% 33/33 [00:00<00:00, 6029.97it/s]
[1,1]<stdout>:  0% 0/33 [00:00<?, ?it/s][1,1]<stdout>:100% 33/33 [00:00<00:00, 5570.57it/s]
[1,1]<stdout>:[2025-10-12 03:11:23 DP1 TP10] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=3072, K=1536, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-12 03:11:23 DP1 TP13] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=3072, K=1536, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:  0% 0/29 [00:00<?, ?it/s][1,0]<stdout>:100% 29/29 [00:00<00:00, 12337.44it/s]
[1,1]<stdout>:  0% 0/29 [00:00<?, ?it/s][1,1]<stdout>:100% 29/29 [00:00<00:00, 11764.66it/s]
[1,0]<stdout>:  0% 0/29 [00:00<?, ?it/s][1,0]<stdout>:  0% 0/29 [00:00<?, ?it/s][1,0]<stdout>:  0% 0/29 [00:00<?, ?it/s][1,0]<stdout>:100% 29/29 [00:00<00:00, 13595.04it/s]
[1,0]<stdout>:100% 29/29 [00:00<00:00, 13958.55it/s]
[1,0]<stdout>:
[1,0]<stdout>:100% 29/29 [00:00<00:00, 13543.57it/s]
[1,0]<stdout>:  0% 0/29 [00:00<?, ?it/s][1,0]<stdout>:[A[1,0]<stdout>:100% 29/29 [00:00<00:00, 16432.70it/s]
[1,0]<stdout>:  0% 0/29 [00:00<?, ?it/s][1,0]<stdout>:  0% 0/29 [00:00<?, ?it/s][1,0]<stdout>:100% 29/29 [00:00<00:00, 12288.83it/s]
[1,0]<stdout>:100% 29/29 [00:00<00:00, 12745.97it/s]
[1,0]<stdout>:  0% 0/29 [00:00<?, ?it/s][1,0]<stdout>:100% 29/29 [00:00<00:00, 14125.52it/s]
[1,1]<stdout>:  0% 0/29 [00:00<?, ?it/s][1,1]<stdout>:100% 29/29 [00:00<00:00, 14878.88it/s]
[1,1]<stdout>:  0% 0/29 [00:00<?, ?it/s][1,1]<stdout>:100% 29/29 [00:00<00:00, 14249.63it/s]
[1,1]<stdout>:  0% 0/29 [00:00<?, ?it/s][1,1]<stdout>:100% 29/29 [00:00<00:00, 14050.46it/s]
[1,1]<stdout>:  0% 0/29 [00:00<?, ?it/s][1,1]<stdout>:100% 29/29 [00:00<00:00, 16025.67it/s]
[1,1]<stdout>:  0% 0/29 [00:00<?, ?it/s][1,1]<stdout>:100% 29/29 [00:00<00:00, 13407.72it/s]
[1,1]<stdout>:  0% 0/29 [00:00<?, ?it/s][1,1]<stdout>:100% 29/29 [00:00<00:00, 13221.18it/s]
[1,1]<stdout>:  0% 0/29 [00:00<?, ?it/s][1,1]<stdout>:100% 29/29 [00:00<00:00, 14535.71it/s]
[1,0]<stdout>:[2025-10-12 03:11:24 DP0 TP2] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=7168, K=2048, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-12 03:11:24 DP1 TP11] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=7168, K=2048, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-12 03:11:24 DP1 TP9] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=7168, K=2048, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-12 03:11:24 DP1 TP12] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=7168, K=2048, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-12 03:11:24 DP1 TP8] Entering DeepGEMM JIT Pre-Compile session. It may takes a long time (typically 10-20 mins) if you have not run `sglang.compile_deep_gemm`. It is recommended to run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to reduce the overhead if you have not run it before. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[1,1]<stdout>:[2025-10-12 03:11:24 DP1 TP8] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=7168, K=2048, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-12 03:11:24 DP1 TP15] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=7168, K=2048, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-12 03:11:24 DP1 TP14] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=7168, K=2048, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-12 03:11:24 DP0 TP5] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=7168, K=2048, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-12 03:11:24 DP1 TP10] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=7168, K=2048, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-12 03:11:24 DP0 TP4] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=7168, K=2048, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-12 03:11:24 DP0 TP0] Entering DeepGEMM JIT Pre-Compile session. It may takes a long time (typically 10-20 mins) if you have not run `sglang.compile_deep_gemm`. It is recommended to run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to reduce the overhead if you have not run it before. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[1,0]<stdout>:[2025-10-12 03:11:24 DP0 TP0] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=7168, K=2048, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-12 03:11:24 DP0 TP3] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=7168, K=2048, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-12 03:11:24 DP0 TP1] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=7168, K=2048, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-12 03:11:24 DP0 TP7] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=7168, K=2048, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-12 03:11:24 DP0 TP6] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=7168, K=2048, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-12 03:11:24 DP1 TP13] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=7168, K=2048, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:  0% 0/16 [00:00<?, ?it/s][1,0]<stdout>:100% 16/16 [00:00<00:00, 13246.91it/s]
[1,1]<stdout>:  0% 0/16 [00:00<?, ?it/s][1,1]<stdout>:100% 16/16 [00:00<00:00, 11634.69it/s]
[1,1]<stdout>:  0% 0/16 [00:00<?, ?it/s][1,1]<stdout>:  0% 0/16 [00:00<?, ?it/s][1,1]<stdout>:100% 16/16 [00:00<00:00, 17521.90it/s][1,1]<stdout>:
[1,1]<stdout>:  0% 0/16 [00:00<?, ?it/s][1,1]<stdout>:  0% 0/16 [00:00<?, ?it/s][1,1]<stdout>:100% 16/16 [00:00<00:00, 13524.56it/s]
[1,1]<stdout>:100% 16/16 [00:00<00:00, 15183.00it/s]
[1,1]<stdout>:100% 16/16 [00:00<00:00, 13684.52it/s]
[1,1]<stdout>:NCCL version 2.27.3+cuda12.9
[1,0]<stdout>:  0% 0/16 [00:00<?, ?it/s][1,0]<stdout>:100% 16/16 [00:00<00:00, 16104.84it/s]
[1,1]<stdout>:  0% 0/16 [00:00<?, ?it/s][1,1]<stdout>:100% 16/16 [00:00<00:00, 14936.32it/s]
[1,0]<stdout>:  0% 0/16 [00:00<?, ?it/s][1,0]<stdout>:100% 16/16 [00:00<00:00, 17843.36it/s]
[1,1]<stdout>:  0% 0/16 [00:00<?, ?it/s][1,1]<stdout>:100% 16/16 [00:00<00:00, 12393.14it/s]
[1,0]<stdout>:
[1,0]<stdout>:  0% 0/16 [00:00<?, ?it/s][A[1,0]<stdout>:100% 16/16 [00:00<00:00, 16186.41it/s]
[1,0]<stdout>:  0% 0/16 [00:00<?, ?it/s][1,0]<stdout>:100% 16/16 [00:00<00:00, 16027.91it/s]
[1,0]<stdout>:  0% 0/16 [00:00<?, ?it/s][1,0]<stdout>:100% 16/16 [00:00<00:00, 10873.11it/s]
[1,0]<stdout>:  0% 0/16 [00:00<?, ?it/s][1,0]<stdout>:100% 16/16 [00:00<00:00, 18610.33it/s]
[1,0]<stdout>:  0% 0/16 [00:00<?, ?it/s][1,0]<stdout>:100% 16/16 [00:00<00:00, 18020.64it/s]
[1,1]<stdout>:  0% 0/16 [00:00<?, ?it/s][1,1]<stdout>:100% 16/16 [00:00<00:00, 19130.23it/s]
[1,1]<stdout>:[2025-10-12 03:11:27 DP1 TP15] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=2304, K=7168, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-12 03:11:27 DP0 TP2] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=2304, K=7168, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-12 03:11:27 DP0 TP3] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=2304, K=7168, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-12 03:11:27 DP0 TP0] Entering DeepGEMM JIT Pre-Compile session. It may takes a long time (typically 10-20 mins) if you have not run `sglang.compile_deep_gemm`. It is recommended to run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to reduce the overhead if you have not run it before. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[1,0]<stdout>:[2025-10-12 03:11:27 DP0 TP0] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=2304, K=7168, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-12 03:11:27 DP0 TP1] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=2304, K=7168, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-12 03:11:27 DP1 TP14] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=2304, K=7168, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-12 03:11:27 DP1 TP13] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=2304, K=7168, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-12 03:11:27 DP0 TP4] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=2304, K=7168, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-12 03:11:27 DP0 TP7] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=2304, K=7168, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-12 03:11:27 DP1 TP12] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=2304, K=7168, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-12 03:11:27 DP1 TP9] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=2304, K=7168, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-12 03:11:27 DP1 TP11] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=2304, K=7168, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-12 03:11:27 DP0 TP6] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=2304, K=7168, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-12 03:11:27 DP1 TP8] Entering DeepGEMM JIT Pre-Compile session. It may takes a long time (typically 10-20 mins) if you have not run `sglang.compile_deep_gemm`. It is recommended to run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to reduce the overhead if you have not run it before. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[1,1]<stdout>:[2025-10-12 03:11:27 DP1 TP8] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=2304, K=7168, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-12 03:11:27 DP1 TP10] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=2304, K=7168, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-12 03:11:27 DP0 TP5] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=2304, K=7168, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:  0% 0/32 [00:00<?, ?it/s][1,1]<stdout>:100% 32/32 [00:00<00:00, 13897.05it/s]
[1,1]<stdout>:[2025-10-12 03:11:28 DP1 TP15] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=7168, K=1152, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:  0% 0/32 [00:00<?, ?it/s][1,0]<stdout>:  0% 0/32 [00:00<?, ?it/s][1,0]<stdout>:100% 32/32 [00:00<00:00, 15191.59it/s]
[1,0]<stdout>:100% 32/32 [00:00<00:00, 14757.31it/s]
[1,0]<stdout>:[2025-10-12 03:11:28 DP0 TP2] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=7168, K=1152, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-12 03:11:28 DP0 TP3] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=7168, K=1152, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:
[1,0]<stdout>:  0% 0/32 [00:00<?, ?it/s][A[1,0]<stdout>:100% 32/32 [00:00<00:00, 17293.87it/s]
[1,0]<stdout>:  0% 0/32 [00:00<?, ?it/s][1,0]<stdout>:100% 32/32 [00:00<00:00, 14908.11it/s]
[1,0]<stdout>:[2025-10-12 03:11:28 DP0 TP0] Entering DeepGEMM JIT Pre-Compile session. It may takes a long time (typically 10-20 mins) if you have not run `sglang.compile_deep_gemm`. It is recommended to run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to reduce the overhead if you have not run it before. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[1,0]<stdout>:[2025-10-12 03:11:28 DP0 TP0] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=7168, K=1152, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-12 03:11:28 DP0 TP1] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=7168, K=1152, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:  0% 0/32 [00:00<?, ?it/s][1,0]<stdout>:100% 32/32 [00:00<00:00, 14484.97it/s]
[1,0]<stdout>:  0% 0/32 [00:00<?, ?it/s][1,0]<stdout>:100% 32/32 [00:00<00:00, 14700.74it/s]
[1,1]<stdout>:  0% 0/32 [00:00<?, ?it/s][1,1]<stdout>:100% 32/32 [00:00<00:00, 13454.06it/s]
[1,1]<stdout>:  0% 0/32 [00:00<?, ?it/s][1,0]<stdout>:[2025-10-12 03:11:28 DP0 TP7] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=7168, K=1152, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:100% 32/32 [00:00<00:00, 13475.68it/s]
[1,0]<stdout>:[2025-10-12 03:11:28 DP0 TP4] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=7168, K=1152, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:  0% 0/32 [00:00<?, ?it/s][1,1]<stdout>:  0% 0/32 [00:00<?, ?it/s][1,1]<stdout>:[2025-10-12 03:11:28 DP1 TP14] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=7168, K=1152, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:100% 32/32 [00:00<00:00, 13801.31it/s]
[1,1]<stdout>:100% 32/32 [00:00<00:00, 14315.03it/s]
[1,1]<stdout>:  0% 0/32 [00:00<?, ?it/s][1,1]<stdout>:[2025-10-12 03:11:28 DP1 TP13] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=7168, K=1152, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:  0% 0/32 [00:00<?, ?it/s][1,1]<stdout>:100% 32/32 [00:00<00:00, 12927.93it/s]
[1,1]<stdout>:[2025-10-12 03:11:28 DP1 TP12] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=7168, K=1152, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:100% 32/32 [00:00<00:00, 13558.72it/s]
[1,1]<stdout>:[2025-10-12 03:11:28 DP1 TP8] Entering DeepGEMM JIT Pre-Compile session. It may takes a long time (typically 10-20 mins) if you have not run `sglang.compile_deep_gemm`. It is recommended to run `sglang.compile_deep_gemm` with same args as `sglang.launch_server` for pre-compilation to reduce the overhead if you have not run it before. For example: `python3 -m sglang.compile_deep_gemm --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code`
[1,1]<stdout>:[2025-10-12 03:11:28 DP1 TP8] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=7168, K=1152, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:  0% 0/32 [00:00<?, ?it/s][1,1]<stdout>:100% 32/32 [00:00<00:00, 12618.01it/s]
[1,1]<stdout>:[2025-10-12 03:11:28 DP1 TP11] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=7168, K=1152, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-12 03:11:28 DP1 TP10] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=7168, K=1152, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:[2025-10-12 03:11:28 DP1 TP9] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=7168, K=1152, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:  0% 0/32 [00:00<?, ?it/s][1,0]<stdout>:100% 32/32 [00:00<00:00, 14374.82it/s]
[1,0]<stdout>:  0% 0/32 [00:00<?, ?it/s][1,0]<stdout>:100% 32/32 [00:00<00:00, 15006.45it/s]
[1,0]<stdout>:[2025-10-12 03:11:28 DP0 TP6] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=7168, K=1152, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,0]<stdout>:[2025-10-12 03:11:28 DP0 TP5] Try DeepGEMM JIT Compiling for <gemm_fp8_fp8_bf16_nt> N=7168, K=1152, num_groups=1 with all Ms. It only takes a little time (typically 1 sec) if you have run `python3 -m sglang.compile_deep_gemm`. 
[1,1]<stdout>:  0% 0/16 [00:00<?, ?it/s][1,1]<stdout>:100% 16/16 [00:00<00:00, 9576.04it/s]
[1,0]<stdout>:  0% 0/16 [00:00<?, ?it/s][1,0]<stdout>:100% 16/16 [00:00<00:00, 12406.89it/s]
[1,0]<stdout>:  0% 0/16 [00:00<?, ?it/s][1,0]<stdout>:100% 16/16 [00:00<00:00, 16933.85it/s]
[1,1]<stdout>:  0% 0/16 [00:00<?, ?it/s][1,1]<stdout>:100% 16/16 [00:00<00:00, 11975.17it/s]
[1,0]<stdout>:  0% 0/16 [00:00<?, ?it/s][1,0]<stdout>:100% 16/16 [00:00<00:00, 17683.50it/s]
[1,0]<stdout>:
[1,0]<stdout>:  0% 0/16 [00:00<?, ?it/s][1,0]<stdout>:[A[1,0]<stdout>:100% 16/16 [00:00<00:00, 15124.83it/s]
[1,0]<stdout>:  0% 0/16 [00:00<?, ?it/s][1,0]<stdout>:100% 16/16 [00:00<00:00, 17011.12it/s]
[1,0]<stdout>:  0% 0/16 [00:00<?, ?it/s][1,1]<stdout>:  0% 0/16 [00:00<?, ?it/s][1,0]<stdout>:100% 16/16 [00:00<00:00, 14538.32it/s][1,0]<stdout>:
[1,1]<stdout>:100% 16/16 [00:00<00:00, 17512.75it/s]
[1,1]<stdout>:  0% 0/16 [00:00<?, ?it/s][1,1]<stdout>:100% 16/16 [00:00<00:00, 13530.01it/s]
[1,1]<stdout>:  0% 0/16 [00:00<?, ?it/s][1,1]<stdout>:  0% 0/16 [00:00<?, ?it/s][1,1]<stdout>:  0% 0/16 [00:00<?, ?it/s][1,1]<stdout>:100% 16/16 [00:00<00:00, 13197.42it/s]
[1,1]<stdout>:100% 16/16 [00:00<00:00, 10370.71it/s]
[1,1]<stdout>:100% 16/16 [00:00<00:00, 13347.03it/s]
[1,1]<stdout>:  0% 0/16 [00:00<?, ?it/s][1,1]<stdout>:100% 16/16 [00:00<00:00, 15145.31it/s]
[1,0]<stdout>:  0% 0/16 [00:00<?, ?it/s][1,0]<stdout>:100% 16/16 [00:00<00:00, 16598.78it/s]
[1,0]<stdout>:  0% 0/16 [00:00<?, ?it/s][1,0]<stdout>:100% 16/16 [00:00<00:00, 20134.67it/s]
[1,0]<stdout>:Capturing batches (bs=32 avail_mem=21.84 GB):   0% 0/2 [00:10<?, ?it/s]
[1,1]<stdout>:[2025-10-12 03:11:31 DP1 TP8] Scheduler hit an exception: Traceback (most recent call last):
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2555, in run_scheduler_process
[1,1]<stdout>:    scheduler = Scheduler(
[1,1]<stdout>:                ^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 315, in __init__
[1,1]<stdout>:    self.tp_worker = TpWorkerClass(
[1,1]<stdout>:                     ^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 84, in __init__
[1,1]<stdout>:    self.model_runner = ModelRunner(
[1,1]<stdout>:                        ^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 242, in __init__
[1,1]<stdout>:    self.initialize(min_per_gpu_memory)
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 342, in initialize
[1,1]<stdout>:    self.init_cuda_graphs()
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 1611, in init_cuda_graphs
[1,1]<stdout>:    self.cuda_graph_runner = CudaGraphRunner(self)
[1,1]<stdout>:                             ^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 380, in __init__
[1,1]<stdout>:    self.capture()
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 485, in capture
[1,1]<stdout>:    ) = self.capture_one_batch_size(bs, forward)
[1,1]<stdout>:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 649, in capture_one_batch_size
[1,1]<stdout>:    run_once()
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 637, in run_once
[1,1]<stdout>:    logits_output_or_pp_proxy_tensors = forward(
[1,1]<stdout>:                                        ^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1,1]<stdout>:    return func(*args, **kwargs)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/models/deepseek_v2.py", line 2221, in forward
[1,1]<stdout>:    hidden_states = self.model(input_ids, positions, forward_batch, input_embeds)
[1,1]<stdout>:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
[1,1]<stdout>:    return self._call_impl(*args, **kwargs)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
[1,1]<stdout>:    return forward_call(*args, **kwargs)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/models/deepseek_v2.py", line 2107, in forward
[1,1]<stdout>:    hidden_states, residual = model_forward_maybe_tbo(
[1,1]<stdout>:                              ^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/two_batch_overlap.py", line 800, in model_forward_maybe_tbo
[1,1]<stdout>:    return _model_forward_tbo(
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/two_batch_overlap.py", line 832, in _model_forward_tbo
[1,1]<stdout>:    outputs_arr = execute_overlapped_operations(
[1,1]<stdout>:                [1,1]<stdout>:  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/operations.py", line 48, in execute_overlapped_operations
[1,1]<stdout>:    executor_a.next()
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/operations.py", line 99, in next
[1,1]<stdout>:    self._stage_output = op.fn(
[1,1]<stdout>:                         ^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/models/deepseek_v2.py", line 1968, in op_comm_prepare_mlp
[1,1]<stdout>:    self.layer_communicator.prepare_mlp(
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/layers/communicator.py", line 229, in prepare_mlp
[1,1]<stdout>:    return self._communicate_with_all_reduce_and_layer_norm_fn(
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/layers/communicator.py", line 430, in _gather_hidden_states_and_residual
[1,1]<stdout>:    dp_gather_partial(hidden_states, local_hidden_states, forward_batch)
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/layers/dp_attention.py", line 417, in dp_gather_partial
[1,1]<stdout>:    _dp_gather(global_tokens, local_tokens, forward_batch, is_partial=True)
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/layers/dp_attention.py", line 402, in _dp_gather
[1,1]<stdout>:    if forward_batch.dp_padding_mode.is_max_len():
[1,1]<stdout>:       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:AttributeError: 'NoneType' object has no attribute 'is_max_len'
[1,1]<stdout>:
[1,1]<stdout>:[2025-10-12 03:11:31 DP1 TP9] Scheduler hit an exception: Traceback (most recent call last):
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2555, in run_scheduler_process
[1,1]<stdout>:    scheduler = Scheduler(
[1,1]<stdout>:                ^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 315, in __init__
[1,1]<stdout>:    self.tp_worker = TpWorkerClass(
[1,1]<stdout>:                     ^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 84, in __init__
[1,1]<stdout>:    self.model_runner = ModelRunner(
[1,1]<stdout>:                        ^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 242, in __init__
[1,1]<stdout>:    self.initialize(min_per_gpu_memory)
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 342, in initialize
[1,1]<stdout>:    self.init_cuda_graphs()
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 1611, in init_cuda_graphs
[1,1]<stdout>:    self.cuda_graph_runner = CudaGraphRunner(self)
[1,1]<stdout>:                             ^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 380, in __init__
[1,1]<stdout>:    self.capture()
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 485, in capture
[1,1]<stdout>:    ) = self.capture_one_batch_size(bs, forward)
[1,1]<stdout>:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 649, in capture_one_batch_size
[1,1]<stdout>:    run_once()
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 637, in run_once
[1,1]<stdout>:    logits_output_or_pp_proxy_tensors = forward(
[1,1]<stdout>:                                        ^^^^^^^^
[1,1]<stdout>:  File "[1,1]<stdout>:/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1,1]<stdout>:    return func(*args, **kwargs)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/models/deepseek_v2.py", line 2221, in forward
[1,1]<stdout>:    hidden_states = self.model(input_ids, positions, forward_batch, input_embeds)
[1,1]<stdout>:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
[1,1]<stdout>:    return self._call_impl(*args, **kwargs)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
[1,1]<stdout>:    return forward_call(*args, **kwargs)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/models/deepseek_v2.py", line 2107, in forward
[1,1]<stdout>:    hidden_states, residual = model_forward_maybe_tbo(
[1,1]<stdout>:                              ^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/two_batch_overlap.py", line 800, in model_forward_maybe_tbo
[1,1]<stdout>:    return _model_forward_tbo(
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/two_batch_overlap.py", line 832, in _model_forward_tbo
[1,1]<stdout>:    outputs_arr = execute_overlapped_operations(
[1,1]<stdout>:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/operations.py", line 48, in execute_overlapped_operations
[1,1]<stdout>:    executor_a.next()
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/operations.py", line 99, in next
[1,1]<stdout>:    self._stage_output = op.fn(
[1,1]<stdout>:                         ^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/models/deepseek_v2.py", line 1968, in op_comm_prepare_mlp
[1,1]<stdout>:    self.layer_communicator.prepare_mlp(
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/layers/communicator.py", line 229, in prepare_mlp
[1,1]<stdout>:    return self._communicate_with_all_reduce_and_layer_norm_fn(
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/layers/communicator.py", line 430, in _gather_hidden_states_and_residual
[1,1]<stdout>:    dp_gather_partial(hidden_states, local_hidden_states, forward_batch)
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/layers/dp_attention.py", line 417, in dp_gather_partial
[1,1]<stdout>:    _dp_gather(global_tokens, local_tokens, forward_batch, is_partial=True)
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/layers/dp_attention.py", line 402, in _dp_gather
[1,1]<stdout>:    if forward_batch.dp_padding_mode.is_max_len():
[1,1]<stdout>:       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:AttributeError: 'NoneType' object has no attribute 'is_max_len'
[1,1]<stdout>:
[1,0]<stdout>:[2025-10-12 03:11:31 DP0 TP2] Scheduler hit an exception: Traceback (most recent call last):
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2555, in run_scheduler_process
[1,0]<stdout>:    scheduler = Scheduler(
[1,0]<stdout>:                ^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 315, in __init__
[1,0]<stdout>:    self.tp_worker = TpWorkerClass(
[1,0]<stdout>:                     ^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 84, in __init__
[1,0]<stdout>:    self.model_runner = ModelRunner(
[1,0]<stdout>:                        ^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 242, in __init__
[1,0]<stdout>:    self.initialize(min_per_gpu_memory)
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 342, in initialize
[1,0]<stdout>:    self.init_cuda_graphs()
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 1611, in init_cuda_graphs
[1,0]<stdout>:    self.cuda_graph_runner = CudaGraphRunner(self)
[1,0]<stdout>:                             ^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 380, in __init__
[1,0]<stdout>:    self.capture()
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 485, in capture
[1,0]<stdout>:    ) = self.capture_one_batch_size(bs, forward)
[1,0]<stdout>:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 649, in capture_one_batch_size
[1,0]<stdout>:    run_once()
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 637, in run_once
[1,0]<stdout>:    logits_output_or_pp_proxy_tensors = forward(
[1,0]<stdout>:                                        ^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1,0]<stdout>:    return func(*args, **kwargs)
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/models/deepseek_v2.py", line 2221, in forward
[1,0]<stdout>:    hidden_states = self.model(input_ids, positions, forward_batch, input_embeds)
[1,0]<stdout>:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
[1,0]<stdout>:    return self._call_impl(*args, **kwargs)
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
[1,0]<stdout>:    return forward_call(*args, **kwargs)
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/models/deepseek_v2.py", line 2107, in forward
[1,0]<stdout>:    hidden_states, residual = model_forward_maybe_tbo(
[1,0]<stdout>:                              ^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/two_batch_overlap.py", line 800, in model_forward_maybe_tbo
[1,0]<stdout>:    return _model_forward_tbo(
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/two_batch_overlap.py", line 832, in _model_forward_tbo
[1,0]<stdout>:    outputs_arr = execute_overlapped_operations(
[1,0]<stdout>:                [1,0]<stdout>:  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/operations.py", line 48, in execute_overlapped_operations
[1,0]<stdout>:    executor_a.next()
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/operations.py", line 99, in next
[1,0]<stdout>:    self._stage_output = op.fn(
[1,0]<stdout>:                         ^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/models/deepseek_v2.py", line 1968, in op_comm_prepare_mlp
[1,0]<stdout>:    self.layer_communicator.prepare_mlp(
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/layers/communicator.py", line 229, in prepare_mlp
[1,0]<stdout>:    return self._communicate_with_all_reduce_and_layer_norm_fn(
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/layers/communicator.py", line 430, in _gather_hidden_states_and_residual
[1,0]<stdout>:    dp_gather_partial(hidden_states, local_hidden_states, forward_batch)
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/layers/dp_attention.py", line 417, in dp_gather_partial
[1,0]<stdout>:    _dp_gather(global_tokens, local_tokens, forward_batch, is_partial=True)
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/layers/dp_attention.py", line 402, in _dp_gather
[1,0]<stdout>:    if forward_batch.dp_padding_mode.is_max_len():
[1,0]<stdout>:       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:AttributeError: 'NoneType' object has no attribute 'is_max_len'
[1,0]<stdout>:
[1,0]<stdout>:[2025-10-12 03:11:31 DP0 TP5] Scheduler hit an exception: Traceback (most recent call last):
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2555, in run_scheduler_process
[1,0]<stdout>:    scheduler = Scheduler(
[1,0]<stdout>:                ^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 315, in __init__
[1,0]<stdout>:    self.tp_worker = TpWorkerClass(
[1,0]<stdout>:                     ^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 84, in __init__
[1,0]<stdout>:    self.model_runner = ModelRunner(
[1,0]<stdout>:                        ^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 242, in __init__
[1,0]<stdout>:    self.initialize(min_per_gpu_memory)
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 342, in initialize
[1,0]<stdout>:    self.init_cuda_graphs()
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 1611, in init_cuda_graphs
[1,0]<stdout>:    self.cuda_graph_runner = CudaGraphRunner(self)
[1,0]<stdout>:                             ^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 380, in __init__
[1,0]<stdout>:    self.capture()
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 485, in capture
[1,0]<stdout>:    ) = self.capture_one_batch_size(bs, forward)
[1,0]<stdout>:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 649, in capture_one_batch_size
[1,0]<stdout>:    run_once()
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 637, in run_once
[1,0]<stdout>:    logits_output_or_pp_proxy_tensors = forward(
[1,0]<stdout>:                                        ^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1,0]<stdout>:    return func(*args, **kwargs)
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/models/deepseek_v2.py", line 2221, in forward
[1,0]<stdout>:    hidden_states = self.model(input_ids, positions, forward_batch, input_embeds)
[1,0]<stdout>:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
[1,0]<stdout>:    return self._call_impl(*args, **kwargs)
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
[1,0]<stdout>:    return forward_call(*args, **kwargs)
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/models/deepseek_v2.py", line 2107, in forward
[1,0]<stdout>:    hidden_states, residual = model_forward_maybe_tbo(
[1,0]<stdout>:                              ^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/two_batch_overlap.py", line 800, in model_forward_maybe_tbo
[1,0]<stdout>:    return _model_forward_tbo(
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/two_batch_overlap.py", line 832, in _model_forward_tbo
[1,0]<stdout>:    outputs_arr = execute_overlapped_operations(
[1,0]<stdout>:                 [1,0]<stdout>: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/operations.py", line 48, in execute_overlapped_operations
[1,0]<stdout>:    executor_a.next()
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/operations.py", line 99, in next
[1,0]<stdout>:    self._stage_output = op.fn(
[1,0]<stdout>:                         ^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/models/deepseek_v2.py", line 1968, in op_comm_prepare_mlp
[1,0]<stdout>:    self.layer_communicator.prepare_mlp(
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/layers/communicator.py", line 229, in prepare_mlp
[1,0]<stdout>:    return self._communicate_with_all_reduce_and_layer_norm_fn(
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/layers/communicator.py", line 430, in _gather_hidden_states_and_residual
[1,0]<stdout>:    dp_gather_partial(hidden_states, local_hidden_states, forward_batch)
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/layers/dp_attention.py", line 417, in dp_gather_partial
[1,0]<stdout>:    _dp_gather(global_tokens, local_tokens, forward_batch, is_partial=True)
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/layers/dp_attention.py", line 402, in _dp_gather
[1,0]<stdout>:    if forward_batch.dp_padding_mode.is_max_len():
[1,0]<stdout>:       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:AttributeError: 'NoneType' object has no attribute 'is_max_len'
[1,0]<stdout>:
[1,1]<stdout>:[2025-10-12 03:11:31 DP1 TP12] Scheduler hit an exception: Traceback (most recent call last):
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2555, in run_scheduler_process
[1,1]<stdout>:    scheduler = Scheduler(
[1,1]<stdout>:                ^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 315, in __init__
[1,1]<stdout>:    self.tp_worker = TpWorkerClass(
[1,1]<stdout>:                     ^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 84, in __init__
[1,1]<stdout>:    self.model_runner = ModelRunner(
[1,1]<stdout>:                        ^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 242, in __init__
[1,1]<stdout>:    self.initialize(min_per_gpu_memory)
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 342, in initialize
[1,1]<stdout>:    self.init_cuda_graphs()
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 1611, in init_cuda_graphs
[1,1]<stdout>:    self.cuda_graph_runner = CudaGraphRunner(self)
[1,1]<stdout>:                             ^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 380, in __init__
[1,1]<stdout>:    self.capture()
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 485, in capture
[1,1]<stdout>:    ) = self.capture_one_batch_size(bs, forward)
[1,1]<stdout>:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 649, in capture_one_batch_size
[1,1]<stdout>:    run_once()
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 637, in run_once
[1,1]<stdout>:    logits_output_or_pp_proxy_tensors = forward(
[1,1]<stdout>:                                        ^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1,1]<stdout>:    return func(*args, **kwargs)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/models/deepseek_v2.py", line 2221, in forward
[1,1]<stdout>:    hidden_states = self.model(input_ids, positions, forward_batch, input_embeds)
[1,1]<stdout>:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
[1,1]<stdout>:    return self._call_impl(*args, **kwargs)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
[1,1]<stdout>:    return forward_call(*args, **kwargs)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/models/deepseek_v2.py", line 2107, in forward
[1,1]<stdout>:    hidden_states, residual = model_forward_maybe_tbo(
[1,1]<stdout>:                              ^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/two_batch_overlap.py", line 800, in model_forward_maybe_tbo
[1,1]<stdout>:    return _model_forward_tbo(
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/two_batch_overlap.py", line 832, in _model_forward_tbo
[1,1]<stdout>:    outputs_arr = execute_overlapped_operations(
[1,1]<stdout>:               [1,1]<stdout>:   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/operations.py", line 48, in execute_overlapped_operations
[1,1]<stdout>:    executor_a.next()
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/operations.py", line 99, in next
[1,1]<stdout>:    self._stage_output = op.fn(
[1,1]<stdout>:                         ^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/models/deepseek_v2.py", line 1968, in op_comm_prepare_mlp
[1,1]<stdout>:    self.layer_communicator.prepare_mlp(
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/layers/communicator.py", line 229, in prepare_mlp
[1,1]<stdout>:    return self._communicate_with_all_reduce_and_layer_norm_fn(
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/layers/communicator.py", line 430, in _gather_hidden_states_and_residual
[1,1]<stdout>:    dp_gather_partial(hidden_states, local_hidden_states, forward_batch)
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/layers/dp_attention.py", line 417, in dp_gather_partial
[1,1]<stdout>:    _dp_gather(global_tokens, local_tokens, forward_batch, is_partial=True)
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/layers/dp_attention.py", line 402, in _dp_gather
[1,1]<stdout>:    if forward_batch.dp_padding_mode.is_max_len():
[1,1]<stdout>:       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:AttributeError: 'NoneType' object has no attribute 'is_max_len'
[1,1]<stdout>:
[1,1]<stdout>:[2025-10-12 03:11:31 DP1 TP14] Scheduler hit an exception: Traceback (most recent call last):
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2555, in run_scheduler_process
[1,1]<stdout>:    scheduler = Scheduler(
[1,1]<stdout>:                ^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 315, in __init__
[1,1]<stdout>:    self.tp_worker = TpWorkerClass(
[1,1]<stdout>:                     ^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 84, in __init__
[1,1]<stdout>:    self.model_runner = ModelRunner(
[1,1]<stdout>:                        ^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 242, in __init__
[1,1]<stdout>:    self.initialize(min_per_gpu_memory)
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 342, in initialize
[1,1]<stdout>:    self.init_cuda_graphs()
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 1611, in init_cuda_graphs
[1,1]<stdout>:    self.cuda_graph_runner = CudaGraphRunner(self)
[1,1]<stdout>:                             ^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 380, in __init__
[1,1]<stdout>:    self.capture()
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 485, in capture
[1,1]<stdout>:    ) = self.capture_one_batch_size(bs, forward)
[1,1]<stdout>:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 649, in capture_one_batch_size
[1,1]<stdout>:    run_once()
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 637, in run_once
[1,1]<stdout>:    logits_output_or_pp_proxy_tensors = forward(
[1,1]<stdout>:                                        ^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1,1]<stdout>:    return func(*args, **kwargs)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/models/deepseek_v2.py", line 2221, in forward
[1,1]<stdout>:    hidden_states = self.model(input_ids, positions, forward_batch, input_embeds)
[1,1]<stdout>:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
[1,1]<stdout>:    return self._call_impl(*args, **kwargs)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
[1,1]<stdout>:    return forward_call(*args, **kwargs)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/models/deepseek_v2.py", line 2107, in forward
[1,1]<stdout>:    hidden_states, residual = model_forward_maybe_tbo(
[1,1]<stdout>:                              ^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/two_batch_overlap.py", line 800, in model_forward_maybe_tbo
[1,1]<stdout>:    return _model_forward_tbo(
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/two_batch_overlap.py", line 832, in _model_forward_tbo
[1,1]<stdout>:    outputs_arr = execute_overlapped_operations(
[1,1]<stdout>:               [1,1]<stdout>:   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/operations.py", line 48, in execute_overlapped_operations
[1,1]<stdout>:    executor_a.next()
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/operations.py", line 99, in next
[1,1]<stdout>:    self._stage_output = op.fn(
[1,1]<stdout>:                         ^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/models/deepseek_v2.py", line 1968, in op_comm_prepare_mlp
[1,1]<stdout>:    self.layer_communicator.prepare_mlp(
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/layers/communicator.py", line 229, in prepare_mlp
[1,1]<stdout>:    return self._communicate_with_all_reduce_and_layer_norm_fn(
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/layers/communicator.py", line 430, in _gather_hidden_states_and_residual
[1,1]<stdout>:    dp_gather_partial(hidden_states, local_hidden_states, forward_batch)
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/layers/dp_attention.py", line 417, in dp_gather_partial
[1,1]<stdout>:    _dp_gather(global_tokens, local_tokens, forward_batch, is_partial=True)
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/layers/dp_attention.py", line 402, in _dp_gather
[1,1]<stdout>:    if forward_batch.dp_padding_mode.is_max_len():
[1,1]<stdout>:       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:AttributeError: 'NoneType' object has no attribute 'is_max_len'
[1,1]<stdout>:
[1,1]<stdout>:[2025-10-12 03:11:31 DP1 TP11] Scheduler hit an exception: Traceback (most recent call last):
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2555, in run_scheduler_process
[1,1]<stdout>:    scheduler = Scheduler(
[1,1]<stdout>:                ^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 315, in __init__
[1,1]<stdout>:    self.tp_worker = TpWorkerClass(
[1,1]<stdout>:                     ^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 84, in __init__
[1,1]<stdout>:    self.model_runner = ModelRunner(
[1,1]<stdout>:                        ^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 242, in __init__
[1,1]<stdout>:    self.initialize(min_per_gpu_memory)
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 342, in initialize
[1,1]<stdout>:    self.init_cuda_graphs()
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 1611, in init_cuda_graphs
[1,1]<stdout>:    self.cuda_graph_runner = CudaGraphRunner(self)
[1,1]<stdout>:                             ^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 380, in __init__
[1,1]<stdout>:    self.capture()
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 485, in capture
[1,1]<stdout>:    ) = self.capture_one_batch_size(bs, forward)
[1,1]<stdout>:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 649, in capture_one_batch_size
[1,1]<stdout>:    run_once()
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 637, in run_once
[1,1]<stdout>:    logits_output_or_pp_proxy_tensors = forward(
[1,1]<stdout>:                                        ^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1,1]<stdout>:    return func(*args, **kwargs)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/models/deepseek_v2.py", line 2221, in forward
[1,1]<stdout>:    hidden_states = self.model(input_ids, positions, forward_batch, input_embeds)
[1,1]<stdout>:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
[1,1]<stdout>:    return self._call_impl(*args, **kwargs)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
[1,1]<stdout>:    return forward_call(*args, **kwargs)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/models/deepseek_v2.py", line 2107, in forward
[1,1]<stdout>:    hidden_states, residual = model_forward_maybe_tbo(
[1,1]<stdout>:                              ^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/two_batch_overlap.py", line 800, in model_forward_maybe_tbo
[1,1]<stdout>:    return _model_forward_tbo(
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/two_batch_overlap.py", line 832, in _model_forward_tbo
[1,1]<stdout>:    outputs_arr = execute_overlapped_operations(
[1,1]<stdout>:               [1,1]<stdout>:   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/operations.py", line 48, in execute_overlapped_operations
[1,1]<stdout>:    executor_a.next()
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/operations.py", line 99, in next
[1,1]<stdout>:    self._stage_output = op.fn(
[1,1]<stdout>:                         ^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/models/deepseek_v2.py", line 1968, in op_comm_prepare_mlp
[1,1]<stdout>:    self.layer_communicator.prepare_mlp(
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/layers/communicator.py", line 229, in prepare_mlp
[1,1]<stdout>:    return self._communicate_with_all_reduce_and_layer_norm_fn(
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/layers/communicator.py", line 430, in _gather_hidden_states_and_residual
[1,1]<stdout>:    dp_gather_partial(hidden_states, local_hidden_states, forward_batch)
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/layers/dp_attention.py", line 417, in dp_gather_partial
[1,1]<stdout>:    _dp_gather(global_tokens, local_tokens, forward_batch, is_partial=True)
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/layers/dp_attention.py", line 402, in _dp_gather
[1,1]<stdout>:    if forward_batch.dp_padding_mode.is_max_len():
[1,1]<stdout>:       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:AttributeError: 'NoneType' object has no attribute 'is_max_len'
[1,1]<stdout>:
[1,0]<stdout>:[2025-10-12 03:11:31 DP0 TP0] Scheduler hit an exception: Traceback (most recent call last):
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2555, in run_scheduler_process
[1,0]<stdout>:    scheduler = Scheduler(
[1,0]<stdout>:                ^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 315, in __init__
[1,0]<stdout>:    self.tp_worker = TpWorkerClass(
[1,0]<stdout>:                     ^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 84, in __init__
[1,0]<stdout>:    self.model_runner = ModelRunner(
[1,0]<stdout>:                        ^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 242, in __init__
[1,0]<stdout>:    self.initialize(min_per_gpu_memory)
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 342, in initialize
[1,0]<stdout>:    self.init_cuda_graphs()
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 1611, in init_cuda_graphs
[1,0]<stdout>:    self.cuda_graph_runner = CudaGraphRunner(self)
[1,0]<stdout>:                             ^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 380, in __init__
[1,0]<stdout>:    self.capture()
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 485, in capture
[1,0]<stdout>:    ) = self.capture_one_batch_size(bs, forward)
[1,0]<stdout>:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 649, in capture_one_batch_size
[1,0]<stdout>:    run_once()
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 637, in run_once
[1,0]<stdout>:    logits_output_or_pp_proxy_tensors = forward(
[1,0]<stdout>:                                        ^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1,0]<stdout>:    return func(*args, **kwargs)
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/models/deepseek_v2.py", line 2221, in forward
[1,0]<stdout>:    hidden_states = self.model(input_ids, positions, forward_batch, input_embeds)
[1,0]<stdout>:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
[1,0]<stdout>:    return self._call_impl(*args, **kwargs)
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
[1,0]<stdout>:    return forward_call(*args, **kwargs)
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/models/deepseek_v2.py", line 2107, in forward
[1,0]<stdout>:    hidden_states, residual = model_forward_maybe_tbo(
[1,0]<stdout>:                              ^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/two_batch_overlap.py", line 800, in model_forward_maybe_tbo
[1,0]<stdout>:    return _model_forward_tbo(
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/two_batch_overlap.py", line 832, in _model_forward_tbo
[1,0]<stdout>:    outputs_arr = execute_overlapped_operations(
[1,0]<stdout>:                [1,0]<stdout>:  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/operations.py", line 48, in execute_overlapped_operations
[1,0]<stdout>:    executor_a.next()
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/operations.py", line 99, in next
[1,0]<stdout>:    self._stage_output = op.fn(
[1,0]<stdout>:                         ^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/models/deepseek_v2.py", line 1968, in op_comm_prepare_mlp
[1,0]<stdout>:    self.layer_communicator.prepare_mlp(
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/layers/communicator.py", line 229, in prepare_mlp
[1,0]<stdout>:    return self._communicate_with_all_reduce_and_layer_norm_fn(
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/layers/communicator.py", line 430, in _gather_hidden_states_and_residual
[1,0]<stdout>:    dp_gather_partial(hidden_states, local_hidden_states, forward_batch)
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/layers/dp_attention.py", line 417, in dp_gather_partial
[1,0]<stdout>:    _dp_gather(global_tokens, local_tokens, forward_batch, is_partial=True)
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/layers/dp_attention.py", line 402, in _dp_gather
[1,0]<stdout>:    if forward_batch.dp_padding_mode.is_max_len():
[1,0]<stdout>:       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:AttributeError: 'NoneType' object has no attribute 'is_max_len'
[1,0]<stdout>:
[1,1]<stdout>:[2025-10-12 03:11:31 DP1 TP13] Scheduler hit an exception: Traceback (most recent call last):
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2555, in run_scheduler_process
[1,1]<stdout>:    scheduler = Scheduler(
[1,1]<stdout>:                ^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 315, in __init__
[1,1]<stdout>:    self.tp_worker = TpWorkerClass(
[1,1]<stdout>:                     ^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 84, in __init__
[1,1]<stdout>:    self.model_runner = ModelRunner(
[1,1]<stdout>:                        ^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 242, in __init__
[1,1]<stdout>:    self.initialize(min_per_gpu_memory)
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 342, in initialize
[1,1]<stdout>:    self.init_cuda_graphs()
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 1611, in init_cuda_graphs
[1,1]<stdout>:    self.cuda_graph_runner = CudaGraphRunner(self)
[1,1]<stdout>:                             ^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 380, in __init__
[1,1]<stdout>:    self.capture()
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 485, in capture
[1,1]<stdout>:    ) = self.capture_one_batch_size(bs, forward)
[1,1]<stdout>:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 649, in capture_one_batch_size
[1,1]<stdout>:    run_once()
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 637, in run_once
[1,1]<stdout>:    logits_output_or_pp_proxy_tensors = forward(
[1,1]<stdout>:                                        ^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1,1]<stdout>:    return func(*args, **kwargs)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/models/deepseek_v2.py", line 2221, in forward
[1,1]<stdout>:    hidden_states = self.model(input_ids, positions, forward_batch, input_embeds)
[1,1]<stdout>:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
[1,1]<stdout>:    return self._call_impl(*args, **kwargs)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
[1,1]<stdout>:    return forward_call(*args, **kwargs)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/models/deepseek_v2.py", line 2107, in forward
[1,1]<stdout>:    hidden_states, residual = model_forward_maybe_tbo(
[1,1]<stdout>:                              ^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/two_batch_overlap.py", line 800, in model_forward_maybe_tbo
[1,1]<stdout>:    return _model_forward_tbo(
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/two_batch_overlap.py", line 832, in _model_forward_tbo
[1,1]<stdout>:    outputs_arr = execute_overlapped_operations(
[1,1]<stdout>:                [1,1]<stdout>:  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/operations.py", line 48, in execute_overlapped_operations
[1,1]<stdout>:    executor_a.next()
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/operations.py", line 99, in next
[1,1]<stdout>:    self._stage_output = op.fn(
[1,1]<stdout>:                         ^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/models/deepseek_v2.py", line 1968, in op_comm_prepare_mlp
[1,1]<stdout>:    self.layer_communicator.prepare_mlp(
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/layers/communicator.py", line 229, in prepare_mlp
[1,1]<stdout>:    return self._communicate_with_all_reduce_and_layer_norm_fn(
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/layers/communicator.py", line 430, in _gather_hidden_states_and_residual
[1,1]<stdout>:    dp_gather_partial(hidden_states, local_hidden_states, forward_batch)
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/layers/dp_attention.py", line 417, in dp_gather_partial
[1,1]<stdout>:    _dp_gather(global_tokens, local_tokens, forward_batch, is_partial=True)
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/layers/dp_attention.py", line 402, in _dp_gather
[1,1]<stdout>:    if forward_batch.dp_padding_mode.is_max_len():
[1,1]<stdout>:       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:AttributeError: 'NoneType' object has no attribute 'is_max_len'
[1,1]<stdout>:
[1,0]<stdout>:[2025-10-12 03:11:31 DP0 TP6] Scheduler hit an exception: Traceback (most recent call last):
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2555, in run_scheduler_process
[1,0]<stdout>:    scheduler = Scheduler(
[1,0]<stdout>:                ^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 315, in __init__
[1,0]<stdout>:    self.tp_worker = TpWorkerClass(
[1,0]<stdout>:                     ^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 84, in __init__
[1,0]<stdout>:    self.model_runner = ModelRunner(
[1,0]<stdout>:                        ^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 242, in __init__
[1,0]<stdout>:    self.initialize(min_per_gpu_memory)
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 342, in initialize
[1,0]<stdout>:    self.init_cuda_graphs()
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 1611, in init_cuda_graphs
[1,0]<stdout>:    self.cuda_graph_runner = CudaGraphRunner(self)
[1,0]<stdout>:                             ^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 380, in __init__
[1,0]<stdout>:    self.capture()
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 485, in capture
[1,0]<stdout>:    ) = self.capture_one_batch_size(bs, forward)
[1,0]<stdout>:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 649, in capture_one_batch_size
[1,0]<stdout>:    run_once()
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 637, in run_once
[1,0]<stdout>:    logits_output_or_pp_proxy_tensors = forward(
[1,0]<stdout>:                                        ^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1,0]<stdout>:    return func(*args, **kwargs)
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/models/deepseek_v2.py", line 2221, in forward
[1,0]<stdout>:    hidden_states = self.model(input_ids, positions, forward_batch, input_embeds)
[1,0]<stdout>:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
[1,0]<stdout>:    return self._call_impl(*args, **kwargs)
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
[1,0]<stdout>:    return forward_call(*args, **kwargs)
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/models/deepseek_v2.py", line 2107, in forward
[1,0]<stdout>:    hidden_states, residual = model_forward_maybe_tbo(
[1,0]<stdout>:                              ^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/two_batch_overlap.py", line 800, in model_forward_maybe_tbo
[1,0]<stdout>:    return _model_forward_tbo(
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/two_batch_overlap.py", line 832, in _model_forward_tbo
[1,0]<stdout>:    outputs_arr = execute_overlapped_operations(
[1,0]<stdout>:                 [1,0]<stdout>: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/operations.py", line 48, in execute_overlapped_operations
[1,0]<stdout>:    executor_a.next()
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/operations.py", line 99, in next
[1,0]<stdout>:    self._stage_output = op.fn(
[1,0]<stdout>:                         ^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/models/deepseek_v2.py", line 1968, in op_comm_prepare_mlp
[1,0]<stdout>:    self.layer_communicator.prepare_mlp(
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/layers/communicator.py", line 229, in prepare_mlp
[1,0]<stdout>:    return self._communicate_with_all_reduce_and_layer_norm_fn(
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/layers/communicator.py", line 430, in _gather_hidden_states_and_residual
[1,0]<stdout>:    dp_gather_partial(hidden_states, local_hidden_states, forward_batch)
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/layers/dp_attention.py", line 417, in dp_gather_partial
[1,0]<stdout>:    _dp_gather(global_tokens, local_tokens, forward_batch, is_partial=True)
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/layers/dp_attention.py", line 402, in _dp_gather
[1,0]<stdout>:    if forward_batch.dp_padding_mode.is_max_len():
[1,0]<stdout>:       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:AttributeError: 'NoneType' object has no attribute 'is_max_len'
[1,0]<stdout>:
[1,0]<stdout>:[2025-10-12 03:11:31 DP0 TP4] Scheduler hit an exception: Traceback (most recent call last):
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2555, in run_scheduler_process
[1,0]<stdout>:    scheduler = Scheduler(
[1,0]<stdout>:                ^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 315, in __init__
[1,0]<stdout>:    self.tp_worker = TpWorkerClass(
[1,0]<stdout>:                     ^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 84, in __init__
[1,0]<stdout>:    self.model_runner = ModelRunner(
[1,0]<stdout>:                        ^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 242, in __init__
[1,0]<stdout>:    self.initialize(min_per_gpu_memory)
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 342, in initialize
[1,0]<stdout>:    self.init_cuda_graphs()
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 1611, in init_cuda_graphs
[1,0]<stdout>:    self.cuda_graph_runner = CudaGraphRunner(self)
[1,0]<stdout>:                             ^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 380, in __init__
[1,0]<stdout>:    self.capture()
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 485, in capture
[1,0]<stdout>:    ) = self.capture_one_batch_size(bs, forward)
[1,0]<stdout>:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 649, in capture_one_batch_size
[1,0]<stdout>:    run_once()
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 637, in run_once
[1,0]<stdout>:    logits_output_or_pp_proxy_tensors = forward(
[1,0]<stdout>:                                        ^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1,0]<stdout>:    return func(*args, **kwargs)
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/models/deepseek_v2.py", line 2221, in forward
[1,0]<stdout>:    hidden_states = self.model(input_ids, positions, forward_batch, input_embeds)
[1,0]<stdout>:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
[1,0]<stdout>:    return self._call_impl(*args, **kwargs)
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
[1,0]<stdout>:    return forward_call(*args, **kwargs)
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/models/deepseek_v2.py", line 2107, in forward
[1,0]<stdout>:    hidden_states, residual = model_forward_maybe_tbo(
[1,0]<stdout>:                              ^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/two_batch_overlap.py", line 800, in model_forward_maybe_tbo
[1,0]<stdout>:    return _model_forward_tbo(
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/two_batch_overlap.py", line 832, in _model_forward_tbo
[1,0]<stdout>:    outputs_arr = execute_overlapped_operations(
[1,0]<stdout>:                 [1,0]<stdout>: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/operations.py", line 48, in execute_overlapped_operations
[1,0]<stdout>:    executor_a.next()
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/operations.py", line 99, in next
[1,0]<stdout>:    self._stage_output = op.fn(
[1,0]<stdout>:                         ^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/models/deepseek_v2.py", line 1968, in op_comm_prepare_mlp
[1,0]<stdout>:    self.layer_communicator.prepare_mlp(
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/layers/communicator.py", line 229, in prepare_mlp
[1,0]<stdout>:    return self._communicate_with_all_reduce_and_layer_norm_fn(
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/layers/communicator.py", line 430, in _gather_hidden_states_and_residual
[1,0]<stdout>:    dp_gather_partial(hidden_states, local_hidden_states, forward_batch)
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/layers/dp_attention.py", line 417, in dp_gather_partial
[1,0]<stdout>:    _dp_gather(global_tokens, local_tokens, forward_batch, is_partial=True)
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/layers/dp_attention.py", line 402, in _dp_gather
[1,0]<stdout>:    if forward_batch.dp_padding_mode.is_max_len():
[1,0]<stdout>:       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:AttributeError: 'NoneType' object has no attribute 'is_max_len'
[1,0]<stdout>:
[1,0]<stdout>:[2025-10-12 03:11:31 DP0 TP7] Scheduler hit an exception: Traceback (most recent call last):
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2555, in run_scheduler_process
[1,0]<stdout>:    scheduler = Scheduler(
[1,0]<stdout>:                ^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 315, in __init__
[1,0]<stdout>:    self.tp_worker = TpWorkerClass(
[1,0]<stdout>:                     ^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 84, in __init__
[1,0]<stdout>:    self.model_runner = ModelRunner(
[1,0]<stdout>:                        ^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 242, in __init__
[1,0]<stdout>:    self.initialize(min_per_gpu_memory)
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 342, in initialize
[1,0]<stdout>:    self.init_cuda_graphs()
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 1611, in init_cuda_graphs
[1,0]<stdout>:    self.cuda_graph_runner = CudaGraphRunner(self)
[1,0]<stdout>:                             ^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 380, in __init__
[1,0]<stdout>:    self.capture()
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 485, in capture
[1,0]<stdout>:    ) = self.capture_one_batch_size(bs, forward)
[1,0]<stdout>:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 649, in capture_one_batch_size
[1,0]<stdout>:    run_once()
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 637, in run_once
[1,0]<stdout>:    logits_output_or_pp_proxy_tensors = forward(
[1,0]<stdout>:                                        ^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1,0]<stdout>:    return func(*args, **kwargs)
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/models/deepseek_v2.py", line 2221, in forward
[1,0]<stdout>:    hidden_states = self.model(input_ids, positions, forward_batch, input_embeds)
[1,0]<stdout>:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
[1,0]<stdout>:    return self._call_impl(*args, **kwargs)
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
[1,0]<stdout>:    return forward_call(*args, **kwargs)
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/models/deepseek_v2.py", line 2107, in forward
[1,0]<stdout>:    hidden_states, residual = model_forward_maybe_tbo(
[1,0]<stdout>:                              ^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/two_batch_overlap.py", line 800, in model_forward_maybe_tbo
[1,0]<stdout>:    return _model_forward_tbo(
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/two_batch_overlap.py", line 832, in _model_forward_tbo
[1,0]<stdout>:    outputs_arr = execute_overlapped_operations(
[1,0]<stdout>:                 [1,0]<stdout>: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/operations.py", line 48, in execute_overlapped_operations
[1,0]<stdout>:    executor_a.next()
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/operations.py", line 99, in next
[1,0]<stdout>:    self._stage_output = op.fn(
[1,0]<stdout>:                         ^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/models/deepseek_v2.py", line 1968, in op_comm_prepare_mlp
[1,0]<stdout>:    self.layer_communicator.prepare_mlp(
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/layers/communicator.py", line 229, in prepare_mlp
[1,0]<stdout>:    return self._communicate_with_all_reduce_and_layer_norm_fn(
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/layers/communicator.py", line 430, in _gather_hidden_states_and_residual
[1,0]<stdout>:    dp_gather_partial(hidden_states, local_hidden_states, forward_batch)
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/layers/dp_attention.py", line 417, in dp_gather_partial
[1,0]<stdout>:    _dp_gather(global_tokens, local_tokens, forward_batch, is_partial=True)
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/layers/dp_attention.py", line 402, in _dp_gather
[1,0]<stdout>:    if forward_batch.dp_padding_mode.is_max_len():
[1,0]<stdout>:       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:AttributeError: 'NoneType' object has no attribute 'is_max_len'
[1,0]<stdout>:
[1,1]<stdout>:[2025-10-12 03:11:31 DP1 TP15] Scheduler hit an exception: Traceback (most recent call last):
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2555, in run_scheduler_process
[1,1]<stdout>:    scheduler = Scheduler(
[1,1]<stdout>:                ^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 315, in __init__
[1,1]<stdout>:    self.tp_worker = TpWorkerClass(
[1,1]<stdout>:                     ^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 84, in __init__
[1,1]<stdout>:    self.model_runner = ModelRunner(
[1,1]<stdout>:                        ^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 242, in __init__
[1,1]<stdout>:    self.initialize(min_per_gpu_memory)
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 342, in initialize
[1,1]<stdout>:    self.init_cuda_graphs()
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 1611, in init_cuda_graphs
[1,1]<stdout>:    self.cuda_graph_runner = CudaGraphRunner(self)
[1,1]<stdout>:                             ^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 380, in __init__
[1,1]<stdout>:    self.capture()
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 485, in capture
[1,1]<stdout>:    ) = self.capture_one_batch_size(bs, forward)
[1,1]<stdout>:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 649, in capture_one_batch_size
[1,1]<stdout>:    run_once()
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 637, in run_once
[1,1]<stdout>:    logits_output_or_pp_proxy_tensors = forward(
[1,1]<stdout>:                                        ^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1,1]<stdout>:    return func(*args, **kwargs)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/models/deepseek_v2.py", line 2221, in forward
[1,1]<stdout>:    hidden_states = self.model(input_ids, positions, forward_batch, input_embeds)
[1,1]<stdout>:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
[1,1]<stdout>:    return self._call_impl(*args, **kwargs)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
[1,1]<stdout>:    return forward_call(*args, **kwargs)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/models/deepseek_v2.py", line 2107, in forward
[1,1]<stdout>:    hidden_states, residual = model_forward_maybe_tbo(
[1,1]<stdout>:                              ^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/two_batch_overlap.py", line 800, in model_forward_maybe_tbo
[1,1]<stdout>:    return _model_forward_tbo(
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/two_batch_overlap.py", line 832, in _model_forward_tbo
[1,1]<stdout>:    outputs_arr = execute_overlapped_operations(
[1,1]<stdout>:                [1,1]<stdout>:  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/operations.py", line 48, in execute_overlapped_operations
[1,1]<stdout>:    executor_a.next()
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/operations.py", line 99, in next
[1,1]<stdout>:    self._stage_output = op.fn(
[1,1]<stdout>:                         ^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/models/deepseek_v2.py", line 1968, in op_comm_prepare_mlp
[1,1]<stdout>:    self.layer_communicator.prepare_mlp(
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/layers/communicator.py", line 229, in prepare_mlp
[1,1]<stdout>:    return self._communicate_with_all_reduce_and_layer_norm_fn(
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/layers/communicator.py", line 430, in _gather_hidden_states_and_residual
[1,1]<stdout>:    dp_gather_partial(hidden_states, local_hidden_states, forward_batch)
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/layers/dp_attention.py", line 417, in dp_gather_partial
[1,1]<stdout>:    _dp_gather(global_tokens, local_tokens, forward_batch, is_partial=True)
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/layers/dp_attention.py", line 402, in _dp_gather
[1,1]<stdout>:    if forward_batch.dp_padding_mode.is_max_len():
[1,1]<stdout>:       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:AttributeError: 'NoneType' object has no attribute 'is_max_len'
[1,1]<stdout>:
[1,0]<stdout>:[2025-10-12 03:11:31 DP0 TP1] Scheduler hit an exception: Traceback (most recent call last):
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2555, in run_scheduler_process
[1,0]<stdout>:    scheduler = Scheduler(
[1,0]<stdout>:                ^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 315, in __init__
[1,0]<stdout>:    self.tp_worker = TpWorkerClass(
[1,0]<stdout>:                     ^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 84, in __init__
[1,0]<stdout>:    self.model_runner = ModelRunner(
[1,0]<stdout>:                        ^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 242, in __init__
[1,0]<stdout>:    self.initialize(min_per_gpu_memory)
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 342, in initialize
[1,0]<stdout>:    self.init_cuda_graphs()
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 1611, in init_cuda_graphs
[1,0]<stdout>:    self.cuda_graph_runner = CudaGraphRunner(self)
[1,0]<stdout>:                             ^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 380, in __init__
[1,0]<stdout>:    self.capture()
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 485, in capture
[1,0]<stdout>:    ) = self.capture_one_batch_size(bs, forward)
[1,0]<stdout>:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 649, in capture_one_batch_size
[1,0]<stdout>:    run_once()
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 637, in run_once
[1,0]<stdout>:    logits_output_or_pp_proxy_tensors = forward(
[1,0]<stdout>:                                        ^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1,0]<stdout>:    return func(*args, **kwargs)
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/models/deepseek_v2.py", line 2221, in forward
[1,0]<stdout>:    hidden_states = self.model(input_ids, positions, forward_batch, input_embeds)
[1,0]<stdout>:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
[1,0]<stdout>:    return self._call_impl(*args, **kwargs)
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
[1,0]<stdout>:    return forward_call(*args, **kwargs)
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/models/deepseek_v2.py", line 2107, in forward
[1,0]<stdout>:    hidden_states, residual = model_forward_maybe_tbo(
[1,0]<stdout>:                              ^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/two_batch_overlap.py", line 800, in model_forward_maybe_tbo
[1,0]<stdout>:    return _model_forward_tbo(
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/two_batch_overlap.py", line 832, in _model_forward_tbo
[1,0]<stdout>:    outputs_arr = execute_overlapped_operations(
[1,0]<stdout>:                [1,0]<stdout>:  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/operations.py", line 48, in execute_overlapped_operations
[1,0]<stdout>:    executor_a.next()
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/operations.py", line 99, in next
[1,0]<stdout>:    self._stage_output = op.fn(
[1,0]<stdout>:                         ^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/models/deepseek_v2.py", line 1968, in op_comm_prepare_mlp
[1,0]<stdout>:    self.layer_communicator.prepare_mlp(
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/layers/communicator.py", line 229, in prepare_mlp
[1,0]<stdout>:    return self._communicate_with_all_reduce_and_layer_norm_fn(
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/layers/communicator.py", line 430, in _gather_hidden_states_and_residual
[1,0]<stdout>:    dp_gather_partial(hidden_states, local_hidden_states, forward_batch)
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/layers/dp_attention.py", line 417, in dp_gather_partial
[1,0]<stdout>:    _dp_gather(global_tokens, local_tokens, forward_batch, is_partial=True)
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/layers/dp_attention.py", line 402, in _dp_gather
[1,0]<stdout>:    if forward_batch.dp_padding_mode.is_max_len():
[1,0]<stdout>:       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:AttributeError: 'NoneType' object has no attribute 'is_max_len'
[1,0]<stdout>:
[1,0]<stdout>:[2025-10-12 03:11:31 DP0 TP3] Scheduler hit an exception: Traceback (most recent call last):
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2555, in run_scheduler_process
[1,0]<stdout>:    scheduler = Scheduler(
[1,0]<stdout>:                ^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 315, in __init__
[1,0]<stdout>:    self.tp_worker = TpWorkerClass(
[1,0]<stdout>:                     ^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 84, in __init__
[1,0]<stdout>:    self.model_runner = ModelRunner(
[1,0]<stdout>:                        ^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 242, in __init__
[1,0]<stdout>:    self.initialize(min_per_gpu_memory)
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 342, in initialize
[1,0]<stdout>:    self.init_cuda_graphs()
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 1611, in init_cuda_graphs
[1,0]<stdout>:    self.cuda_graph_runner = CudaGraphRunner(self)
[1,0]<stdout>:                             ^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 380, in __init__
[1,0]<stdout>:    self.capture()
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 485, in capture
[1,0]<stdout>:    ) = self.capture_one_batch_size(bs, forward)
[1,0]<stdout>:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 649, in capture_one_batch_size
[1,0]<stdout>:    run_once()
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 637, in run_once
[1,0]<stdout>:    logits_output_or_pp_proxy_tensors = forward(
[1,0]<stdout>:                                        ^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1,0]<stdout>:    return func(*args, **kwargs)
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/models/deepseek_v2.py", line 2221, in forward
[1,0]<stdout>:    hidden_states = self.model(input_ids, positions, forward_batch, input_embeds)
[1,0]<stdout>:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
[1,0]<stdout>:    return self._call_impl(*args, **kwargs)
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
[1,0]<stdout>:    return forward_call(*args, **kwargs)
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/models/deepseek_v2.py", line 2107, in forward
[1,0]<stdout>:    hidden_states, residual = model_forward_maybe_tbo(
[1,0]<stdout>:                              ^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/two_batch_overlap.py", line 800, in model_forward_maybe_tbo
[1,0]<stdout>:    return _model_forward_tbo(
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/two_batch_overlap.py", line 832, in _model_forward_tbo
[1,0]<stdout>:    outputs_arr = execute_overlapped_operations(
[1,0]<stdout>:                 [1,0]<stdout>: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/operations.py", line 48, in execute_overlapped_operations
[1,0]<stdout>:    executor_a.next()
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/operations.py", line 99, in next
[1,0]<stdout>:    self._stage_output = op.fn(
[1,0]<stdout>:                         ^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/models/deepseek_v2.py", line 1968, in op_comm_prepare_mlp
[1,0]<stdout>:    self.layer_communicator.prepare_mlp(
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/layers/communicator.py", line 229, in prepare_mlp
[1,0]<stdout>:    return self._communicate_with_all_reduce_and_layer_norm_fn(
[1,0]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/layers/communicator.py", line 430, in _gather_hidden_states_and_residual
[1,0]<stdout>:    dp_gather_partial(hidden_states, local_hidden_states, forward_batch)
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/layers/dp_attention.py", line 417, in dp_gather_partial
[1,0]<stdout>:    _dp_gather(global_tokens, local_tokens, forward_batch, is_partial=True)
[1,0]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/layers/dp_attention.py", line 402, in _dp_gather
[1,0]<stdout>:    if forward_batch.dp_padding_mode.is_max_len():
[1,0]<stdout>:       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stdout>:AttributeError: 'NoneType' object has no attribute 'is_max_len'
[1,0]<stdout>:
[1,1]<stdout>:[2025-10-12 03:11:31 DP1 TP10] Scheduler hit an exception: Traceback (most recent call last):
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2555, in run_scheduler_process
[1,1]<stdout>:    scheduler = Scheduler(
[1,1]<stdout>:                ^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 315, in __init__
[1,1]<stdout>:    self.tp_worker = TpWorkerClass(
[1,1]<stdout>:                     ^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 84, in __init__
[1,1]<stdout>:    self.model_runner = ModelRunner(
[1,1]<stdout>:                        ^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 242, in __init__
[1,1]<stdout>:    self.initialize(min_per_gpu_memory)
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 342, in initialize
[1,1]<stdout>:    self.init_cuda_graphs()
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 1611, in init_cuda_graphs
[1,1]<stdout>:    self.cuda_graph_runner = CudaGraphRunner(self)
[1,1]<stdout>:                             ^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 380, in __init__
[1,1]<stdout>:    self.capture()
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 485, in capture
[1,1]<stdout>:    ) = self.capture_one_batch_size(bs, forward)
[1,1]<stdout>:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 649, in capture_one_batch_size
[1,1]<stdout>:    run_once()
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py", line 637, in run_once
[1,1]<stdout>:    logits_output_or_pp_proxy_tensors = forward(
[1,1]<stdout>:                                        ^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1,1]<stdout>:    return func(*args, **kwargs)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/models/deepseek_v2.py", line 2221, in forward
[1,1]<stdout>:    hidden_states = self.model(input_ids, positions, forward_batch, input_embeds)
[1,1]<stdout>:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
[1,1]<stdout>:    return self._call_impl(*args, **kwargs)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
[1,1]<stdout>:    return forward_call(*args, **kwargs)
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/models/deepseek_v2.py", line 2107, in forward
[1,1]<stdout>:    hidden_states, residual = model_forward_maybe_tbo(
[1,1]<stdout>:                              ^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/two_batch_overlap.py", line 800, in model_forward_maybe_tbo
[1,1]<stdout>:    return _model_forward_tbo(
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/two_batch_overlap.py", line 832, in _model_forward_tbo
[1,1]<stdout>:    outputs_arr = execute_overlapped_operations(
[1,1]<stdout>:               [1,1]<stdout>:   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/operations.py", line 48, in execute_overlapped_operations
[1,1]<stdout>:    executor_a.next()
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/operations.py", line 99, in next
[1,1]<stdout>:    self._stage_output = op.fn(
[1,1]<stdout>:                         ^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/models/deepseek_v2.py", line 1968, in op_comm_prepare_mlp
[1,1]<stdout>:    self.layer_communicator.prepare_mlp(
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/layers/communicator.py", line 229, in prepare_mlp
[1,1]<stdout>:    return self._communicate_with_all_reduce_and_layer_norm_fn(
[1,1]<stdout>:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/layers/communicator.py", line 430, in _gather_hidden_states_and_residual
[1,1]<stdout>:    dp_gather_partial(hidden_states, local_hidden_states, forward_batch)
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/layers/dp_attention.py", line 417, in dp_gather_partial
[1,1]<stdout>:    _dp_gather(global_tokens, local_tokens, forward_batch, is_partial=True)
[1,1]<stdout>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/layers/dp_attention.py", line 402, in _dp_gather
[1,1]<stdout>:    if forward_batch.dp_padding_mode.is_max_len():
[1,1]<stdout>:       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,1]<stdout>:AttributeError: 'NoneType' object has no attribute 'is_max_len'
[1,1]<stdout>:
=>> PBS: job killed: walltime 603 exceeded limit 600
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
			Resource Usage on 2025-10-12 03:19:55.319535:
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	JobId: 97120.pbs111
	Project: 50000128
	Exit Status: -29
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	NCPUs: Requested(224), Used(224)
	CPU Time Used: 00:11:35
	Memory: Requested(3760gb), Used(25810064kb)
	Vmem Used: 75496883116kb
	Walltime: Requested(00:10:00), Used(00:10:15)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	Execution Nodes Used: (a2ap-dgx007:ncpus=112:ngpus=8:mem=1971322880kb)+(a2ap-dgx010:ncpus=112:ngpus=8:mem=1971322880kb)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	GPU Duration: 10.4mins
	GPU Power Consumed: 153.29W
	GPU Max GPU Memory Used: 1.24TB
	Memory Throughput Rate (Average): a2ap-dgx007:(gpu1:0%+gpu0:0%+gpu2:0%+gpu3:0%+gpu5:0%+gpu4:0%+gpu6:0%+gpu7:0%)+a2ap-dgx010:(gpu1:0%+gpu0:0%+gpu2:0%+gpu3:0%+gpu5:0%+gpu4:0%+gpu6:0%+gpu7:0%)
	Memory Throughput Rate (Max): a2ap-dgx007:(gpu1:0%+gpu0:0%+gpu2:1%+gpu3:0%+gpu5:0%+gpu4:1%+gpu6:0%+gpu7:1%)+a2ap-dgx010:(gpu1:0%+gpu0:0%+gpu2:1%+gpu3:0%+gpu5:1%+gpu4:0%+gpu6:1%+gpu7:0%)
	Memory Throughput Rate (Min): a2ap-dgx007:(gpu1:0%+gpu0:0%+gpu2:0%+gpu3:0%+gpu5:0%+gpu4:0%+gpu6:0%+gpu7:0%)+a2ap-dgx010:(gpu1:0%+gpu0:0%+gpu2:0%+gpu3:0%+gpu5:0%+gpu4:0%+gpu6:0%+gpu7:0%)
	GPU SM Utilization (Average): a2ap-dgx007:(gpu1:0%+gpu0:0%+gpu2:0%+gpu3:0%+gpu5:0%+gpu4:0%+gpu6:0%+gpu7:0%)+a2ap-dgx010:(gpu1:0%+gpu0:1%+gpu2:0%+gpu3:0%+gpu5:0%+gpu4:0%+gpu6:0%+gpu7:1%)
	GPU SM Utilization (Max): a2ap-dgx007:(gpu1:3%+gpu0:3%+gpu2:4%+gpu3:1%+gpu5:2%+gpu4:1%+gpu6:5%+gpu7:6%)+a2ap-dgx010:(gpu1:1%+gpu0:85%+gpu2:12%+gpu3:2%+gpu5:5%+gpu4:5%+gpu6:0%+gpu7:48%)
	GPU SM Utilization (Min): a2ap-dgx007:(gpu1:0%+gpu0:0%+gpu2:0%+gpu3:0%+gpu5:0%+gpu4:0%+gpu6:0%+gpu7:0%)+a2ap-dgx010:(gpu1:0%+gpu0:0%+gpu2:0%+gpu3:0%+gpu5:0%+gpu4:0%+gpu6:0%+gpu7:0%)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Warning: GPUs 1, 2, 3, 5, 4, 6 have a percentage of 0 utilisation.
GPU application profile: Low
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

