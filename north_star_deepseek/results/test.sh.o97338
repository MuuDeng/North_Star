?? 2-NODE TP16 - MINIMAL WORKING VERSION (Based on successful pattern)
========== 2-NODE TP16 SU SUMMARY ==========
Prepaid SU: 341.334 | 420s SU: 238.934 | Balance: 35905.356
N/A
Job ID: 97338.pbs111 | GPUs: 16 | Nodes: 2 DGX
Model: DeepSeek-R1 (HuggingFace) | Workers: 2 MPI processes
============================================
[21:42:51] Starting 2-node TP16 - Minimal working version
[21:42:51] Master: a2ap-dgx013.asp2p.nscc.sg:5000
[21:42:51] Model: /home/users/industry/ai-hpc/apacsc34/scratch/model/DeepSeek-R1
[21:42:51] Data: /home/users/industry/ai-hpc/apacsc34/scratch/ShareGPT_V3_unfiltered_cleaned_split.json
[21:42:51] Validating setup...
[21:42:51] Validation passed
[21:42:51] Launching 2-node TP16 benchmark...
 Data for JOB [37444,1] offset 0 Total slots allocated 4

 ========================   JOB MAP   ========================

 Data for node: a2ap-dgx013	Num slots: 2	Max slots: 0	Num procs: 1
 	Process OMPI jobid: [37444,1] App: 0 Process rank: 0 Bound: N/A

 Data for node: a2ap-dgx014	Num slots: 2	Max slots: 0	Num procs: 1
 	Process OMPI jobid: [37444,1] App: 0 Process rank: 1 Bound: N/A

 =============================================================
 Data for JOB [37444,1] offset 0 Total slots allocated 4

 ========================   JOB MAP   ========================

 Data for node: a2ap-dgx013	Num slots: 2	Max slots: 0	Num procs: 1
 	Process OMPI jobid: [37444,1] App: 0 Process rank: 0 Bound: N/A

 Data for node: a2ap-dgx014	Num slots: 2	Max slots: 0	Num procs: 1
 	Process OMPI jobid: [37444,1] App: 0 Process rank: 1 Bound: N/A

 =============================================================
[1,0]<stdout>:[Node 0] Starting SGLang benchmark...
[1,0]<stdout>:[Node 0] DIST_INIT_ADDR: a2ap-dgx013.asp2p.nscc.sg:5000
[1,0]<stdout>:[Node 0] NODE_RANK: 0
[1,1]<stdout>:[Node 1] Starting SGLang benchmark...
[1,1]<stdout>:[Node 1] DIST_INIT_ADDR: a2ap-dgx013.asp2p.nscc.sg:5000
[1,1]<stdout>:[Node 1] NODE_RANK: 1
[1,0]<stderr>:/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/tvm_ffi/_optional_torch_c_dlpack.py:559: UserWarning: Failed to load torch c dlpack extension: Ninja is required to load C++ extensions (pip install ninja to get it),EnvTensorAllocator will not be enabled.
[1,0]<stderr>:  warnings.warn(
[1,0]<stderr>:Traceback (most recent call last):
[1,0]<stderr>:  File "<frozen runpy>", line 198, in _run_module_as_main
[1,0]<stderr>:  File "<frozen runpy>", line 88, in _run_code
[1,0]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/bench_offline_throughput.py", line 449, in <module>
[1,0]<stderr>:    throughput_test(server_args, bench_args)
[1,0]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/bench_offline_throughput.py", line 315, in throughput_test
[1,0]<stderr>:    backend = Engine(**dataclasses.asdict(server_args))
[1,0]<stderr>:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1,0]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/entrypoints/engine.py", line 124, in __init__
[1,0]<stderr>:    tokenizer_manager, template_manager, scheduler_info = _launch_subprocesses(
[1,0]<stderr>:                                                          ^^^^^^^^^^^^^^^^^^^^^
[1,0]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/entrypoints/engine.py", line 688, in _launch_subprocesses
[1,0]<stderr>:    server_args.check_server_args()
[1,0]<stderr>:  File "/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/sglang/srt/server_args.py", line 2008, in check_server_args
[1,0]<stderr>:    assert not (
[1,0]<stderr>:           ^^^^^
[1,0]<stderr>:AssertionError: multi-node data parallel is not supported unless dp attention!
[1,0]<stderr>:
[1,0]<stderr>:real	0m22.114s
[1,0]<stderr>:user	0m31.927s
[1,0]<stderr>:sys	0m8.661s
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
[1,1]<stderr>:/home/users/industry/ai-hpc/apacsc34/scratch/tanathep/py312/lib/python3.12/site-packages/tvm_ffi/_optional_torch_c_dlpack.py:559: UserWarning: Failed to load torch c dlpack extension: Ninja is required to load C++ extensions (pip install ninja to get it),EnvTensorAllocator will not be enabled.
[1,1]<stderr>:  warnings.warn(
--------------------------------------------------------------------------
mpirun detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[37444,1],0]
  Exit code:    1
--------------------------------------------------------------------------

real	0m26.820s
user	0m31.956s
sys	0m8.739s
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
			Resource Usage on 2025-10-12 21:43:26.217808:
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	JobId: 97338.pbs111
	Project: 50000128
	Exit Status: 1
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	NCPUs: Requested(224), Used(224)
	CPU Time Used: 00:00:41
	Memory: Requested(3760gb), Used(920140kb)
	Vmem Used: 52207188kb
	Walltime: Requested(00:10:00), Used(00:00:33)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	Execution Nodes Used: (a2ap-dgx013:ncpus=112:ngpus=8:mem=1971322880kb)+(a2ap-dgx014:ncpus=112:ngpus=8:mem=1971322880kb)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	GPU Duration: 45.63secs
	GPU Power Consumed: 122.33W
	GPU Max GPU Memory Used: 0.0B
	Memory Throughput Rate (Average): a2ap-dgx013:(gpu1:0%+gpu0:0%+gpu2:0%+gpu3:0%+gpu5:0%+gpu4:0%+gpu6:0%+gpu7:0%)+a2ap-dgx014:(gpu1:0%+gpu0:0%+gpu2:0%+gpu3:0%+gpu5:0%+gpu4:0%+gpu6:0%+gpu7:0%)
	Memory Throughput Rate (Max): a2ap-dgx013:(gpu1:0%+gpu0:0%+gpu2:0%+gpu3:0%+gpu5:0%+gpu4:0%+gpu6:0%+gpu7:0%)+a2ap-dgx014:(gpu1:0%+gpu0:0%+gpu2:0%+gpu3:0%+gpu5:0%+gpu4:0%+gpu6:0%+gpu7:0%)
	Memory Throughput Rate (Min): a2ap-dgx013:(gpu1:0%+gpu0:0%+gpu2:0%+gpu3:0%+gpu5:0%+gpu4:0%+gpu6:0%+gpu7:0%)+a2ap-dgx014:(gpu1:0%+gpu0:0%+gpu2:0%+gpu3:0%+gpu5:0%+gpu4:0%+gpu6:0%+gpu7:0%)
	GPU SM Utilization (Average): a2ap-dgx013:(gpu1:0%+gpu0:0%+gpu2:0%+gpu3:0%+gpu5:0%+gpu4:0%+gpu6:0%+gpu7:0%)+a2ap-dgx014:(gpu1:0%+gpu0:0%+gpu2:0%+gpu3:0%+gpu5:0%+gpu4:0%+gpu6:0%+gpu7:0%)
	GPU SM Utilization (Max): a2ap-dgx013:(gpu1:0%+gpu0:0%+gpu2:0%+gpu3:0%+gpu5:0%+gpu4:0%+gpu6:0%+gpu7:0%)+a2ap-dgx014:(gpu1:0%+gpu0:0%+gpu2:0%+gpu3:0%+gpu5:0%+gpu4:0%+gpu6:0%+gpu7:0%)
	GPU SM Utilization (Min): a2ap-dgx013:(gpu1:0%+gpu0:0%+gpu2:0%+gpu3:0%+gpu5:0%+gpu4:0%+gpu6:0%+gpu7:0%)+a2ap-dgx014:(gpu1:0%+gpu0:0%+gpu2:0%+gpu3:0%+gpu5:0%+gpu4:0%+gpu6:0%+gpu7:0%)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Warning: All GPUs have a percentage of 0 utilisation.
GPU application profile: Idle
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

